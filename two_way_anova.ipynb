{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebf36cd7-d8b6-4dba-a891-c7d5a62b27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Two-way ANOVA analysis for genomic oxidation data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=ValueWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bb17d31-539f-433f-afd3-238cf8a8b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Preprocess the raw data file to extract bin information and create a feature matrix\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file with oxidation data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (pivot_df, bin_ids, metadata_df)\n",
    "        pivot_df: DataFrame with samples as rows and bins as columns\n",
    "        bin_ids: List of bin IDs\n",
    "        metadata_df: DataFrame with sample metadata extracted from sample names\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    \n",
    "    # Extract bin size from file path\n",
    "    bin_size = re.search(r'Normalized_(\\d+)', file_path).group(1)\n",
    "    print(f\"Detected bin size: {bin_size}\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Create FeatureID by combining Bin, Strand, and Chromosome\n",
    "    df['FeatureID'] = df['Bin'].astype(str) + '_' + df['Strand'] + df['Chromosome'].astype(str)\n",
    "    \n",
    "    # Handle potential duplicates by grouping\n",
    "    df['Group'] = df.groupby(['Sample', 'FeatureID']).cumcount().astype(str)\n",
    "    \n",
    "    # Show sample of the processed data\n",
    "    print(\"Sample of the processed dataframe:\")\n",
    "    print(df[['Sample', 'Bin', 'Strand', 'FeatureID', 'Group', 'Median_Normalized_Damage']].head(5))\n",
    "    \n",
    "    # Create pivot table: samples as rows, features as columns\n",
    "    pivot_df = df.pivot_table(\n",
    "        index='Sample', \n",
    "        columns='FeatureID', \n",
    "        values='Median_Normalized_Damage',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Extract bin IDs (these will be the column names in the pivot table)\n",
    "    bin_ids = pivot_df.columns.tolist()\n",
    "    \n",
    "    # Extract metadata from sample names\n",
    "    metadata_df = pd.DataFrame(index=pivot_df.index)\n",
    "    \n",
    "    # Extract group (treatment) and time point from sample names\n",
    "    metadata_df['treatment'] = metadata_df.index.map(lambda x: re.search(r'(CRS|Ctrl)', x).group(1) if re.search(r'(CRS|Ctrl)', x) else \"Unknown\")\n",
    "    metadata_df['timepoint'] = metadata_df.index.map(lambda x: re.search(r'(evening|morning)', x).group(1) if re.search(r'(evening|morning)', x) else \"Unknown\")\n",
    "    \n",
    "    # Reset index to make Sample a standard column\n",
    "    metadata_df = metadata_df.reset_index().rename(columns={'index': 'Sample'})\n",
    "    \n",
    "    print(f\"Created pivot table with {pivot_df.shape[0]} samples and {pivot_df.shape[1]} genomic features\")\n",
    "    print(f\"Extracted metadata with factors: treatment ({metadata_df['treatment'].unique()}) and timepoint ({metadata_df['timepoint'].unique()})\")\n",
    "    print(metadata_df.head(5))\n",
    "    \n",
    "    return pivot_df, bin_ids, metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a8adfdc-6a12-4716-80b2-5c3bf5b45b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_two_way_anova(bin_data, factor1_name, factor2_name):\n",
    "    \"\"\"\n",
    "    Perform two-way ANOVA on a single bin's data with improved error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if we have at least two levels for each factor\n",
    "        factor1_levels = bin_data[factor1_name].nunique()\n",
    "        factor2_levels = bin_data[factor2_name].nunique()\n",
    "        \n",
    "        if factor1_levels < 2 or factor2_levels < 2:\n",
    "            print(f\"Skipping bin: Insufficient factor levels ({factor1_levels}, {factor2_levels})\")\n",
    "            return {\n",
    "                'factor1_pvalue': np.nan,\n",
    "                'factor2_pvalue': np.nan,\n",
    "                'interaction_pvalue': np.nan,\n",
    "                'factor1_Fvalue': np.nan,\n",
    "                'factor2_Fvalue': np.nan,\n",
    "                'interaction_Fvalue': np.nan\n",
    "            }\n",
    "        \n",
    "        # Check for complete cells - make sure each combination has data\n",
    "        combinations = bin_data.groupby([factor1_name, factor2_name]).size()\n",
    "        if 0 in combinations.values or combinations.shape[0] < factor1_levels * factor2_levels:\n",
    "            print(f\"Skipping bin: Incomplete factorial design\")\n",
    "            return {\n",
    "                'factor1_pvalue': np.nan,\n",
    "                'factor2_pvalue': np.nan,\n",
    "                'interaction_pvalue': np.nan,\n",
    "                'factor1_Fvalue': np.nan,\n",
    "                'factor2_Fvalue': np.nan,\n",
    "                'interaction_Fvalue': np.nan\n",
    "            }\n",
    "            \n",
    "        # Create the formula for the model\n",
    "        formula = f\"measurement ~ C({factor1_name}) + C({factor2_name}) + C({factor1_name}):C({factor2_name})\"\n",
    "        \n",
    "        # Fit the model with try/except to catch specific warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "            warnings.filterwarnings('ignore', category=ValueWarning)\n",
    "            model = ols(formula, data=bin_data).fit()\n",
    "        \n",
    "        # Get ANOVA table\n",
    "        anova_table = sm.stats.anova_lm(model, typ=3)\n",
    "        \n",
    "        # Extract results with checks for invalid values\n",
    "        results = {}\n",
    "        for factor, prefix in zip(\n",
    "            [f'C({factor1_name})', f'C({factor2_name})', f'C({factor1_name}):C({factor2_name})'],\n",
    "            ['factor1_', 'factor2_', 'interaction_']\n",
    "        ):\n",
    "            if factor in anova_table.index:\n",
    "                results[f'{prefix}pvalue'] = anova_table.loc[factor, 'PR(>F)']\n",
    "                results[f'{prefix}Fvalue'] = anova_table.loc[factor, 'F']\n",
    "            else:\n",
    "                results[f'{prefix}pvalue'] = np.nan\n",
    "                results[f'{prefix}Fvalue'] = np.nan\n",
    "        \n",
    "        # Check for invalid values\n",
    "        for key in results:\n",
    "            if pd.isna(results[key]) or np.isinf(results[key]):\n",
    "                results[key] = np.nan\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in ANOVA: {str(e)}\")\n",
    "        return {\n",
    "            'factor1_pvalue': np.nan,\n",
    "            'factor2_pvalue': np.nan,\n",
    "            'interaction_pvalue': np.nan,\n",
    "            'factor1_Fvalue': np.nan,\n",
    "            'factor2_Fvalue': np.nan,\n",
    "            'interaction_Fvalue': np.nan\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6cba59f-9834-4acb-b02a-ce81329bf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_bin(args):\n",
    "    \"\"\"Process a single bin - for parallel processing\"\"\"\n",
    "    bin_idx, bin_id, pivot_df, metadata_df, factor1_name, factor2_name = args\n",
    "    \n",
    "    # Extract measurements for this bin\n",
    "    bin_values = pivot_df.iloc[:, bin_idx].values\n",
    "    \n",
    "    # Create DataFrame with measurements and factors\n",
    "    df = pd.DataFrame({\n",
    "        'measurement': bin_values,\n",
    "        'Sample': pivot_df.index.tolist()\n",
    "    })\n",
    "\n",
    "    \n",
    "    # Merge with metadata to get the factors\n",
    "    df = pd.merge(df, metadata_df, on='Sample')\n",
    "    \n",
    "    # Perform ANOVA\n",
    "    result = perform_two_way_anova(df, factor1_name, factor2_name)\n",
    "    result['bin_id'] = bin_id\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c045eb4-7ea1-4c2a-8bd6-12aa2ae5906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_genomic_bins(file_path, factor1_name='treatment', factor2_name='timepoint', \n",
    "                       output_prefix=None, n_cores=None):\n",
    "    \"\"\"\n",
    "    Analyze thousands of genomic bins with two-way ANOVA\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to CSV file with oxidation data\n",
    "    factor1_name : str\n",
    "        Name of the first factor (default: 'treatment')\n",
    "    factor2_name : str\n",
    "        Name of the second factor (default: 'timepoint')\n",
    "    output_prefix : str\n",
    "        Prefix for output files (default: derived from input file)\n",
    "    n_cores : int, optional\n",
    "        Number of CPU cores to use for parallel processing\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with ANOVA results for all bins\n",
    "    \"\"\"\n",
    "    # Set default output prefix if not provided\n",
    "    if output_prefix is None:\n",
    "        output_prefix = os.path.splitext(os.path.basename(file_path))[0] + \"_anova\"\n",
    "    \n",
    "    # Process the data\n",
    "    pivot_df, bin_ids, metadata_df = preprocess_data(file_path)\n",
    "    \n",
    "    # Determine number of cores for parallel processing\n",
    "    if n_cores is None:\n",
    "        n_cores = max(1, mp.cpu_count() - 1)  # Leave one core free\n",
    "    \n",
    "    print(f\"Processing {len(bin_ids)} bins using {n_cores} cores...\")\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = [(i, bin_id, pivot_df, metadata_df, factor1_name, factor2_name) \n",
    "                for i, bin_id in enumerate(bin_ids)]\n",
    "    \n",
    "    # Initialize results list\n",
    "    results_list = []\n",
    "    \n",
    "    # Set up parallel processing\n",
    "    with mp.Pool(n_cores) as pool:\n",
    "        # Process bins in parallel with progress bar\n",
    "        for result in tqdm(pool.imap(process_bin, args_list), total=len(bin_ids)): #Perform test\n",
    "            results_list.append(result)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Apply multiple testing correction\n",
    "    for col in ['factor1_pvalue', 'factor2_pvalue', 'interaction_pvalue']:\n",
    "        # Benjamini-Hochberg FDR correction\n",
    "        mask = ~np.isnan(results_df[col])\n",
    "        corrected = np.full(len(results_df), np.nan)\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            corrected[mask] = multipletests(results_df.loc[mask, col], method='fdr_bh')[1]\n",
    "            \n",
    "        results_df[f'{col}_adj'] = corrected\n",
    "        \n",
    "        # Add significance flag (True/False)\n",
    "        results_df[f'{col.replace(\"pvalue\", \"significant\")}'] = results_df[f'{col}_adj'] < 0.05\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(os.path.dirname(output_prefix) if os.path.dirname(output_prefix) else '.', exist_ok=True)\n",
    "    \n",
    "    # Write results to file\n",
    "    results_df.to_csv(f\"{output_prefix}_results.csv\", index=False)\n",
    "    \n",
    "    print(f\"Analysis complete. Found:\")\n",
    "    print(f\"  - {results_df['factor1_significant'].sum()} bins significant for {factor1_name}\")\n",
    "    print(f\"  - {results_df['factor2_significant'].sum()} bins significant for {factor2_name}\")\n",
    "    print(f\"  - {results_df['interaction_significant'].sum()} bins significant for interaction\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9a02e1c-b89a-414a-b3e9-1d420441b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_anova_results(results_df, factor1_name, factor2_name, output_prefix):\n",
    "    \"\"\"\n",
    "    Create visualizations of ANOVA results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame with ANOVA results from analyze_genomic_bins()\n",
    "    factor1_name : str\n",
    "        Name of the first factor\n",
    "    factor2_name : str\n",
    "        Name of the second factor\n",
    "    output_prefix : str\n",
    "        Prefix for output files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with matplotlib figure objects\n",
    "    \"\"\"\n",
    "    # Create output directory for plots\n",
    "    plots_dir = \"images/anova_results\"\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Creating visualizations...\")\n",
    "    figures = {}\n",
    "    \n",
    "    # 1. Manhattan-like plot of p-values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(range(len(results_df)), -np.log10(results_df['factor1_pvalue']), \n",
    "               alpha=0.5, s=10, label=factor1_name)\n",
    "    plt.scatter(range(len(results_df)), -np.log10(results_df['factor2_pvalue']), \n",
    "               alpha=0.5, s=10, label=factor2_name)\n",
    "    plt.scatter(range(len(results_df)), -np.log10(results_df['interaction_pvalue']), \n",
    "               alpha=0.5, s=10, label='Interaction')\n",
    "    \n",
    "    # Add significance thresholds\n",
    "    plt.axhline(-np.log10(0.05), linestyle='--', color='red', label='p=0.05')\n",
    "    plt.axhline(-np.log10(0.05/len(results_df)), linestyle='--', color='blue', label='Bonferroni')\n",
    "    \n",
    "    plt.xlabel('Genomic Bin Index')\n",
    "    plt.ylabel('-log10(p-value)')\n",
    "    plt.title('Manhattan Plot of Two-way ANOVA p-values')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f\"{plots_dir}/manhattan_plot.png\", dpi=300)\n",
    "    figures['manhattan'] = plt.gcf()\n",
    "    \n",
    "    # 2. Volcano plots for each factor\n",
    "    factor_cols = [\n",
    "        (factor1_name, 'factor1_pvalue', 'factor1_Fvalue', 'factor1_significant'),\n",
    "        (factor2_name, 'factor2_pvalue', 'factor2_Fvalue', 'factor2_significant'),\n",
    "        ('Interaction', 'interaction_pvalue', 'interaction_Fvalue', 'interaction_significant')\n",
    "    ]\n",
    "    \n",
    "    for name, pval_col, fval_col, sig_col in factor_cols:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        # Create a mask for non-NaN values\n",
    "        mask = ~np.isnan(results_df[pval_col]) & ~np.isnan(results_df[fval_col])\n",
    "        \n",
    "        # Create a scatter plot with color indicating significance\n",
    "        plt.scatter(\n",
    "            results_df.loc[mask, fval_col],\n",
    "            -np.log10(results_df.loc[mask, pval_col]),\n",
    "            c=results_df.loc[mask, sig_col].map({True: 'red', False: 'black'}),\n",
    "            alpha=0.5,\n",
    "            s=15\n",
    "        )\n",
    "        \n",
    "        plt.axhline(-np.log10(0.05), linestyle='--', color='red', label='p=0.05')\n",
    "        plt.xlabel('F value (effect size)')\n",
    "        plt.ylabel('-log10(p-value)')\n",
    "        plt.title(f'Volcano Plot: {name}')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(f\"{plots_dir}/volcano_{name.lower().replace(' ', '_')}.png\", dpi=300)\n",
    "        figures[f'volcano_{name.lower().replace(\" \", \"_\")}'] = plt.gcf()\n",
    "    \n",
    "    # 3. Distribution of p-values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create histograms for each factor\n",
    "    bins = np.linspace(0, 1, 21)  # 20 bins from 0 to 1\n",
    "    \n",
    "    for i, (name, col) in enumerate(zip(\n",
    "        [factor1_name, factor2_name, 'Interaction'],\n",
    "        ['factor1_pvalue', 'factor2_pvalue', 'interaction_pvalue']\n",
    "    )):\n",
    "        # Skip NaN values\n",
    "        p_values = results_df[col].dropna()\n",
    "        if len(p_values) > 0:\n",
    "            plt.hist(p_values, bins=bins, alpha=0.5, label=name)\n",
    "    \n",
    "    plt.xlabel('p-value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of p-values')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f\"{plots_dir}/pvalue_distribution.png\", dpi=300)\n",
    "\n",
    "    figures['pvalue_distribution'] = plt.gcf()\n",
    "    \n",
    "    # 4. Pie charts for significant results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Extract counts\n",
    "    sig_factor1 = results_df['factor1_significant'].sum()\n",
    "    sig_factor2 = results_df['factor2_significant'].sum()\n",
    "    sig_interaction = results_df['interaction_significant'].sum()\n",
    "    total_bins = len(results_df)\n",
    "    \n",
    "    # Create subplots for pie charts\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.pie([sig_factor1, total_bins - sig_factor1], \n",
    "           labels=[f'Significant\\n{sig_factor1} ({sig_factor1/total_bins:.1%})', \n",
    "                  f'Not significant\\n{total_bins - sig_factor1} ({1-sig_factor1/total_bins:.1%})'],\n",
    "           colors=['red', 'lightgray'], autopct='%1.1f%%')\n",
    "    plt.title(f'Significant bins: {factor1_name}')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.pie([sig_factor2, total_bins - sig_factor2],\n",
    "           labels=[f'Significant\\n{sig_factor2} ({sig_factor2/total_bins:.1%})', \n",
    "                  f'Not significant\\n{total_bins - sig_factor2} ({1-sig_factor2/total_bins:.1%})'],\n",
    "           colors=['blue', 'lightgray'], autopct='%1.1f%%')\n",
    "    plt.title(f'Significant bins: {factor2_name}')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.pie([sig_interaction, total_bins - sig_interaction],\n",
    "           labels=[f'Significant\\n{sig_interaction} ({sig_interaction/total_bins:.1%})', \n",
    "                  f'Not significant\\n{total_bins - sig_interaction} ({1-sig_interaction/total_bins:.1%})'],\n",
    "           colors=['purple', 'lightgray'], autopct='%1.1f%%')\n",
    "    plt.title(f'Significant bins: Interaction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/significance_summary.png\", dpi=300)\n",
    "\n",
    "    figures['significance_summary'] = plt.gcf()\n",
    "    \n",
    "    # 5. Overlap between significant sets (Venn-like diagram)\n",
    "    # Create a DataFrame with all combinations of significance\n",
    "    sig_df = pd.DataFrame({\n",
    "        factor1_name: results_df['factor1_significant'],\n",
    "        factor2_name: results_df['factor2_significant'],\n",
    "        'Interaction': results_df['interaction_significant']\n",
    "    })\n",
    "    \n",
    "    # Create a summary table\n",
    "    summary = pd.DataFrame(columns=['Category', 'Count'])\n",
    "    \n",
    "    # Add total\n",
    "    summary = pd.concat([summary, pd.DataFrame({'Category': ['Total bins'], 'Count': [total_bins]})])\n",
    "    \n",
    "    # Add individual significances\n",
    "    for factor in [factor1_name, factor2_name, 'Interaction']:\n",
    "        count = sig_df[factor].sum()\n",
    "        summary = pd.concat([summary, pd.DataFrame({'Category': [f'Significant for {factor}'], 'Count': [count]})])\n",
    "    \n",
    "    # Add overlap counts\n",
    "    for i, f1 in enumerate([factor1_name, factor2_name, 'Interaction']):\n",
    "        for f2 in [factor1_name, factor2_name, 'Interaction'][i+1:]:\n",
    "            count = (sig_df[f1] & sig_df[f2]).sum()\n",
    "            summary = pd.concat([summary, pd.DataFrame({'Category': [f'Significant for both {f1} and {f2}'], 'Count': [count]})])\n",
    "    \n",
    "    # Add triple overlap\n",
    "    count = (sig_df[factor1_name] & sig_df[factor2_name] & sig_df['Interaction']).sum()\n",
    "    summary = pd.concat([summary, pd.DataFrame({'Category': ['Significant for all three factors'], 'Count': [count]})])\n",
    "    \n",
    "    # Save summary in current directory\n",
    "    summary.to_csv(f\"{output_prefix}_significance_summary.csv\", index=False)\n",
    "    \n",
    "    print(\"Visualizations complete. All plots saved to\", plots_dir)\n",
    "    \n",
    "    return figures\n",
    "\n",
    "def plot_significant_bins(file_path, results_df, factor1_name, factor2_name, output_prefix, max_bins=10):\n",
    "    \"\"\"\n",
    "    Plot significant bins with their oxidation patterns across factors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the original data file\n",
    "    results_df : pandas.DataFrame\n",
    "        ANOVA results from analyze_genomic_bins()\n",
    "    factor1_name : str\n",
    "        Name of the first factor\n",
    "    factor2_name : str\n",
    "        Name of the second factor\n",
    "    output_prefix : str\n",
    "        Prefix for output files\n",
    "    max_bins : int\n",
    "        Maximum number of top bins to plot\n",
    "    \"\"\"\n",
    "    # Process the data again to get the pivot table and metadata\n",
    "    pivot_df, bin_ids, metadata_df = preprocess_data(file_path)\n",
    "    \n",
    "    # Create directory for bin plots\n",
    "    bin_plots_dir = \"images/anova_results\"\n",
    "    os.makedirs(bin_plots_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Plotting top significant bins...\")\n",
    "    \n",
    "    # For each factor, plot the top significant bins\n",
    "    factor_cols = [\n",
    "        (factor1_name, 'factor1_pvalue', 'factor1_significant'),\n",
    "        (factor2_name, 'factor2_pvalue', 'factor2_significant'),\n",
    "        ('Interaction', 'interaction_pvalue', 'interaction_significant')\n",
    "    ]\n",
    "    \n",
    "    # Dictionary to store DataFrames with top bins\n",
    "    top_bins_dict = {}\n",
    "    \n",
    "    for name, pval_col, sig_col in factor_cols:\n",
    "        # Get significant bins\n",
    "        sig_bins = results_df[results_df[sig_col] == True].copy()\n",
    "        \n",
    "        if len(sig_bins) == 0:\n",
    "            print(f\"No significant bins found for {name}\")\n",
    "            continue\n",
    "            \n",
    "        # Sort by p-value\n",
    "        sig_bins = sig_bins.sort_values(pval_col)\n",
    "        \n",
    "        # Take top bins\n",
    "        top_bins = sig_bins.head(min(max_bins, len(sig_bins)))\n",
    "        top_bins_dict[f\"{name.lower()}_top_bins\"] = top_bins\n",
    "        \n",
    "        # Save to file in current directory\n",
    "        top_bins.to_csv(f\"{output_prefix}_{name.lower()}_top_bins.csv\", index=False)\n",
    "        \n",
    "        # Plot each top bin\n",
    "        for i, row in enumerate(top_bins.itertuples()):\n",
    "            try:\n",
    "                # Get the bin ID\n",
    "                bin_id = row.bin_id\n",
    "                \n",
    "                # Check if bin exists in pivot table\n",
    "                if bin_id not in pivot_df.columns:\n",
    "                    print(f\"Warning: Bin {bin_id} not found in data\")\n",
    "                    continue\n",
    "                \n",
    "                # Create a DataFrame for this bin\n",
    "                bin_data = pd.DataFrame({\n",
    "                    'Sample': pivot_df.index,\n",
    "                    'measurement': pivot_df[bin_id].values\n",
    "                })\n",
    "                \n",
    "                # Merge with metadata\n",
    "                bin_data = pd.merge(bin_data, metadata_df, on='Sample')\n",
    "                \n",
    "                # Create the plot\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                \n",
    "                # Use seaborn for better visualization\n",
    "                ax = sns.pointplot(data=bin_data, x=factor1_name, y='measurement', \n",
    "                                  hue=factor2_name, dodge=True, errorbar=('se', 1), \n",
    "                                  capsize=0.2)\n",
    "                \n",
    "                # Add title and p-values\n",
    "                plt.title(f\"Bin: {bin_id}\")\n",
    "                plt.suptitle(\n",
    "                    f\"{factor1_name} p = {getattr(row, f'factor1_pvalue'):.3e}, \"\n",
    "                    f\"{factor2_name} p = {getattr(row, f'factor2_pvalue'):.3e}, \"\n",
    "                    f\"Interaction p = {getattr(row, f'interaction_pvalue'):.3e}\",\n",
    "                    y=0.92, fontsize=9\n",
    "                )\n",
    "                \n",
    "                plt.ylabel('Oxidation Level')\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save the figure in images/anova_results\n",
    "                output_file = f\"{bin_plots_dir}/{name.lower()}_top{i+1}_{bin_id.replace(':', '_')}\"\n",
    "                plt.savefig(f\"{output_file}.png\", dpi=300)\n",
    "\n",
    "                plt.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting bin {bin_id}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Bin plots complete. All plots saved to {bin_plots_dir}\")\n",
    "    return top_bins_dict\n",
    "\n",
    "def create_heatmap(file_path, results_df, factor1_name, factor2_name, output_prefix, max_bins=50):\n",
    "    \"\"\"\n",
    "    Create heatmaps of the top significant bins\n",
    "    \"\"\"\n",
    "    # Process the data again to get the pivot table and metadata\n",
    "    pivot_df, bin_ids, metadata_df = preprocess_data(file_path)\n",
    "    \n",
    "    # Create directory for heatmaps\n",
    "    heatmap_dir = \"images/anova_results\"\n",
    "    os.makedirs(heatmap_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Creating heatmaps of top significant bins...\")\n",
    "    \n",
    "    # For each factor, create a heatmap of the top significant bins\n",
    "    factor_cols = [\n",
    "        (factor1_name, 'factor1_pvalue', 'factor1_significant'),\n",
    "        (factor2_name, 'factor2_pvalue', 'factor2_significant'),\n",
    "        ('Interaction', 'interaction_pvalue', 'interaction_significant')\n",
    "    ]\n",
    "    \n",
    "    for name, pval_col, sig_col in factor_cols:\n",
    "        # Get significant bins\n",
    "        sig_bins = results_df[results_df[sig_col] == True].copy()\n",
    "        \n",
    "        if len(sig_bins) == 0:\n",
    "            print(f\"No significant bins found for {name}, skipping heatmap.\")\n",
    "            continue\n",
    "            \n",
    "        # Sort by p-value\n",
    "        sig_bins = sig_bins.sort_values(pval_col)\n",
    "        \n",
    "        # Take top bins\n",
    "        top_bins = sig_bins.head(min(max_bins, len(sig_bins)))\n",
    "        \n",
    "        # Get the bin IDs\n",
    "        bin_ids = top_bins['bin_id'].tolist()\n",
    "        \n",
    "        # Filter pivot table to only include these bins\n",
    "        # Check if all bin IDs exist in the pivot table\n",
    "        existing_bins = [b for b in bin_ids if b in pivot_df.columns]\n",
    "        \n",
    "        if len(existing_bins) < 2:\n",
    "            print(f\"Not enough valid bins for {name} to create a heatmap, need at least 2.\")\n",
    "            continue\n",
    "            \n",
    "        bins_pivot = pivot_df[existing_bins]\n",
    "        \n",
    "        # Check for NaN values and handle them\n",
    "        if bins_pivot.isnull().values.any():\n",
    "            print(f\"Warning: NaN values found in the data for {name}. Filling with column means.\")\n",
    "            bins_pivot = bins_pivot.fillna(bins_pivot.mean())\n",
    "        \n",
    "        # Add the factors to the index\n",
    "        heatmap_df = bins_pivot.copy()\n",
    "        heatmap_df[factor1_name] = metadata_df.set_index('Sample')[factor1_name]\n",
    "        heatmap_df[factor2_name] = metadata_df.set_index('Sample')[factor2_name]\n",
    "        \n",
    "        # Sort by factors\n",
    "        heatmap_df = heatmap_df.sort_values([factor1_name, factor2_name])\n",
    "        \n",
    "        # Extract the factors for the row colors\n",
    "        row_colors = pd.DataFrame({\n",
    "            factor1_name: heatmap_df[factor1_name],\n",
    "            factor2_name: heatmap_df[factor2_name]\n",
    "        })\n",
    "        \n",
    "        # Remove the factors from the dataframe before plotting\n",
    "        heatmap_data = heatmap_df.drop([factor1_name, factor2_name], axis=1)\n",
    "        \n",
    "        # Check if we have enough data to cluster\n",
    "        if heatmap_data.shape[1] < 2 or heatmap_data.shape[0] < 2:\n",
    "            print(f\"Not enough data to create a heatmap for {name}.\")\n",
    "            continue\n",
    "            \n",
    "        # Create color map for factors\n",
    "        factor1_values = row_colors[factor1_name].unique()\n",
    "        factor2_values = row_colors[factor2_name].unique()\n",
    "        \n",
    "        factor1_cmap = dict(zip(factor1_values, sns.color_palette(\"Set1\", len(factor1_values))))\n",
    "        factor2_cmap = dict(zip(factor2_values, sns.color_palette(\"Set2\", len(factor2_values))))\n",
    "        \n",
    "        # Apply color maps\n",
    "        row_colors_mapped = pd.DataFrame({\n",
    "            factor1_name: row_colors[factor1_name].map(factor1_cmap),\n",
    "            factor2_name: row_colors[factor2_name].map(factor2_cmap)\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # Create the heatmap\n",
    "            plt.figure(figsize=(max(10, len(existing_bins) * 0.4), max(8, len(heatmap_data) * 0.4)))\n",
    "            \n",
    "            # Use clustermap for hierarchical clustering\n",
    "            g = sns.clustermap(\n",
    "                heatmap_data,\n",
    "                cmap=\"viridis\",\n",
    "                z_score=0,  # Z-score normalize the rows\n",
    "                row_colors=row_colors_mapped,\n",
    "                col_cluster=True,\n",
    "                row_cluster=False,\n",
    "                xticklabels=True,\n",
    "                yticklabels=False,\n",
    "                figsize=(max(10, len(existing_bins) * 0.4), max(8, len(heatmap_data) * 0.4))\n",
    "            )\n",
    "            \n",
    "            # Add title\n",
    "            plt.suptitle(f\"Heatmap of top {len(existing_bins)} bins significant for {name}\", y=1.02)\n",
    "            \n",
    "            # Create legends for factors\n",
    "            for factor, cmap, pos in zip(\n",
    "                [factor1_name, factor2_name],\n",
    "                [factor1_cmap, factor2_cmap],\n",
    "                [1.01, 1.01 + 0.05 * len(factor1_values)]\n",
    "            ):\n",
    "                for label, color in cmap.items():\n",
    "                    g.ax_row_dendrogram.bar(0, 0, color=color, label=f\"{factor}: {label}\", linewidth=0)\n",
    "                g.ax_row_dendrogram.legend(loc=\"center left\", ncol=1, bbox_to_anchor=(1, pos))\n",
    "            \n",
    "            # Save the figure in images/anova_results\n",
    "            plt.savefig(f\"{heatmap_dir}/{name.lower()}_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating heatmap for {name}: {str(e)}\")\n",
    "            # Try a simpler heatmap without clustering\n",
    "            try:\n",
    "                plt.figure(figsize=(max(10, len(existing_bins) * 0.4), max(8, len(heatmap_data) * 0.4)))\n",
    "                sns.heatmap(heatmap_data, cmap=\"viridis\", xticklabels=True, yticklabels=False)\n",
    "                plt.title(f\"Heatmap of top {len(existing_bins)} bins significant for {name}\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{heatmap_dir}/{name.lower()}_heatmap_simple.png\", dpi=300)\n",
    "\n",
    "                plt.close()\n",
    "            except Exception as e2:\n",
    "                print(f\"Could not create even a simple heatmap for {name}: {str(e2)}\")\n",
    "    \n",
    "    print(f\"Heatmaps complete. All heatmaps saved to {heatmap_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a21a007-4d08-47a0-a4b3-8233661a73e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from data_normalized/cleaned_Normalized_100000.csv...\n",
      "Detected bin size: 100000\n",
      "Sample of the processed dataframe:\n",
      "                       Sample       Bin Strand              FeatureID Group  \\\n",
      "0  Sample_14_CRS_evening_S14_       0.0      +       0.0_+NC_000067.7     0   \n",
      "1  Sample_14_CRS_evening_S14_       0.0      -       0.0_-NC_000067.7     0   \n",
      "2  Sample_14_CRS_evening_S14_  100000.0      +  100000.0_+NC_000067.7     0   \n",
      "3  Sample_14_CRS_evening_S14_  100000.0      -  100000.0_-NC_000067.7     0   \n",
      "4  Sample_14_CRS_evening_S14_  200000.0      +  200000.0_+NC_000067.7     0   \n",
      "\n",
      "   Median_Normalized_Damage  \n",
      "0                       0.0  \n",
      "1                       0.0  \n",
      "2                       0.0  \n",
      "3                       0.0  \n",
      "4                       0.0  \n",
      "Created pivot table with 20 samples and 54486 genomic features\n",
      "Extracted metadata with factors: treatment (['Ctrl' 'CRS']) and timepoint (['morning' 'evening'])\n",
      "                       Sample treatment timepoint\n",
      "0  Sample_01_Ctrl_morning_S1_      Ctrl   morning\n",
      "1   Sample_02_CRS_morning_S2_       CRS   morning\n",
      "2  Sample_03_Ctrl_morning_S3_      Ctrl   morning\n",
      "3   Sample_04_CRS_morning_S4_       CRS   morning\n",
      "4  Sample_05_Ctrl_morning_S5_      Ctrl   morning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>FeatureID</th>\n",
       "      <th>0.0_+NC_000067.7</th>\n",
       "      <th>0.0_+NC_000068.8</th>\n",
       "      <th>0.0_+NC_000069.7</th>\n",
       "      <th>0.0_+NC_000070.7</th>\n",
       "      <th>0.0_+NC_000071.7</th>\n",
       "      <th>0.0_+NC_000072.7</th>\n",
       "      <th>0.0_+NC_000073.7</th>\n",
       "      <th>0.0_+NC_000074.7</th>\n",
       "      <th>0.0_+NC_000075.7</th>\n",
       "      <th>0.0_+NC_000076.7</th>\n",
       "      <th>...</th>\n",
       "      <th>99900000.0_-NC_000073.7</th>\n",
       "      <th>99900000.0_-NC_000074.7</th>\n",
       "      <th>99900000.0_-NC_000075.7</th>\n",
       "      <th>99900000.0_-NC_000076.7</th>\n",
       "      <th>99900000.0_-NC_000077.7</th>\n",
       "      <th>99900000.0_-NC_000078.7</th>\n",
       "      <th>99900000.0_-NC_000079.7</th>\n",
       "      <th>99900000.0_-NC_000080.7</th>\n",
       "      <th>99900000.0_-NC_000081.7</th>\n",
       "      <th>99900000.0_-NC_000086.8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sample_01_Ctrl_morning_S1_</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849249</td>\n",
       "      <td>0.929127</td>\n",
       "      <td>1.282609</td>\n",
       "      <td>0.775262</td>\n",
       "      <td>0.943953</td>\n",
       "      <td>0.824291</td>\n",
       "      <td>1.054163</td>\n",
       "      <td>0.810096</td>\n",
       "      <td>1.004788</td>\n",
       "      <td>0.720863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_02_CRS_morning_S2_</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.944873</td>\n",
       "      <td>1.036385</td>\n",
       "      <td>1.051391</td>\n",
       "      <td>0.800325</td>\n",
       "      <td>1.147813</td>\n",
       "      <td>1.307092</td>\n",
       "      <td>1.216669</td>\n",
       "      <td>0.945022</td>\n",
       "      <td>1.055067</td>\n",
       "      <td>0.727344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_03_Ctrl_morning_S3_</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.015057</td>\n",
       "      <td>0.834947</td>\n",
       "      <td>1.252251</td>\n",
       "      <td>1.013523</td>\n",
       "      <td>0.874564</td>\n",
       "      <td>0.999333</td>\n",
       "      <td>1.050962</td>\n",
       "      <td>1.013886</td>\n",
       "      <td>0.977799</td>\n",
       "      <td>0.824327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_04_CRS_morning_S4_</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921588</td>\n",
       "      <td>0.743868</td>\n",
       "      <td>1.254160</td>\n",
       "      <td>0.697528</td>\n",
       "      <td>0.941538</td>\n",
       "      <td>0.838626</td>\n",
       "      <td>0.931485</td>\n",
       "      <td>0.739589</td>\n",
       "      <td>0.861289</td>\n",
       "      <td>0.613679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_05_Ctrl_morning_S5_</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905597</td>\n",
       "      <td>0.863649</td>\n",
       "      <td>1.333553</td>\n",
       "      <td>0.942068</td>\n",
       "      <td>0.954911</td>\n",
       "      <td>0.922569</td>\n",
       "      <td>1.187176</td>\n",
       "      <td>0.955254</td>\n",
       "      <td>0.815876</td>\n",
       "      <td>0.549317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54486 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "FeatureID                   0.0_+NC_000067.7  0.0_+NC_000068.8  \\\n",
       "Sample                                                           \n",
       "Sample_01_Ctrl_morning_S1_               0.0               0.0   \n",
       "Sample_02_CRS_morning_S2_                0.0               0.0   \n",
       "Sample_03_Ctrl_morning_S3_               0.0               0.0   \n",
       "Sample_04_CRS_morning_S4_                0.0               0.0   \n",
       "Sample_05_Ctrl_morning_S5_               0.0               0.0   \n",
       "\n",
       "FeatureID                   0.0_+NC_000069.7  0.0_+NC_000070.7  \\\n",
       "Sample                                                           \n",
       "Sample_01_Ctrl_morning_S1_               0.0               0.0   \n",
       "Sample_02_CRS_morning_S2_                0.0               0.0   \n",
       "Sample_03_Ctrl_morning_S3_               0.0               0.0   \n",
       "Sample_04_CRS_morning_S4_                0.0               0.0   \n",
       "Sample_05_Ctrl_morning_S5_               0.0               0.0   \n",
       "\n",
       "FeatureID                   0.0_+NC_000071.7  0.0_+NC_000072.7  \\\n",
       "Sample                                                           \n",
       "Sample_01_Ctrl_morning_S1_               0.0               0.0   \n",
       "Sample_02_CRS_morning_S2_                0.0               0.0   \n",
       "Sample_03_Ctrl_morning_S3_               0.0               0.0   \n",
       "Sample_04_CRS_morning_S4_                0.0               0.0   \n",
       "Sample_05_Ctrl_morning_S5_               0.0               0.0   \n",
       "\n",
       "FeatureID                   0.0_+NC_000073.7  0.0_+NC_000074.7  \\\n",
       "Sample                                                           \n",
       "Sample_01_Ctrl_morning_S1_               0.0               0.0   \n",
       "Sample_02_CRS_morning_S2_                0.0               0.0   \n",
       "Sample_03_Ctrl_morning_S3_               0.0               0.0   \n",
       "Sample_04_CRS_morning_S4_                0.0               0.0   \n",
       "Sample_05_Ctrl_morning_S5_               0.0               0.0   \n",
       "\n",
       "FeatureID                   0.0_+NC_000075.7  0.0_+NC_000076.7  ...  \\\n",
       "Sample                                                          ...   \n",
       "Sample_01_Ctrl_morning_S1_               0.0               0.0  ...   \n",
       "Sample_02_CRS_morning_S2_                0.0               0.0  ...   \n",
       "Sample_03_Ctrl_morning_S3_               0.0               0.0  ...   \n",
       "Sample_04_CRS_morning_S4_                0.0               0.0  ...   \n",
       "Sample_05_Ctrl_morning_S5_               0.0               0.0  ...   \n",
       "\n",
       "FeatureID                   99900000.0_-NC_000073.7  99900000.0_-NC_000074.7  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 0.849249                 0.929127   \n",
       "Sample_02_CRS_morning_S2_                  0.944873                 1.036385   \n",
       "Sample_03_Ctrl_morning_S3_                 1.015057                 0.834947   \n",
       "Sample_04_CRS_morning_S4_                  0.921588                 0.743868   \n",
       "Sample_05_Ctrl_morning_S5_                 0.905597                 0.863649   \n",
       "\n",
       "FeatureID                   99900000.0_-NC_000075.7  99900000.0_-NC_000076.7  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 1.282609                 0.775262   \n",
       "Sample_02_CRS_morning_S2_                  1.051391                 0.800325   \n",
       "Sample_03_Ctrl_morning_S3_                 1.252251                 1.013523   \n",
       "Sample_04_CRS_morning_S4_                  1.254160                 0.697528   \n",
       "Sample_05_Ctrl_morning_S5_                 1.333553                 0.942068   \n",
       "\n",
       "FeatureID                   99900000.0_-NC_000077.7  99900000.0_-NC_000078.7  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 0.943953                 0.824291   \n",
       "Sample_02_CRS_morning_S2_                  1.147813                 1.307092   \n",
       "Sample_03_Ctrl_morning_S3_                 0.874564                 0.999333   \n",
       "Sample_04_CRS_morning_S4_                  0.941538                 0.838626   \n",
       "Sample_05_Ctrl_morning_S5_                 0.954911                 0.922569   \n",
       "\n",
       "FeatureID                   99900000.0_-NC_000079.7  99900000.0_-NC_000080.7  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 1.054163                 0.810096   \n",
       "Sample_02_CRS_morning_S2_                  1.216669                 0.945022   \n",
       "Sample_03_Ctrl_morning_S3_                 1.050962                 1.013886   \n",
       "Sample_04_CRS_morning_S4_                  0.931485                 0.739589   \n",
       "Sample_05_Ctrl_morning_S5_                 1.187176                 0.955254   \n",
       "\n",
       "FeatureID                   99900000.0_-NC_000081.7  99900000.0_-NC_000086.8  \n",
       "Sample                                                                        \n",
       "Sample_01_Ctrl_morning_S1_                 1.004788                 0.720863  \n",
       "Sample_02_CRS_morning_S2_                  1.055067                 0.727344  \n",
       "Sample_03_Ctrl_morning_S3_                 0.977799                 0.824327  \n",
       "Sample_04_CRS_morning_S4_                  0.861289                 0.613679  \n",
       "Sample_05_Ctrl_morning_S5_                 0.815876                 0.549317  \n",
       "\n",
       "[5 rows x 54486 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>treatment</th>\n",
       "      <th>timepoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sample_01_Ctrl_morning_S1_</td>\n",
       "      <td>Ctrl</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sample_02_CRS_morning_S2_</td>\n",
       "      <td>CRS</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sample_03_Ctrl_morning_S3_</td>\n",
       "      <td>Ctrl</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sample_04_CRS_morning_S4_</td>\n",
       "      <td>CRS</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sample_05_Ctrl_morning_S5_</td>\n",
       "      <td>Ctrl</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Sample treatment timepoint\n",
       "0  Sample_01_Ctrl_morning_S1_      Ctrl   morning\n",
       "1   Sample_02_CRS_morning_S2_       CRS   morning\n",
       "2  Sample_03_Ctrl_morning_S3_      Ctrl   morning\n",
       "3   Sample_04_CRS_morning_S4_       CRS   morning\n",
       "4  Sample_05_Ctrl_morning_S5_      Ctrl   morning"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from data_normalized/cleaned_Normalized_100000.csv...\n",
      "Detected bin size: 100000\n",
      "Sample of the processed dataframe:\n",
      "                       Sample       Bin Strand              FeatureID Group  \\\n",
      "0  Sample_14_CRS_evening_S14_       0.0      +       0.0_+NC_000067.7     0   \n",
      "1  Sample_14_CRS_evening_S14_       0.0      -       0.0_-NC_000067.7     0   \n",
      "2  Sample_14_CRS_evening_S14_  100000.0      +  100000.0_+NC_000067.7     0   \n",
      "3  Sample_14_CRS_evening_S14_  100000.0      -  100000.0_-NC_000067.7     0   \n",
      "4  Sample_14_CRS_evening_S14_  200000.0      +  200000.0_+NC_000067.7     0   \n",
      "\n",
      "   Median_Normalized_Damage  \n",
      "0                       0.0  \n",
      "1                       0.0  \n",
      "2                       0.0  \n",
      "3                       0.0  \n",
      "4                       0.0  \n",
      "Created pivot table with 20 samples and 54486 genomic features\n",
      "Extracted metadata with factors: treatment (['Ctrl' 'CRS']) and timepoint (['morning' 'evening'])\n",
      "                       Sample treatment timepoint\n",
      "0  Sample_01_Ctrl_morning_S1_      Ctrl   morning\n",
      "1   Sample_02_CRS_morning_S2_       CRS   morning\n",
      "2  Sample_03_Ctrl_morning_S3_      Ctrl   morning\n",
      "3   Sample_04_CRS_morning_S4_       CRS   morning\n",
      "4  Sample_05_Ctrl_morning_S5_      Ctrl   morning\n",
      "Processing 54486 bins using 127 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 37548/54486 [13:45<06:13, 45.40it/s]"
     ]
    }
   ],
   "source": [
    "# 1. Load and preprocess data\n",
    "file_path = 'data_normalized/cleaned_Normalized_100000.csv'\n",
    "pivot_df, bin_ids, metadata_df = preprocess_data(file_path)\n",
    "\n",
    "# 2. View the first few rows of processed data\n",
    "display(pivot_df.head())\n",
    "display(metadata_df.head())\n",
    "\n",
    "# 3. Configure and run ANOVA analysis\n",
    "factor1_name = 'treatment'  # CRS vs Ctrl\n",
    "factor2_name = 'timepoint'  # morning vs evening\n",
    "output_prefix = 'oxidation_analysis'\n",
    "n_cores = 4  # Adjust based on your system\n",
    "\n",
    "# 4. Run the analysis\n",
    "results_df = analyze_genomic_bins(\n",
    "    file_path, factor1_name, factor2_name)\n",
    "\n",
    "# 5. Examine results\n",
    "display(results_df.head())\n",
    "print(f\"Significant for {factor1_name}: {results_df['factor1_significant'].sum()}\")\n",
    "print(f\"Significant for {factor2_name}: {results_df['factor2_significant'].sum()}\")\n",
    "print(f\"Significant for interaction: {results_df['interaction_significant'].sum()}\")\n",
    "\n",
    "# 6. Create visualizations\n",
    "visualize_anova_results(results_df, factor1_name, factor2_name, output_prefix)\n",
    "\n",
    "# 7. Visualize top significant bins\n",
    "plot_significant_bins(file_path, results_df, factor1_name, factor2_name, output_prefix)\n",
    "\n",
    "# 8. Create heatmaps\n",
    "create_heatmap(file_path, results_df, factor1_name, factor2_name, output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42eb646-34c7-4986-9c1e-754dbd78aca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55814504-40a0-4186-ade6-c6f75b626719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4ed4e-a50b-425a-9246-22c50418bc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67946bf5-8b73-489c-ba4c-53764e655aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
