{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82445ade-bd78-434f-a921-91a3e6dd51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import subprocess\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26097f1d-4ba4-45f0-83eb-562685966bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path with files outputted from bedtools intersect\n",
    "PATH = \"/cluster/scratch/taekim/data_oxidation/gene_bodies_intersect\" \n",
    "OUTPUT_FILE = \"../data_normalized/gene_bodies_Normalized.csv\"\n",
    "GENOME_PATH = \"/nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Navnit/genomes/mouse/GRCm39_NCBI_Bowtie2.fasta\"\n",
    "# Original path with all CpG coordinates\n",
    "ORG_PATH = \"/nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Vakil/mouse_genome_annotation/Genes_Promoters_CpG_islands_for_Tae/knownGenes_canononical_GRCm39_GENCODE.VM36.bed\" \n",
    "\n",
    "# Define column names for the input files\n",
    "INPUT_COLUMNS = [\"Chr1\", \"Start1\", \"End1\", \"Value\", \"MAPQ\", \"Chr2\", \"Start2\", \"End2\", \"Gene\", \"idk\", \"ignore\"]\n",
    "\n",
    "# Define columns for the output file\n",
    "OUTPUT_COLUMNS = [\"id\", \"sample\", \"gene\", \"strand\", \"GC_count\", \"damage\", \n",
    "                  \"GC_normalized_damage\", \"median\", \"median_normalized_damage\"]\n",
    "\n",
    "# Define the median values for each sample\n",
    "# Median value for each sample from 100kb bins\n",
    "median_values = {\n",
    "    \"Sample_14_CRS_evening_S14_\": 2.546314,\n",
    "    \"Sample_15_Ctrl_evening_S15_\": 3.570246,\n",
    "    \"Sample_05_Ctrl_morning_S5_\": 6.184096,\n",
    "    \"Sample_01_Ctrl_morning_S1_\": 6.921409,\n",
    "    \"Sample_16_CRS_evening_S16_\": 2.879618,\n",
    "    \"Sample_11_Ctrl_evening_S11_\": 2.485161,\n",
    "    \"Sample_13_Ctrl_evening_S13_\": 2.403964,\n",
    "    \"Sample_08_CRS_morning_S8_\": 5.246539,\n",
    "    \"Sample_20_CRS_evening_S20_\": 10.633666,\n",
    "    \"Sample_18_CRS_evening_S18_\": 4.602043,\n",
    "    \"Sample_17_Ctrl_evening_S17_\": 2.929046,\n",
    "    \"Sample_04_CRS_morning_S4_\": 6.227455,\n",
    "    \"Sample_19_Ctrl_evening_S19_\": 2.545900,\n",
    "    \"Sample_03_Ctrl_morning_S3_\": 9.529751,\n",
    "    \"Sample_02_CRS_morning_S2_\": 7.151634,\n",
    "    \"Sample_09_Ctrl_morning_S9_\": 7.923174,\n",
    "    \"Sample_10_CRS_morning_S10_\": 8.815860,\n",
    "    \"Sample_06_CRS_morning_S6_\": 7.930738,\n",
    "    \"Sample_12_CRS_evening_S12_\": 2.849694,\n",
    "    \"Sample_07_Ctrl_morning_S7_\": 4.924262\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4217a79c-cc57-4af1-ba70-9c55bf81cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: /cluster/software/stacks/2024-06/spack/opt/spack/linux-ubuntu22.04-x86_64_v3/gcc-12.2.0/bedtools2-2.31.0-a4obbslkxntgdx2criopqpwx662gcftq/bin/bedtools getfasta -fi /nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Navnit/genomes/mouse/GRCm39_NCBI_Bowtie2.fasta -bed /nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Vakil/mouse_genome_annotation/Genes_Promoters_CpG_islands_for_Tae/knownGenes_canononical_GRCm39_GENCODE.VM36.bed -bedOut > /cluster/scratch/taekim/data_oxidation/gene_bodies_intersect/gene_bodies.SEQ.bed\n",
      "Command completed successfully\n"
     ]
    }
   ],
   "source": [
    "def run_bedtools_getfasta(bed_file, output_file, genome_path):\n",
    "    \"\"\"Run bedtools getfasta command to get sequence data for a BED file\"\"\"\n",
    "    command = f\"/cluster/software/stacks/2024-06/spack/opt/spack/linux-ubuntu22.04-x86_64_v3/gcc-12.2.0/bedtools2-2.31.0-a4obbslkxntgdx2criopqpwx662gcftq/bin/bedtools getfasta -fi {genome_path} -bed {bed_file} -bedOut > {output_file}\"\n",
    "    print(f\"Running command: {command}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command, shell=True, check=True, \n",
    "                               stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
    "                               text=True)\n",
    "        print(\"Command completed successfully\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running bedtools: {e}\")\n",
    "        print(f\"STDERR: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "SEQ_COLUMNS = [\"Chr\", \"Start\", \"End\", \"Gene\", \"idk\", \"ignore\", \"Seq\"]\n",
    "seq_output_file = os.path.join(PATH, \"gene_bodies.SEQ.bed\")\n",
    "# Run getfasta with original CpG coordinate file\n",
    "run_bedtools_getfasta(ORG_PATH, seq_output_file, GENOME_PATH)\n",
    "seq_df = pd.read_csv(seq_output_file, sep=\"\\t\", header=None, names=SEQ_COLUMNS)\n",
    "\n",
    "seq_counts_plus = seq_df.copy()\n",
    "seq_counts_plus['id'] = seq_counts_plus['Gene'] + '+'  # Use Gene for consistency with previous code\n",
    "seq_counts_plus['GC_count'] = seq_counts_plus['Seq'].str.upper().str.count('G')\n",
    "seq_counts_plus = seq_counts_plus.drop_duplicates('id')\n",
    "\n",
    "seq_counts_minus = seq_df.copy()\n",
    "seq_counts_minus['id'] = seq_counts_minus['Gene'] + '-'  # Use Gene for consistency with previous code\n",
    "seq_counts_minus['GC_count'] = seq_counts_minus['Seq'].str.upper().str.count('C')\n",
    "seq_counts_minus = seq_counts_minus.drop_duplicates('id')\n",
    "\n",
    "# Combine both strand data\n",
    "seq_counts = pd.concat([seq_counts_plus, seq_counts_minus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6ed40-ed50-40cf-9d7f-435a0eadabc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass: Collecting all unique IDs...\n"
     ]
    }
   ],
   "source": [
    "# Collect all files for each sample\n",
    "bed_files = glob.glob(os.path.join(PATH, \"*strand.bed\"))\n",
    "\n",
    "all_unique_ids = set()  # To store all unique IDs across all samples\n",
    "# First pass: Collect all unique IDs across all samples\n",
    "print(\"First pass: Collecting all unique IDs...\")\n",
    "for file_path in sorted(bed_files):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        # Read the file\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=INPUT_COLUMNS,  on_bad_lines='skip')\n",
    "\n",
    "\n",
    "        if \"plus_strand\" in file_name:\n",
    "            strand = \"+\"\n",
    "        elif \"minus_strand\" in file_name:\n",
    "            strand = \"-\"        \n",
    "        # Create the identifier\n",
    "        ids = df['Gene'] + strand\n",
    "        \n",
    "        # Add to the set of all unique IDs\n",
    "        all_unique_ids.update(ids.unique())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error collecting IDs from {file_name}: {str(e)}\")\n",
    "\n",
    "print(f\"Collected {len(all_unique_ids)} unique IDs across all samples\")\n",
    "\n",
    "\n",
    "# Create a dictionary to store chromosome info for each ID\n",
    "id_to_chrom = {}\n",
    "for file_path in sorted(bed_files):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=INPUT_COLUMNS)\n",
    "        # Map each ID to its chromosome\n",
    "        id_df = df.copy()\n",
    "        \n",
    "        if \"plus_strand\" in file_name:\n",
    "            strand = \"+\"\n",
    "        elif \"minus_strand\" in file_name:\n",
    "            strand = \"-\"   \n",
    "            \n",
    "        id_df['id'] = id_df['Gene'] + strand\n",
    "        \n",
    "        for _, row in id_df.drop_duplicates('id').iterrows():\n",
    "            id_to_chrom[row['id']] = row['Chr1']\n",
    "    except Exception as e:\n",
    "        print(f\"  Error mapping chromosomes: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862cee5-bcc8-4547-a834-be4e82e0f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame for the final output\n",
    "all_data = pd.DataFrame(columns=OUTPUT_COLUMNS)\n",
    "\n",
    "print(\"Starting processing of samples\")\n",
    "\n",
    "# Process each file\n",
    "sample_data_dict = {}  # Store data for each sample\n",
    "\n",
    "print(\"starting\")\n",
    "for file_path in sorted(bed_files):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    \n",
    "    # Extract sample name and determine strand\n",
    "    sample_name = file_name.split('.')[0]\n",
    "    \n",
    "    if \"plus_strand\" in file_name:\n",
    "        strand = \"+\"\n",
    "        nucleotide = 'G'  # Count G on plus strand\n",
    "    elif \"minus_strand\" in file_name:\n",
    "        strand = \"-\"\n",
    "        nucleotide = 'C'  # Count C on minus strand\n",
    "    else:\n",
    "        print(f\"  Warning: Could not determine strand for {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Read and process the file\n",
    "        # dataframe of intersection from oxidation sites per sample and cpg coordinates\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=INPUT_COLUMNS)\n",
    "        print(f\"  Records: {len(df)}\")\n",
    "        \n",
    "        # Create IDs and calculate damage per genomic position\n",
    "        df['id'] = df['Gene'] + strand \n",
    "        \n",
    "        # Create damage_df with both summed damage and first gene in one operation\n",
    "        damage_df = df.groupby('id').agg({\n",
    "            'Value': 'sum',\n",
    "            'Gene': 'first',\n",
    "            'Chr1': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Rename Value column to damage\n",
    "        damage_df = damage_df.rename(columns={'Value': 'damage'})\n",
    "        damage_df = damage_df.rename(columns={'Gene': 'gene'})\n",
    "        damage_df = damage_df.rename(columns={'Chr1': 'chromosome'})\n",
    "        \n",
    "        # Create base result dataframe\n",
    "        file_data = damage_df.copy()\n",
    "        file_data['sample'] = sample_name\n",
    "        file_data['strand'] = strand\n",
    "        \n",
    "        # 4. Merge the counts with our result dataframe\n",
    "        file_data = file_data.merge(\n",
    "            seq_counts[['id', 'GC_count']], \n",
    "            on='id', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Calculate normalized damage\n",
    "        file_data['GC_normalized_damage'] = file_data['damage'] / file_data['GC_count'].replace(0, np.nan)\n",
    "        \n",
    "        # Add median normalization if available\n",
    "        if sample_name in median_values:\n",
    "            file_data['median'] = median_values[sample_name]\n",
    "            file_data['median_normalized_damage'] = 1000 * file_data['GC_normalized_damage'] / file_data['median']\n",
    "        else:\n",
    "            print(f\"  Warning: No median value found for sample {sample_name}\")\n",
    "            file_data['median'] = np.nan\n",
    "            file_data['median_normalized_damage'] = np.nan\n",
    "        \n",
    "        # Store processed data\n",
    "        # file_data saved per sample\n",
    "        sample_data_dict[f\"{sample_name}_{strand}\"] = file_data\n",
    "        print(f\"File data: {len(file_data)}\")\n",
    "        print (file_data.head(1))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {file_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34f245-69af-449e-be0c-b9d8f698cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing IDs for each sample\n",
    "print(\"Adding missing IDs to each sample...\")\n",
    "for sample_key, file_data in sample_data_dict.items():\n",
    "    sample_name = sample_key.rsplit('_', 1)[0]\n",
    "\n",
    "    print (sample_key)\n",
    "    \n",
    "    # Identify missing IDs for this sample\n",
    "    existing_ids = set(file_data['id'])\n",
    "    print (f\"There are {len(existing_ids)} unique ids\")\n",
    "    missing_ids = all_unique_ids - existing_ids\n",
    "    \n",
    "    if missing_ids:\n",
    "        print(f\"  Adding {len(missing_ids)} missing IDs to sample {sample_name} (strand {strand})\")\n",
    "        \n",
    "        # Create rows for missing IDs\n",
    "        missing_rows = []\n",
    "        for missing_id in missing_ids:\n",
    "            # Get the chromosome for this ID\n",
    "            \n",
    "            # Create a row with zeros for all numerical values\n",
    "            missing_row = {\n",
    "                'id': missing_id,\n",
    "                'sample': sample_name,\n",
    "                'gene': missing_id[:-1],\n",
    "                'strand': missing_id[-1],\n",
    "                'GC_count': 0,\n",
    "                'damage': 0,\n",
    "                'GC_normalized_damage': 0,\n",
    "                'chromosome': id_to_chrom[missing_id]\n",
    "            }\n",
    "\n",
    "            \n",
    "            # Add median value if available\n",
    "            if sample_name in median_values:\n",
    "                missing_row['median'] = median_values[sample_name]\n",
    "                missing_row['median_normalized_damage'] = 0\n",
    "            else:\n",
    "                missing_row['median'] = np.nan\n",
    "                missing_row['median_normalized_damage'] = np.nan\n",
    "                \n",
    "            missing_rows.append(missing_row)\n",
    "        \n",
    "        # Add the missing rows to the sample's data\n",
    "        if missing_rows:\n",
    "            missing_df = pd.DataFrame(missing_rows)\n",
    "            file_data = pd.concat([file_data, missing_df], ignore_index=True)\n",
    "            sample_data_dict[sample_key] = file_data\n",
    "       \n",
    "        print (file_data.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bed0c7e3-318e-411e-a922-5d34380e88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.32068806.taekim/ipykernel_2431266/224940275.py:3: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, file_data], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! Combined data saved to ../data_normalized/gene_bodies_Normalized.csv\n",
      "Total records: 6089440\n"
     ]
    }
   ],
   "source": [
    "# Combine all samples into the final DataFrame\n",
    "for sample_key, file_data in sample_data_dict.items():\n",
    "    all_data = pd.concat([all_data, file_data], ignore_index=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "all_data.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Processing complete! Combined data saved to {OUTPUT_FILE}\")\n",
    "print(f\"Total records: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200caeaf-d53e-4357-8e5f-82958991df49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
