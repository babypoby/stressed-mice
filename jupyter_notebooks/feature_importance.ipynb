{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbfee521-d2b6-40bf-b692-0870dd197f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/taekim/stressed_mice/jupyter_notebooks\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a5419e8-6f35-4085-a690-b129a551b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def run_random_forest_feature_selection_by_factor(input_file_path, output_dir=None, top_n=50):\n",
    "    \"\"\"\n",
    "    Perform Random Forest feature selection analysis on genomic data, \n",
    "    with separate analyses for each factor of interest.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file_path : str\n",
    "        Path to the input CSV file with genomic data\n",
    "    output_dir : str, optional\n",
    "        Directory where output files will be saved. If None, uses the current directory.\n",
    "    top_n : int, optional\n",
    "        Number of top features to highlight in detailed analysis, default is 50\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results_dict : dict\n",
    "        Dictionary containing results for each factor analysis\n",
    "    importance_dict : dict\n",
    "        Dictionary containing feature importance DataFrames for each factor\n",
    "    \"\"\"\n",
    "    # Set default output directory if none provided\n",
    "    if output_dir is None:\n",
    "        output_dir = ''\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define a mapping from NCBI RefSeq accessions to UCSC chromosome names\n",
    "    # Based on standard mouse genome mapping (assuming this is mouse data from the RefSeq IDs)\n",
    "    chrom_dict = {\n",
    "        'NC_000067.7': 'chr1',\n",
    "        'NC_000067.8': 'chr1',\n",
    "        'NC_000068.7': 'chr2',\n",
    "        'NC_000068.8': 'chr2',\n",
    "        'NC_000069.7': 'chr3',\n",
    "        'NC_000069.8': 'chr3',\n",
    "        'NC_000070.7': 'chr4',\n",
    "        'NC_000070.8': 'chr4',\n",
    "        'NC_000071.7': 'chr5',\n",
    "        'NC_000071.8': 'chr5',\n",
    "        'NC_000072.7': 'chr6',\n",
    "        'NC_000072.8': 'chr6',\n",
    "        'NC_000073.7': 'chr7',\n",
    "        'NC_000073.8': 'chr7',\n",
    "        'NC_000074.7': 'chr8',\n",
    "        'NC_000074.8': 'chr8',\n",
    "        'NC_000075.7': 'chr9',\n",
    "        'NC_000075.8': 'chr9',\n",
    "        'NC_000076.7': 'chr10',\n",
    "        'NC_000076.8': 'chr10',\n",
    "        'NC_000077.7': 'chr11',\n",
    "        'NC_000077.8': 'chr11',\n",
    "        'NC_000078.7': 'chr12',\n",
    "        'NC_000078.8': 'chr12',\n",
    "        'NC_000079.7': 'chr13',\n",
    "        'NC_000079.8': 'chr13',\n",
    "        'NC_000080.7': 'chr14',\n",
    "        'NC_000080.8': 'chr14',\n",
    "        'NC_000081.7': 'chr15',\n",
    "        'NC_000081.8': 'chr15',\n",
    "        'NC_000082.7': 'chr16',\n",
    "        'NC_000082.8': 'chr16',\n",
    "        'NC_000083.7': 'chr17',\n",
    "        'NC_000083.8': 'chr17',\n",
    "        'NC_000084.7': 'chr18',\n",
    "        'NC_000084.8': 'chr18',\n",
    "        'NC_000085.7': 'chr19',\n",
    "        'NC_000085.8': 'chr19',\n",
    "        'NC_000086.8': 'chrX',\n",
    "        'NC_000086.9': 'chrX',\n",
    "        'NC_000087.8': 'chrY',\n",
    "        'NC_000087.9': 'chrY',\n",
    "        'NC_005089.1': 'chrM'\n",
    "    }\n",
    "    \n",
    "    # Function to parse bin_id and create new UCSC style ID\n",
    "    def create_ucsc_style_id(bin_id):\n",
    "        match = re.match(r'^(\\d+\\.\\d+)_([+-])(.+)$', bin_id)\n",
    "        if match:\n",
    "            index = float(match.group(1))\n",
    "            strand = match.group(2)\n",
    "            ref_seq = match.group(3)\n",
    "            \n",
    "            if ref_seq in chrom_dict:\n",
    "                # Convert index to integer\n",
    "                return f\"{chrom_dict[ref_seq]}{strand}{int(index)}\"\n",
    "        return bin_id  # Return original if conversion fails\n",
    "\n",
    "    # Read the data\n",
    "    print(f\"Reading data from {input_file_path}...\")\n",
    "    df = pd.read_csv(input_file_path)\n",
    "\n",
    "    # Create feature IDs first\n",
    "    df['FeatureID'] = df['Bin'].astype(str) + '_' + df['Strand'] + df['Chromosome'].astype(str)\n",
    "\n",
    "    # Now convert these IDs to UCSC style\n",
    "    df['UCSCFeatureID'] = df['FeatureID'].apply(create_ucsc_style_id)\n",
    "\n",
    "    # Print some examples to verify the conversion\n",
    "    print(\"\\nExample UCSC Feature IDs:\")\n",
    "    for i, (orig, ucsc) in enumerate(zip(df['FeatureID'], df['UCSCFeatureID'])):\n",
    "        if i < 5:  # Show first 5 examples\n",
    "            print(f\"  Original: {orig} → UCSC: {ucsc}\")\n",
    "\n",
    "    # Create pivot table with UCSC-style IDs\n",
    "    pivot_df = df.pivot_table(index='Sample', \n",
    "                             columns='UCSCFeatureID', \n",
    "                             values='Median_Normalized_Damage',\n",
    "                             aggfunc='mean')\n",
    "\n",
    "    print(f\"Data shape: {pivot_df.shape} (samples × features)\")\n",
    "\n",
    "    # Extract sample groups and components\n",
    "    def extract_group_components(sample_name):\n",
    "        match = re.search(r'(CRS|Ctrl)_(evening|morning)', sample_name)\n",
    "        if match:\n",
    "            treatment = match.group(1)  # CRS or Ctrl\n",
    "            time = match.group(2)       # evening or morning\n",
    "            full_group = match.group(0)  # e.g., \"CRS_evening\"\n",
    "            return full_group, treatment, time\n",
    "        else:\n",
    "            return \"Unknown\", \"Unknown\", \"Unknown\"\n",
    "\n",
    "    # Create target variables\n",
    "    samples = pivot_df.index\n",
    "    sample_info = [extract_group_components(sample) for sample in samples]\n",
    "    full_groups = [info[0] for info in sample_info]\n",
    "    treatments = [info[1] for info in sample_info]\n",
    "    times = [info[2] for info in sample_info]\n",
    "\n",
    "    # Print group distribution\n",
    "    print(\"\\nSample group distribution:\")\n",
    "    for group, count in zip(*np.unique(full_groups, return_counts=True)):\n",
    "        print(f\"  {group}: {count} samples\")\n",
    "    \n",
    "    # Prepare feature data\n",
    "    X = pivot_df.values\n",
    "    X = np.nan_to_num(X, nan=0.0)  # Replace NaN with zero\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Define the factors to analyze\n",
    "    factor_analyses = {\n",
    "        'full_groups': full_groups,               # All groups (CRS_evening, CRS_morning, Ctrl_evening, Ctrl_morning)\n",
    "        'treatment': treatments,                  # CRS vs Ctrl (regardless of time)\n",
    "        'time': times,                            # evening vs morning (regardless of treatment)\n",
    "        'CRS_time': [t for i, t in enumerate(times) if treatments[i] == 'CRS'],  # Time effect within CRS\n",
    "        'Ctrl_time': [t for i, t in enumerate(times) if treatments[i] == 'Ctrl'],  # Time effect within Ctrl\n",
    "        'evening_treatment': [tr for i, tr in enumerate(treatments) if times[i] == 'evening'],  # Treatment effect in evening\n",
    "        'morning_treatment': [tr for i, tr in enumerate(treatments) if times[i] == 'morning']   # Treatment effect in morning\n",
    "    }\n",
    "    \n",
    "    # Filter indices for subset analyses\n",
    "    CRS_indices = [i for i, tr in enumerate(treatments) if tr == 'CRS']\n",
    "    Ctrl_indices = [i for i, tr in enumerate(treatments) if tr == 'Ctrl']\n",
    "    evening_indices = [i for i, t in enumerate(times) if t == 'evening']\n",
    "    morning_indices = [i for i, t in enumerate(times) if t == 'morning']\n",
    "    \n",
    "    factor_indices = {\n",
    "        'full_groups': list(range(len(full_groups))),\n",
    "        'treatment': list(range(len(treatments))),\n",
    "        'time': list(range(len(times))),\n",
    "        'CRS_time': CRS_indices,\n",
    "        'Ctrl_time': Ctrl_indices,\n",
    "        'evening_treatment': evening_indices,\n",
    "        'morning_treatment': morning_indices\n",
    "    }\n",
    "    \n",
    "    # Store results\n",
    "    results_dict = {}\n",
    "    importance_dict = {}\n",
    "    \n",
    "    # Run analysis for each factor\n",
    "    for factor_name, factor_values in factor_analyses.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Analyzing factor: {factor_name}\")\n",
    "        \n",
    "        # Get relevant indices for this analysis\n",
    "        indices = factor_indices[factor_name]\n",
    "        \n",
    "        # Skip if not enough samples\n",
    "        if len(indices) < 4:  # Need at least a few samples for meaningful analysis\n",
    "            print(f\"Skipping {factor_name} due to insufficient samples.\")\n",
    "            continue\n",
    "            \n",
    "        # Filter data\n",
    "        X_factor = X_scaled[indices]\n",
    "        y_factor = np.array(factor_values)\n",
    "        \n",
    "        # Skip if only one class\n",
    "        unique_classes = np.unique(y_factor)\n",
    "        if len(unique_classes) < 2:\n",
    "            print(f\"Skipping {factor_name} due to only one class: {unique_classes[0]}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Classes for {factor_name}: {unique_classes}\")\n",
    "        print(f\"Class distribution: {[np.sum(y_factor == c) for c in unique_classes]}\")\n",
    "        \n",
    "        # Encode labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y_factor)\n",
    "        \n",
    "        # Train Random Forest\n",
    "        print(f\"Training Random Forest for {factor_name}...\")\n",
    "        rf = RandomForestClassifier(n_estimators=500, random_state=42, \n",
    "                                   class_weight='balanced', n_jobs=20)\n",
    "        rf.fit(X_factor, y_encoded)\n",
    "        \n",
    "        # Create importance dataframe\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': pivot_df.columns,\n",
    "            'Importance': rf.feature_importances_\n",
    "        })\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Extract genomic information from UCSC-style IDs\n",
    "        def extract_ucsc_info(ucsc_id):\n",
    "            # Pattern to match UCSC style IDs like chr1+12345\n",
    "            match = re.match(r'^(chr[^+-]+)([+-])(\\d+)$', ucsc_id)\n",
    "            if match:\n",
    "                chrom = match.group(1)\n",
    "                strand = match.group(2)\n",
    "                position = int(match.group(3))\n",
    "                return chrom, strand, position\n",
    "            else:\n",
    "                # For non-matching IDs, try to extract using original pattern\n",
    "                match = re.match(r'^(\\d+\\.\\d+)_([+-])(.+)$', ucsc_id)\n",
    "                if match:\n",
    "                    bin_val = match.group(1)\n",
    "                    strand = match.group(2)\n",
    "                    chrom = match.group(3)\n",
    "                    return chrom, strand, bin_val\n",
    "                return \"Unknown\", \"?\", \"Unknown\"\n",
    "        \n",
    "        # Add genomic information to importance dataframe\n",
    "        importance_df['Chromosome'] = importance_df['Feature'].apply(lambda x: extract_ucsc_info(x)[0])\n",
    "        importance_df['Strand'] = importance_df['Feature'].apply(lambda x: extract_ucsc_info(x)[1])\n",
    "        importance_df['Position'] = importance_df['Feature'].apply(lambda x: extract_ucsc_info(x)[2])\n",
    "        \n",
    "        # Replace with this:\n",
    "        # Add percentile and cumulative importance to ALL features\n",
    "        importance_df['Percentile'] = importance_df['Importance'].rank(pct=True) * 100\n",
    "        importance_df['Cumulative_Importance'] = importance_df['Importance'].cumsum() / importance_df['Importance'].sum() * 100\n",
    "\n",
    "        # Save all features with enhanced information in one file\n",
    "        all_features_file = os.path.join(output_dir, f'{factor_name}_all_features_importance.csv')\n",
    "        importance_df.to_csv(all_features_file, index=False)\n",
    "        print(f\"Saved all features with importance metrics for {factor_name} to {all_features_file}\")\n",
    "\n",
    "        top_n_features = importance_df.head(top_n)\n",
    "        \n",
    "        # Print top 10 features\n",
    "        print(f\"Top 10 Features for {factor_name}:\")\n",
    "        for i, (feature, importance) in enumerate(zip(top_n_features['Feature'].head(10), \n",
    "                                                    top_n_features['Importance'].head(10))):\n",
    "            print(f\"  {i+1}. {feature}: {importance:.6f}\")\n",
    "        \n",
    "        # Create a visualization\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.barh(np.arange(min(top_n, len(top_n_features))), \n",
    "                top_n_features['Importance'], align='center')\n",
    "        plt.yticks(np.arange(min(top_n, len(top_n_features))), \n",
    "                  [f\"{i+1}. {f}\" for i, f in enumerate(top_n_features['Feature'])])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Top {top_n} Features for {factor_name}')\n",
    "        plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        top_n_plot_file = os.path.join(output_dir, f'{factor_name}_top_{top_n}_features_plot.png')\n",
    "        plt.savefig(top_n_plot_file, dpi=300)\n",
    "        print(f\"Saved feature importance plot to {top_n_plot_file}\")\n",
    "        \n",
    "        # Store results\n",
    "        results_dict[factor_name] = {\n",
    "            'classes': label_encoder.classes_,\n",
    "            'top_features': top_n_features.head(10)['Feature'].tolist(),\n",
    "            'n_samples': len(y_factor)\n",
    "        }\n",
    "        importance_dict[factor_name] = importance_df\n",
    "    \n",
    "    # Create a comparison plot for the top features across factors\n",
    "    if len(importance_dict) > 1:\n",
    "        print(\"\\nCreating comparison of top features across factors...\")\n",
    "        \n",
    "        # Get top 5 features from each factor\n",
    "        top_features_by_factor = {}\n",
    "        all_top_features = set()\n",
    "        \n",
    "        for factor, imp_df in importance_dict.items():\n",
    "            top_features = imp_df.head(5)['Feature'].tolist()\n",
    "            top_features_by_factor[factor] = top_features\n",
    "            all_top_features.update(top_features)\n",
    "        \n",
    "        # Create comparison dataframe\n",
    "        comparison_data = []\n",
    "        for feature in all_top_features:\n",
    "            row = {'Feature': feature}\n",
    "            for factor, imp_df in importance_dict.items():\n",
    "                feature_imp = imp_df.loc[imp_df['Feature'] == feature, 'Importance'].values\n",
    "                row[factor] = feature_imp[0] if len(feature_imp) > 0 else 0\n",
    "            comparison_data.append(row)\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_df = comparison_df.sort_values('Feature')\n",
    "        print(f\"\\nNumber of unique features in the comparison heatmap: {len(comparison_df)}\")\n",
    "        \n",
    "        # Save comparison\n",
    "        comparison_file = os.path.join(output_dir, 'top_features_comparison.csv')\n",
    "        comparison_df.to_csv(comparison_file, index=False)\n",
    "        print(f\"Saved feature comparison to {comparison_file}\")\n",
    "        \n",
    "        # Create heatmap visualization\n",
    "        feature_cols = comparison_df.columns[1:]  # Skip 'Feature' column\n",
    "        data_for_heatmap = comparison_df.set_index('Feature')[feature_cols]\n",
    "        \n",
    "        plt.figure(figsize=(14, len(all_top_features) * 0.5 + 3))\n",
    "        sns.heatmap(data_for_heatmap, annot=True, cmap='viridis', fmt='.4f')\n",
    "        plt.title('Feature Importance Comparison Across Factors')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save heatmap\n",
    "        heatmap_file = os.path.join(output_dir, 'feature_importance_comparison_heatmap.png')\n",
    "        plt.savefig(heatmap_file, dpi=300)\n",
    "        print(f\"Saved comparison heatmap to {heatmap_file}\")\n",
    "    \n",
    "    return results_dict, importance_dict\n",
    "\n",
    "# For cross-comparison of feature importance across factors\n",
    "def feature_importance_comparison(importance_dict, output_dir=None, n_features=20):\n",
    "    \"\"\"\n",
    "    Compare feature importance across different factors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    importance_dict : dict\n",
    "        Dictionary of feature importance DataFrames from run_random_forest_feature_selection_by_factor\n",
    "    output_dir : str, optional\n",
    "        Directory to save outputs\n",
    "    n_features : int, optional\n",
    "        Number of top features to include in the comparison\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    comparison_df : pandas.DataFrame\n",
    "        DataFrame with feature importance comparison\n",
    "    \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = ''\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Collect top features from all factors\n",
    "    top_features = set()\n",
    "    for factor, imp_df in importance_dict.items():\n",
    "        top_features.update(imp_df.head(n_features)['Feature'].tolist())\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for feature in top_features:\n",
    "        row = {'Feature': feature}\n",
    "        # Get importance for this feature across all factors\n",
    "        for factor, imp_df in importance_dict.items():\n",
    "            match = imp_df[imp_df['Feature'] == feature]\n",
    "            if not match.empty:\n",
    "                row[f\"{factor}_Importance\"] = match['Importance'].values[0]\n",
    "                row[f\"{factor}_Rank\"] = match.index[0] + 1  # Add 1 for 1-based indexing\n",
    "            else:\n",
    "                row[f\"{factor}_Importance\"] = 0\n",
    "                row[f\"{factor}_Rank\"] = np.nan\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Sort by maximum importance across all factors\n",
    "    importance_cols = [col for col in comparison_df.columns if col.endswith('_Importance')]\n",
    "    comparison_df['Max_Importance'] = comparison_df[importance_cols].max(axis=1)\n",
    "    comparison_df = comparison_df.sort_values('Max_Importance', ascending=False)\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_file = os.path.join(output_dir, f'top_{n_features}_features_cross_factor_comparison.csv')\n",
    "    comparison_df.to_csv(comparison_file, index=False)\n",
    "    print(f\"Saved cross-factor feature comparison to {comparison_file}\")\n",
    "    \n",
    "    # Create visualization - focusing on top 20 features by maximum importance\n",
    "    viz_df = comparison_df.head(20).copy()\n",
    "    \n",
    "    # Reshape for better visualization\n",
    "    factor_names = [col.replace('_Importance', '') for col in importance_cols]\n",
    "    viz_data = []\n",
    "    \n",
    "    for _, row in viz_df.iterrows():\n",
    "        feature = row['Feature']\n",
    "        for factor in factor_names:\n",
    "            viz_data.append({\n",
    "                'Feature': feature,\n",
    "                'Factor': factor,\n",
    "                'Importance': row[f\"{factor}_Importance\"]\n",
    "            })\n",
    "    \n",
    "    viz_df_long = pd.DataFrame(viz_data)\n",
    "    \n",
    "    # Create heatmap\n",
    "    pivot_for_heat = viz_df_long.pivot(index='Feature', columns='Factor', values='Importance')\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(pivot_for_heat, annot=True, cmap='viridis', fmt='.4f')\n",
    "    plt.title(f'Feature Importance Comparison Across Factors (Top {len(pivot_for_heat)} Features)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save heatmap\n",
    "    heatmap_file = os.path.join(output_dir, f'top_{len(pivot_for_heat)}_features_importance_heatmap.png')\n",
    "    plt.savefig(heatmap_file, dpi=300)\n",
    "    print(f\"Saved importance heatmap to {heatmap_file}\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Example usage:\n",
    "# results_dict, importance_dict = run_random_forest_feature_selection_by_factor('genomic_data.csv', 'output_dir')\n",
    "# feature_importance_comparison(importance_dict, 'output_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5644d-477d-455a-8b8c-e783a3e845a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the enhanced analysis\n",
    "results_dict, importance_dict = run_random_forest_feature_selection_by_factor(\n",
    "    '../data_normalized/cleaned_Normalized_100000.csv', \n",
    "    '../data_rf/bin100000'\n",
    ")\n",
    "\n",
    "# Optionally run the additional cross-factor comparison\n",
    "comparison_df = feature_importance_comparison(\n",
    "    importance_dict, \n",
    "    '../data_rf/bin100000'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60cbe7-cfd5-4fde-8306-4c417e76ef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../data_normalized/cleaned_Normalized_10000.csv...\n",
      "\n",
      "Example UCSC Feature IDs:\n",
      "  Original: 3040000.0_-NC_000067.7 → UCSC: chr1-3040000\n",
      "  Original: 3050000.0_+NC_000067.7 → UCSC: chr1+3050000\n",
      "  Original: 3050000.0_-NC_000067.7 → UCSC: chr1-3050000\n",
      "  Original: 3060000.0_+NC_000067.7 → UCSC: chr1+3060000\n",
      "  Original: 3060000.0_-NC_000067.7 → UCSC: chr1-3060000\n",
      "Data shape: (20, 530255) (samples × features)\n",
      "\n",
      "Sample group distribution:\n",
      "  CRS_evening: 5 samples\n",
      "  CRS_morning: 5 samples\n",
      "  Ctrl_evening: 5 samples\n",
      "  Ctrl_morning: 5 samples\n",
      "\n",
      "==================================================\n",
      "Analyzing factor: full_groups\n",
      "Classes for full_groups: ['CRS_evening' 'CRS_morning' 'Ctrl_evening' 'Ctrl_morning']\n",
      "Class distribution: [5, 5, 5, 5]\n",
      "Training Random Forest for full_groups...\n",
      "Saved all features with importance metrics for full_groups to ../data_rf/bin10000/full_groups_all_features_importance.csv\n",
      "Top 10 Features for full_groups:\n",
      "  1. chr16+41560000: 0.001660\n",
      "  2. chr4-15710000: 0.001524\n",
      "  3. chr7+53350000: 0.001415\n",
      "  4. chr12-58700000: 0.001290\n",
      "  5. chrX-62270000: 0.001253\n",
      "  6. chr11-73070000: 0.001253\n",
      "  7. chr16+20590000: 0.001253\n",
      "  8. chr1+52760000: 0.001244\n",
      "  9. chr10+11370000: 0.001198\n",
      "  10. chr10-65840000: 0.001174\n",
      "Saved feature importance plot to ../data_rf/bin10000/full_groups_top_50_features_plot.png\n",
      "\n",
      "==================================================\n",
      "Analyzing factor: treatment\n",
      "Classes for treatment: ['CRS' 'Ctrl']\n",
      "Class distribution: [10, 10]\n",
      "Training Random Forest for treatment...\n",
      "Saved all features with importance metrics for treatment to ../data_rf/bin10000/treatment_all_features_importance.csv\n",
      "Top 10 Features for treatment:\n",
      "  1. chr2+38020000: 0.006000\n",
      "  2. chr1+86930000: 0.006000\n",
      "  3. chr3-149110000: 0.004000\n",
      "  4. chrX-151620000: 0.004000\n",
      "  5. chr16+84780000: 0.003636\n",
      "  6. chr17+7540000: 0.003630\n",
      "  7. chr18-31750000: 0.003615\n",
      "  8. chrX+130700000: 0.002364\n",
      "  9. chr10+83520000: 0.002000\n",
      "  10. chr13-115590000: 0.002000\n",
      "Saved feature importance plot to ../data_rf/bin10000/treatment_top_50_features_plot.png\n",
      "\n",
      "==================================================\n",
      "Analyzing factor: time\n",
      "Classes for time: ['evening' 'morning']\n",
      "Class distribution: [10, 10]\n",
      "Training Random Forest for time...\n",
      "Saved all features with importance metrics for time to ../data_rf/bin10000/time_all_features_importance.csv\n",
      "Top 10 Features for time:\n",
      "  1. chr10-6350000: 0.004000\n",
      "  2. chr6+66440000: 0.004000\n",
      "  3. chr13+73940000: 0.004000\n",
      "  4. chr6-127110000: 0.004000\n",
      "  5. chr9+26610000: 0.004000\n",
      "  6. chr10-30690000: 0.004000\n",
      "  7. chr10+30690000: 0.004000\n",
      "  8. chr1+148170000: 0.004000\n",
      "  9. chr9+112160000: 0.003636\n",
      "  10. chrY+37850000: 0.002000\n",
      "Saved feature importance plot to ../data_rf/bin10000/time_top_50_features_plot.png\n",
      "\n",
      "==================================================\n",
      "Analyzing factor: CRS_time\n",
      "Classes for CRS_time: ['evening' 'morning']\n",
      "Class distribution: [5, 5]\n",
      "Training Random Forest for CRS_time...\n",
      "Saved all features with importance metrics for CRS_time to ../data_rf/bin10000/CRS_time_all_features_importance.csv\n",
      "Top 10 Features for CRS_time:\n",
      "  1. chrX-15120000: 0.004016\n",
      "  2. chr2-13380000: 0.002008\n",
      "  3. chrY-31000000: 0.002008\n",
      "  4. chr1-178450000: 0.002008\n",
      "  5. chr6-26850000: 0.002008\n",
      "  6. chr17-16520000: 0.002008\n",
      "  7. chrY-26060000: 0.002008\n",
      "  8. chr2+23380000: 0.002008\n",
      "  9. chr5+45180000: 0.002008\n",
      "  10. chr16+16330000: 0.002008\n",
      "Saved feature importance plot to ../data_rf/bin10000/CRS_time_top_50_features_plot.png\n",
      "\n",
      "==================================================\n",
      "Analyzing factor: Ctrl_time\n",
      "Classes for Ctrl_time: ['evening' 'morning']\n",
      "Class distribution: [5, 5]\n",
      "Training Random Forest for Ctrl_time...\n",
      "Saved all features with importance metrics for Ctrl_time to ../data_rf/bin10000/Ctrl_time_all_features_importance.csv\n",
      "Top 10 Features for Ctrl_time:\n",
      "  1. chrY+18620000: 0.002008\n",
      "  2. chr3-94540000: 0.002008\n",
      "  3. chr13+118260000: 0.002008\n",
      "  4. chr2+155790000: 0.002008\n",
      "  5. chr9-80480000: 0.002008\n",
      "  6. chr18+33150000: 0.002008\n",
      "  7. chr12-23450000: 0.002008\n",
      "  8. chr16+70920000: 0.002008\n",
      "  9. chrY+47720000: 0.002008\n",
      "  10. chr2+156320000: 0.002008\n",
      "Saved feature importance plot to ../data_rf/bin10000/Ctrl_time_top_50_features_plot.png\n",
      "\n",
      "==================================================\n",
      "Analyzing factor: evening_treatment\n",
      "Classes for evening_treatment: ['CRS' 'Ctrl']\n",
      "Class distribution: [5, 5]\n",
      "Training Random Forest for evening_treatment...\n",
      "Saved all features with importance metrics for evening_treatment to ../data_rf/bin10000/evening_treatment_all_features_importance.csv\n",
      "Top 10 Features for evening_treatment:\n",
      "  1. chr2+40930000: 0.002008\n",
      "  2. chrX-60360000: 0.002008\n",
      "  3. chrX+77190000: 0.002008\n",
      "  4. chr2+118760000: 0.002008\n",
      "  5. chr2-10870000: 0.002008\n",
      "  6. chr11+114620000: 0.002008\n",
      "  7. chr1-124390000: 0.002008\n",
      "  8. chr16+89080000: 0.002008\n",
      "  9. chr14-99140000: 0.002008\n",
      "  10. chr9-42500000: 0.002008\n",
      "Saved feature importance plot to ../data_rf/bin10000/evening_treatment_top_50_features_plot.png\n",
      "\n",
      "==================================================\n",
      "Analyzing factor: morning_treatment\n",
      "Classes for morning_treatment: ['CRS' 'Ctrl']\n",
      "Class distribution: [5, 5]\n",
      "Training Random Forest for morning_treatment...\n",
      "Saved all features with importance metrics for morning_treatment to ../data_rf/bin10000/morning_treatment_all_features_importance.csv\n",
      "Top 10 Features for morning_treatment:\n",
      "  1. chr18+43540000: 0.002008\n",
      "  2. chr10-12290000: 0.002008\n",
      "  3. chr15+27320000: 0.002008\n",
      "  4. chr4+62630000: 0.002008\n",
      "  5. chr13+56180000: 0.002008\n",
      "  6. chr12-26220000: 0.002008\n",
      "  7. chr9-81730000: 0.002008\n",
      "  8. chr1+124040000: 0.002008\n",
      "  9. chr12-106040000: 0.002008\n",
      "  10. chr1-50000000: 0.002008\n",
      "Saved feature importance plot to ../data_rf/bin10000/morning_treatment_top_50_features_plot.png\n",
      "\n",
      "Creating comparison of top features across factors...\n",
      "\n",
      "Number of unique features in the comparison heatmap: 35\n",
      "Saved feature comparison to ../data_rf/bin10000/top_features_comparison.csv\n",
      "Saved comparison heatmap to ../data_rf/bin10000/feature_importance_comparison_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# Run the enhanced analysis\n",
    "results_dict, importance_dict = run_random_forest_feature_selection_by_factor(\n",
    "    '../data_normalized/cleaned_Normalized_10000.csv', \n",
    "    '../data_rf/bin10000'\n",
    ")\n",
    "\n",
    "# Optionally run the additional cross-factor comparison\n",
    "comparison_df = feature_importance_comparison(\n",
    "    importance_dict, \n",
    "    '../data_rf/bin10000'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da149a-46b0-4ce5-9aea-2ba0beabcda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the enhanced analysis\n",
    "results_dict, importance_dict = run_random_forest_feature_selection_by_factor(\n",
    "    '../data_normalized/cleaned_Normalized_1000.csv', \n",
    "    '../data_rf/bin1000'\n",
    ")\n",
    "\n",
    "# Optionally run the additional cross-factor comparison\n",
    "comparison_df = feature_importance_comparison(\n",
    "    importance_dict, \n",
    "    '../data_rf/bin1000'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
