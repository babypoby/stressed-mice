{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d38578b-7db1-4ad0-9650-d87e6902949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many modules are hidden in this stack. Use \"module --show_hidden spider SOFTWARE\" if you are not able to find the required software\n",
      "\n",
      "Inactive Modules:\n",
      "  1) python/3.11.6_cuda\n",
      "\n",
      "Due to MODULEPATH changes, the following have been reloaded:\n",
      "  1) hdf5/1.14.3     2) r/4.3.2\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) cuda/12.2.1 => cuda/12.8.0     3) stack/2024-05 => stack/2024-06\n",
      "  2) gcc/13.2.0 => gcc/12.2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module load stack/2024-06 gcc/12.2.0 bedtools2/2.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47402644-7fe7-4970-bcd4-d7a4b35a302c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/taekim/stressed_mice/jupyter_notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ac672-f744-4ebe-aead-6225e83a5456",
   "metadata": {},
   "source": [
    "                        Sample     Median\n",
    "0   Sample_14_CRS_evening_S14_   2.546314\n",
    "0  Sample_15_Ctrl_evening_S15_   3.570246\n",
    "0   Sample_05_Ctrl_morning_S5_   6.184096\n",
    "0   Sample_01_Ctrl_morning_S1_   6.921409\n",
    "0   Sample_16_CRS_evening_S16_   2.879618\n",
    "0  Sample_11_Ctrl_evening_S11_   2.485161\n",
    "0  Sample_13_Ctrl_evening_S13_   2.403964\n",
    "0    Sample_08_CRS_morning_S8_   5.246539\n",
    "0   Sample_20_CRS_evening_S20_  10.633666\n",
    "0   Sample_18_CRS_evening_S18_   4.602043\n",
    "0  Sample_17_Ctrl_evening_S17_   2.929046\n",
    "0    Sample_04_CRS_morning_S4_   6.227455\n",
    "0  Sample_19_Ctrl_evening_S19_   2.545900\n",
    "0   Sample_03_Ctrl_morning_S3_   9.529751\n",
    "0    Sample_02_CRS_morning_S2_   7.151634\n",
    "0   Sample_09_Ctrl_morning_S9_   7.923174\n",
    "0   Sample_10_CRS_morning_S10_   8.815860\n",
    "0    Sample_06_CRS_morning_S6_   7.930738\n",
    "0   Sample_12_CRS_evening_S12_   2.849694\n",
    "0   Sample_07_Ctrl_morning_S7_   4.924262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006a4543-3d87-475f-a3b1-38868734ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import subprocess\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a83b7c-304b-471f-82e0-b4abbd386555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path with files outputted from bedtools intersect\n",
    "PATH = \"/cluster/scratch/taekim/data_oxidation/cpg_intersect\" \n",
    "OUTPUT_FILE = \"all_samples_combined_data.csv\"\n",
    "GENOME_PATH = \"/nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Navnit/genomes/mouse/GRCm39_NCBI_Bowtie2.fasta\"\n",
    "# Original path with all CpG coordinates\n",
    "ORG_PATH = \"/nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Vakil/mouse_genome_annotation/Genes_Promoters_CpG_islands_for_Tae/allCpG_islands_GRCm39.bed\" \n",
    "\n",
    "# Define column names for the input files\n",
    "INPUT_COLUMNS = [\"Chr1\", \"Start1\", \"End1\", \"Value\", \"MAPQ\", \"Chr2\", \"Start2\", \"End2\"]\n",
    "\n",
    "# Define columns for the output file\n",
    "OUTPUT_COLUMNS = [\"id\", \"sample\", \"chromosome\", \"strand\", \"GC_count\", \"damage\", \n",
    "                  \"GC_normalized_damage\", \"median\", \"median_normalized_damage\"]\n",
    "\n",
    "# Define the median values for each sample\n",
    "# Median value for each sample from 100kb bins\n",
    "median_values = {\n",
    "    \"Sample_14_CRS_evening_S14_\": 2.546314,\n",
    "    \"Sample_15_Ctrl_evening_S15_\": 3.570246,\n",
    "    \"Sample_05_Ctrl_morning_S5_\": 6.184096,\n",
    "    \"Sample_01_Ctrl_morning_S1_\": 6.921409,\n",
    "    \"Sample_16_CRS_evening_S16_\": 2.879618,\n",
    "    \"Sample_11_Ctrl_evening_S11_\": 2.485161,\n",
    "    \"Sample_13_Ctrl_evening_S13_\": 2.403964,\n",
    "    \"Sample_08_CRS_morning_S8_\": 5.246539,\n",
    "    \"Sample_20_CRS_evening_S20_\": 10.633666,\n",
    "    \"Sample_18_CRS_evening_S18_\": 4.602043,\n",
    "    \"Sample_17_Ctrl_evening_S17_\": 2.929046,\n",
    "    \"Sample_04_CRS_morning_S4_\": 6.227455,\n",
    "    \"Sample_19_Ctrl_evening_S19_\": 2.545900,\n",
    "    \"Sample_03_Ctrl_morning_S3_\": 9.529751,\n",
    "    \"Sample_02_CRS_morning_S2_\": 7.151634,\n",
    "    \"Sample_09_Ctrl_morning_S9_\": 7.923174,\n",
    "    \"Sample_10_CRS_morning_S10_\": 8.815860,\n",
    "    \"Sample_06_CRS_morning_S6_\": 7.930738,\n",
    "    \"Sample_12_CRS_evening_S12_\": 2.849694,\n",
    "    \"Sample_07_Ctrl_morning_S7_\": 4.924262\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d798831f-e17e-4baa-94da-de28f0eb59f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: /cluster/software/stacks/2024-06/spack/opt/spack/linux-ubuntu22.04-x86_64_v3/gcc-12.2.0/bedtools2-2.31.0-a4obbslkxntgdx2criopqpwx662gcftq/bin/bedtools getfasta -fi /nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Navnit/genomes/mouse/GRCm39_NCBI_Bowtie2.fasta -bed /nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Vakil/mouse_genome_annotation/Genes_Promoters_CpG_islands_for_Tae/allCpG_islands_GRCm39.bed -bedOut > /cluster/scratch/taekim/data_oxidation/cpg_intersect/allCpG_islands.SEQ.bed\n",
      "Command completed successfully\n"
     ]
    }
   ],
   "source": [
    "def run_bedtools_getfasta(bed_file, output_file, genome_path):\n",
    "    \"\"\"Run bedtools getfasta command to get sequence data for a BED file\"\"\"\n",
    "    command = f\"/cluster/software/stacks/2024-06/spack/opt/spack/linux-ubuntu22.04-x86_64_v3/gcc-12.2.0/bedtools2-2.31.0-a4obbslkxntgdx2criopqpwx662gcftq/bin/bedtools getfasta -fi {genome_path} -bed {bed_file} -bedOut > {output_file}\"\n",
    "    print(f\"Running command: {command}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command, shell=True, check=True, \n",
    "                               stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
    "                               text=True)\n",
    "        print(\"Command completed successfully\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running bedtools: {e}\")\n",
    "        print(f\"STDERR: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "SEQ_COLUMNS = [\"Chr\", \"Start\", \"End\", \"Seq\"]\n",
    "seq_output_file = os.path.join(PATH, \"allCpG_islands.SEQ.bed\")\n",
    "# Run getfasta with original CpG coordinate file\n",
    "run_bedtools_getfasta(ORG_PATH, seq_output_file, GENOME_PATH)\n",
    "seq_df = pd.read_csv(seq_output_file, sep=\"\\t\", header=None, names=SEQ_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9102a9ce-5e3f-4016-ab33-3e7186a8b6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing of samples\n",
      "First pass: Collecting all unique IDs...\n",
      "Collected 15968 unique IDs across all samples\n",
      "Processing file: Sample_01_Ctrl_morning_S1_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 12285\n",
      "File data: 5618\n",
      "Processing file: Sample_01_Ctrl_morning_S1_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 12493\n",
      "File data: 5618\n",
      "Processing file: Sample_02_CRS_morning_S2_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 14585\n",
      "File data: 6146\n",
      "Processing file: Sample_02_CRS_morning_S2_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 14884\n",
      "File data: 6230\n",
      "Processing file: Sample_03_Ctrl_morning_S3_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 15610\n",
      "File data: 6872\n",
      "Processing file: Sample_03_Ctrl_morning_S3_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 15956\n",
      "File data: 7010\n",
      "Processing file: Sample_04_CRS_morning_S4_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 11335\n",
      "File data: 5475\n",
      "Processing file: Sample_04_CRS_morning_S4_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 10942\n",
      "File data: 5339\n",
      "Processing file: Sample_05_Ctrl_morning_S5_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 9503\n",
      "File data: 4730\n",
      "Processing file: Sample_05_Ctrl_morning_S5_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 9478\n",
      "File data: 4731\n",
      "Processing file: Sample_06_CRS_morning_S6_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 14900\n",
      "File data: 6926\n",
      "Processing file: Sample_06_CRS_morning_S6_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 15030\n",
      "File data: 6944\n",
      "Processing file: Sample_07_Ctrl_morning_S7_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 10559\n",
      "File data: 4928\n",
      "Processing file: Sample_07_Ctrl_morning_S7_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 10344\n",
      "File data: 4835\n",
      "Processing file: Sample_08_CRS_morning_S8_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 11431\n",
      "File data: 5550\n",
      "Processing file: Sample_08_CRS_morning_S8_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 11220\n",
      "File data: 5443\n",
      "Processing file: Sample_09_Ctrl_morning_S9_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 13233\n",
      "File data: 5761\n",
      "Processing file: Sample_09_Ctrl_morning_S9_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 12675\n",
      "File data: 5809\n",
      "Processing file: Sample_10_CRS_morning_S10_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 21012\n",
      "File data: 8315\n",
      "Processing file: Sample_10_CRS_morning_S10_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 20874\n",
      "File data: 8237\n",
      "Processing file: Sample_11_Ctrl_evening_S11_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 4855\n",
      "File data: 2914\n",
      "Processing file: Sample_11_Ctrl_evening_S11_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 4721\n",
      "File data: 2846\n",
      "Processing file: Sample_12_CRS_evening_S12_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 5491\n",
      "File data: 3380\n",
      "Processing file: Sample_12_CRS_evening_S12_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 5343\n",
      "File data: 3380\n",
      "Processing file: Sample_13_Ctrl_evening_S13_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 4099\n",
      "File data: 2571\n",
      "Processing file: Sample_13_Ctrl_evening_S13_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 4030\n",
      "File data: 2469\n",
      "Processing file: Sample_14_CRS_evening_S14_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 4398\n",
      "File data: 2594\n",
      "Processing file: Sample_14_CRS_evening_S14_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 4566\n",
      "File data: 2664\n",
      "Processing file: Sample_15_Ctrl_evening_S15_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 8875\n",
      "File data: 5103\n",
      "Processing file: Sample_15_Ctrl_evening_S15_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 8948\n",
      "File data: 5096\n",
      "Processing file: Sample_16_CRS_evening_S16_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 6135\n",
      "File data: 3625\n",
      "Processing file: Sample_16_CRS_evening_S16_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 6168\n",
      "File data: 3598\n",
      "Processing file: Sample_17_Ctrl_evening_S17_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 4752\n",
      "File data: 2908\n",
      "Processing file: Sample_17_Ctrl_evening_S17_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 4551\n",
      "File data: 2794\n",
      "Processing file: Sample_18_CRS_evening_S18_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 7849\n",
      "File data: 4592\n",
      "Processing file: Sample_18_CRS_evening_S18_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 8224\n",
      "File data: 4778\n",
      "Processing file: Sample_19_Ctrl_evening_S19_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 5179\n",
      "File data: 3097\n",
      "Processing file: Sample_19_Ctrl_evening_S19_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 5507\n",
      "File data: 3178\n",
      "Processing file: Sample_20_CRS_evening_S20_.GRCh38.p13_G_minus_strand.bed\n",
      "  Records: 39199\n",
      "File data: 12490\n",
      "Processing file: Sample_20_CRS_evening_S20_.GRCh38.p13_G_plus_strand.bed\n",
      "  Records: 40085\n",
      "File data: 12540\n",
      "Processing file: allCpG_islands.SEQ.bed\n",
      "  Warning: Could not determine strand for allCpG_islands.SEQ.bed\n",
      "Adding missing IDs to each sample...\n",
      "There are 5618 unique ids\n",
      "  Adding 10350 missing IDs to sample Sample_01_Ctrl_morning_S1_ (strand -)\n",
      "There are 5618 unique ids\n",
      "  Adding 10350 missing IDs to sample Sample_01_Ctrl_morning_S1_ (strand +)\n",
      "There are 6146 unique ids\n",
      "  Adding 9822 missing IDs to sample Sample_02_CRS_morning_S2_ (strand -)\n",
      "There are 6230 unique ids\n",
      "  Adding 9738 missing IDs to sample Sample_02_CRS_morning_S2_ (strand +)\n",
      "There are 6872 unique ids\n",
      "  Adding 9096 missing IDs to sample Sample_03_Ctrl_morning_S3_ (strand -)\n",
      "There are 7010 unique ids\n",
      "  Adding 8958 missing IDs to sample Sample_03_Ctrl_morning_S3_ (strand +)\n",
      "There are 5475 unique ids\n",
      "  Adding 10493 missing IDs to sample Sample_04_CRS_morning_S4_ (strand -)\n",
      "There are 5339 unique ids\n",
      "  Adding 10629 missing IDs to sample Sample_04_CRS_morning_S4_ (strand +)\n",
      "There are 4730 unique ids\n",
      "  Adding 11238 missing IDs to sample Sample_05_Ctrl_morning_S5_ (strand -)\n",
      "There are 4731 unique ids\n",
      "  Adding 11237 missing IDs to sample Sample_05_Ctrl_morning_S5_ (strand +)\n",
      "There are 6926 unique ids\n",
      "  Adding 9042 missing IDs to sample Sample_06_CRS_morning_S6_ (strand -)\n",
      "There are 6944 unique ids\n",
      "  Adding 9024 missing IDs to sample Sample_06_CRS_morning_S6_ (strand +)\n",
      "There are 4928 unique ids\n",
      "  Adding 11040 missing IDs to sample Sample_07_Ctrl_morning_S7_ (strand -)\n",
      "There are 4835 unique ids\n",
      "  Adding 11133 missing IDs to sample Sample_07_Ctrl_morning_S7_ (strand +)\n",
      "There are 5550 unique ids\n",
      "  Adding 10418 missing IDs to sample Sample_08_CRS_morning_S8_ (strand -)\n",
      "There are 5443 unique ids\n",
      "  Adding 10525 missing IDs to sample Sample_08_CRS_morning_S8_ (strand +)\n",
      "There are 5761 unique ids\n",
      "  Adding 10207 missing IDs to sample Sample_09_Ctrl_morning_S9_ (strand -)\n",
      "There are 5809 unique ids\n",
      "  Adding 10159 missing IDs to sample Sample_09_Ctrl_morning_S9_ (strand +)\n",
      "There are 8315 unique ids\n",
      "  Adding 7653 missing IDs to sample Sample_10_CRS_morning_S10_ (strand -)\n",
      "There are 8237 unique ids\n",
      "  Adding 7731 missing IDs to sample Sample_10_CRS_morning_S10_ (strand +)\n",
      "There are 2914 unique ids\n",
      "  Adding 13054 missing IDs to sample Sample_11_Ctrl_evening_S11_ (strand -)\n",
      "There are 2846 unique ids\n",
      "  Adding 13122 missing IDs to sample Sample_11_Ctrl_evening_S11_ (strand +)\n",
      "There are 3380 unique ids\n",
      "  Adding 12588 missing IDs to sample Sample_12_CRS_evening_S12_ (strand -)\n",
      "There are 3380 unique ids\n",
      "  Adding 12588 missing IDs to sample Sample_12_CRS_evening_S12_ (strand +)\n",
      "There are 2571 unique ids\n",
      "  Adding 13397 missing IDs to sample Sample_13_Ctrl_evening_S13_ (strand -)\n",
      "There are 2469 unique ids\n",
      "  Adding 13499 missing IDs to sample Sample_13_Ctrl_evening_S13_ (strand +)\n",
      "There are 2594 unique ids\n",
      "  Adding 13374 missing IDs to sample Sample_14_CRS_evening_S14_ (strand -)\n",
      "There are 2664 unique ids\n",
      "  Adding 13304 missing IDs to sample Sample_14_CRS_evening_S14_ (strand +)\n",
      "There are 5103 unique ids\n",
      "  Adding 10865 missing IDs to sample Sample_15_Ctrl_evening_S15_ (strand -)\n",
      "There are 5096 unique ids\n",
      "  Adding 10872 missing IDs to sample Sample_15_Ctrl_evening_S15_ (strand +)\n",
      "There are 3625 unique ids\n",
      "  Adding 12343 missing IDs to sample Sample_16_CRS_evening_S16_ (strand -)\n",
      "There are 3598 unique ids\n",
      "  Adding 12370 missing IDs to sample Sample_16_CRS_evening_S16_ (strand +)\n",
      "There are 2908 unique ids\n",
      "  Adding 13060 missing IDs to sample Sample_17_Ctrl_evening_S17_ (strand -)\n",
      "There are 2794 unique ids\n",
      "  Adding 13174 missing IDs to sample Sample_17_Ctrl_evening_S17_ (strand +)\n",
      "There are 4592 unique ids\n",
      "  Adding 11376 missing IDs to sample Sample_18_CRS_evening_S18_ (strand -)\n",
      "There are 4778 unique ids\n",
      "  Adding 11190 missing IDs to sample Sample_18_CRS_evening_S18_ (strand +)\n",
      "There are 3097 unique ids\n",
      "  Adding 12871 missing IDs to sample Sample_19_Ctrl_evening_S19_ (strand -)\n",
      "There are 3178 unique ids\n",
      "  Adding 12790 missing IDs to sample Sample_19_Ctrl_evening_S19_ (strand +)\n",
      "There are 12490 unique ids\n",
      "  Adding 3478 missing IDs to sample Sample_20_CRS_evening_S20_ (strand -)\n",
      "There are 12540 unique ids\n",
      "  Adding 3428 missing IDs to sample Sample_20_CRS_evening_S20_ (strand +)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.31881716.taekim/ipykernel_2310185/1435115188.py:165: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, file_data], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! Combined data saved to all_samples_combined_data.csv\n",
      "Total records: 638720\n"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame for the final output\n",
    "all_data = pd.DataFrame(columns=OUTPUT_COLUMNS)\n",
    "all_unique_ids = set()  # To store all unique IDs across all samples\n",
    "\n",
    "print(\"Starting processing of samples\")\n",
    "\n",
    "# Collect all files for each sample\n",
    "bed_files = glob.glob(os.path.join(PATH, \"*.bed\"))\n",
    "\n",
    "# First pass: Collect all unique IDs across all samples\n",
    "print(\"First pass: Collecting all unique IDs...\")\n",
    "for file_path in sorted(bed_files):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        # Read the file\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=INPUT_COLUMNS)\n",
    "        \n",
    "        # Create the identifier\n",
    "        ids = df['Chr1'] + '_' + df['Start2'].astype(str)\n",
    "        \n",
    "        # Add to the set of all unique IDs\n",
    "        all_unique_ids.update(ids.unique())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error collecting IDs from {file_name}: {str(e)}\")\n",
    "\n",
    "print(f\"Collected {len(all_unique_ids)} unique IDs across all samples\")\n",
    "\n",
    "# Create a dictionary to store chromosome info for each ID\n",
    "id_to_chrom = {}\n",
    "for file_path in sorted(bed_files):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=INPUT_COLUMNS)\n",
    "        # Map each ID to its chromosome\n",
    "        id_df = df.copy()\n",
    "        id_df['id'] = id_df['Chr1'] + '_' + id_df['Start2'].astype(str)\n",
    "        for _, row in id_df.drop_duplicates('id').iterrows():\n",
    "            id_to_chrom[row['id']] = row['Chr1']\n",
    "    except Exception as e:\n",
    "        print(f\"  Error mapping chromosomes: {str(e)}\")\n",
    "\n",
    "# Process each file\n",
    "sample_data_dict = {}  # Store data for each sample\n",
    "\n",
    "\n",
    "for file_path in sorted(bed_files):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    \n",
    "    # Extract sample name and determine strand\n",
    "    sample_name = file_name.split('.')[0]\n",
    "    \n",
    "    if \"plus_strand\" in file_name:\n",
    "        strand = \"+\"\n",
    "        nucleotide = 'G'  # Count G on plus strand\n",
    "    elif \"minus_strand\" in file_name:\n",
    "        strand = \"-\"\n",
    "        nucleotide = 'C'  # Count C on minus strand\n",
    "    else:\n",
    "        print(f\"  Warning: Could not determine strand for {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Read and process the file\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=INPUT_COLUMNS)\n",
    "        print(f\"  Records: {len(df)}\")\n",
    "        \n",
    "        # Create IDs and calculate damage per genomic position\n",
    "        df['id'] = df['Chr1'] + '_' + df['Start2'].astype(str)\n",
    "        damage_df = df.groupby('id')['Value'].sum().reset_index()\n",
    "        damage_df.columns = ['id', 'damage']\n",
    "        \n",
    "        # Create base result dataframe\n",
    "        file_data = damage_df.copy()\n",
    "        file_data['sample'] = sample_name\n",
    "        file_data['chromosome'] = df.groupby('id')['Chr1'].first().reindex(file_data['id']).values\n",
    "        file_data['strand'] = strand\n",
    "        \n",
    "        # Calculate nucleotide counts in a straightforward way\n",
    "        # 1. Create matching IDs in the sequence dataframe\n",
    "        seq_df['id'] = seq_df['Chr'] + '_' + seq_df['Start'].astype(str)\n",
    "        \n",
    "        # 2. Count the relevant nucleotide (G or C) in each sequence\n",
    "        seq_counts = seq_df.copy()\n",
    "        seq_counts['GC_count'] = seq_counts['Seq'].str.upper().str.count(nucleotide)\n",
    "        \n",
    "        # 3. Remove duplicates to ensure one count per position\n",
    "        seq_counts = seq_counts.drop_duplicates('id')\n",
    "        \n",
    "        # 4. Merge the counts with our result dataframe\n",
    "        file_data = file_data.merge(\n",
    "            seq_counts[['id', 'GC_count']], \n",
    "            on='id', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Calculate normalized damage\n",
    "        file_data['GC_normalized_damage'] = file_data['damage'] / file_data['GC_count'].replace(0, np.nan)\n",
    "        \n",
    "        # Add median normalization if available\n",
    "        if sample_name in median_values:\n",
    "            file_data['median'] = median_values[sample_name]\n",
    "            file_data['median_normalized_damage'] = 1000 * file_data['GC_normalized_damage'] / file_data['median']\n",
    "        else:\n",
    "            print(f\"  Warning: No median value found for sample {sample_name}\")\n",
    "            file_data['median'] = np.nan\n",
    "            file_data['median_normalized_damage'] = np.nan\n",
    "        \n",
    "        # Store processed data\n",
    "        sample_data_dict[f\"{sample_name}_{strand}\"] = file_data\n",
    "        print(f\"File data: {len(file_data)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {file_name}: {str(e)}\")\n",
    "\n",
    "# Fill in missing IDs for each sample\n",
    "print(\"Adding missing IDs to each sample...\")\n",
    "for sample_key, file_data in sample_data_dict.items():\n",
    "    sample_name = sample_key.rsplit('_', 1)[0]\n",
    "    strand = sample_key.split('_')[-1]\n",
    "    \n",
    "    # Identify missing IDs for this sample\n",
    "    existing_ids = set(file_data['id'])\n",
    "    print (f\"There are {len(existing_ids)} unique ids\")\n",
    "    missing_ids = all_unique_ids - existing_ids\n",
    "    \n",
    "    if missing_ids:\n",
    "        print(f\"  Adding {len(missing_ids)} missing IDs to sample {sample_name} (strand {strand})\")\n",
    "        \n",
    "        # Create rows for missing IDs\n",
    "        missing_rows = []\n",
    "        for missing_id in missing_ids:\n",
    "            # Get the chromosome for this ID\n",
    "            chromosome = id_to_chrom.get(missing_id, \"unknown\")\n",
    "            \n",
    "            # Create a row with zeros for all numerical values\n",
    "            missing_row = {\n",
    "                'id': missing_id,\n",
    "                'sample': sample_name,\n",
    "                'chromosome': chromosome,\n",
    "                'strand': strand,\n",
    "                'GC_count': 0,\n",
    "                'damage': 0,\n",
    "                'GC_normalized_damage': 0\n",
    "            }\n",
    "            \n",
    "            # Add median value if available\n",
    "            if sample_name in median_values:\n",
    "                missing_row['median'] = median_values[sample_name]\n",
    "                missing_row['median_normalized_damage'] = 0\n",
    "            else:\n",
    "                missing_row['median'] = np.nan\n",
    "                missing_row['median_normalized_damage'] = np.nan\n",
    "                \n",
    "            missing_rows.append(missing_row)\n",
    "        \n",
    "        # Add the missing rows to the sample's data\n",
    "        if missing_rows:\n",
    "            missing_df = pd.DataFrame(missing_rows)\n",
    "            file_data = pd.concat([file_data, missing_df], ignore_index=True)\n",
    "            sample_data_dict[sample_key] = file_data\n",
    "\n",
    "# Combine all samples into the final DataFrame\n",
    "for sample_key, file_data in sample_data_dict.items():\n",
    "    all_data = pd.concat([all_data, file_data], ignore_index=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "all_data.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Processing complete! Combined data saved to {OUTPUT_FILE}\")\n",
    "print(f\"Total records: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6de7c-a53e-486b-86ce-512fa15c5b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
