{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569dbd10-090b-4a4c-9a78-e55e159187fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Two-way ANOVA analysis for genomic oxidation data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "import traceback\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=ValueWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9730bc-6498-4b9b-bdf8-2b7c86ebb3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path):\n",
    "\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    \n",
    "    # Extract bin size from file path\n",
    "    bin_size = re.search(r'Normalized_(\\d+)', file_path).group(1)\n",
    "    print(f\"Detected bin size: {bin_size}\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Create FeatureID by combining Bin, Strand, and Chromosome\n",
    "    df['FeatureID'] = df['Bin'].astype(str) + '_' + df['Strand'] + df['Chromosome'].astype(str)\n",
    "    \n",
    "    # Handle potential duplicates by grouping\n",
    "    df['Group'] = df.groupby(['Sample', 'FeatureID']).cumcount().astype(str)\n",
    "    \n",
    "    # Show sample of the processed data\n",
    "    print(\"Sample of the processed dataframe:\")\n",
    "    print(df[['Sample', 'Bin', 'Strand', 'FeatureID', 'Group', 'Median_Normalized_Damage']].head(5))\n",
    "    \n",
    "    # Create pivot table: samples as rows, features as columns\n",
    "    pivot_df = df.pivot_table(\n",
    "        index='Sample', \n",
    "        columns='FeatureID', \n",
    "        values='Median_Normalized_Damage',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Extract bin IDs (these will be the column names in the pivot table)\n",
    "    bin_ids = pivot_df.columns.tolist()\n",
    "    \n",
    "    # Extract metadata from sample names\n",
    "    metadata_df = pd.DataFrame(index=pivot_df.index)\n",
    "    \n",
    "    # Extract group (treatment) and time point from sample names\n",
    "    metadata_df['treatment'] = metadata_df.index.map(lambda x: re.search(r'(CRS|Ctrl)', x).group(1) if re.search(r'(CRS|Ctrl)', x) else \"Unknown\")\n",
    "    metadata_df['timepoint'] = metadata_df.index.map(lambda x: re.search(r'(evening|morning)', x).group(1) if re.search(r'(evening|morning)', x) else \"Unknown\")\n",
    "    \n",
    "    # Reset index to make Sample a standard column\n",
    "    metadata_df = metadata_df.reset_index().rename(columns={'index': 'Sample'})\n",
    "    \n",
    "    print(f\"Created pivot table with {pivot_df.shape[0]} samples and {pivot_df.shape[1]} genomic features\")\n",
    "    print(f\"Extracted metadata with factors: treatment ({metadata_df['treatment'].unique()}) and timepoint ({metadata_df['timepoint'].unique()})\")\n",
    "    print(metadata_df.head(5))\n",
    "    \n",
    "    return pivot_df, bin_ids, metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0aedf5b-33a5-4ae5-9cc6-80da82d168ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_two_way_anova(bin_data, factor1_name, factor2_name):\n",
    "    try:\n",
    "        # Create the formula for the model\n",
    "        formula = f\"measurement ~ C({factor1_name}) + C({factor2_name}) + C({factor1_name}):C({factor2_name})\"\n",
    "        \n",
    "        # Fit the model (using Type III SS)\n",
    "        model = ols(formula, data=bin_data).fit()\n",
    "        \n",
    "        # Get ANOVA table\n",
    "        anova_table = sm.stats.anova_lm(model, typ=3)\n",
    "        \n",
    "        # Extract results\n",
    "        results = {\n",
    "            'factor1_pvalue': anova_table.loc[f'C({factor1_name})', 'PR(>F)'],\n",
    "            'factor2_pvalue': anova_table.loc[f'C({factor2_name})', 'PR(>F)'],\n",
    "            'interaction_pvalue': anova_table.loc[f'C({factor1_name}):C({factor2_name})', 'PR(>F)'],\n",
    "            'factor1_Fvalue': anova_table.loc[f'C({factor1_name})', 'F'],\n",
    "            'factor2_Fvalue': anova_table.loc[f'C({factor2_name})', 'F'],\n",
    "            'interaction_Fvalue': anova_table.loc[f'C({factor1_name}):C({factor2_name})', 'F']\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Return NaN values if error occurs\n",
    "        return {\n",
    "            'factor1_pvalue': np.nan,\n",
    "            'factor2_pvalue': np.nan,\n",
    "            'interaction_pvalue': np.nan,\n",
    "            'factor1_Fvalue': np.nan,\n",
    "            'factor2_Fvalue': np.nan,\n",
    "            'interaction_Fvalue': np.nan\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d912f21-ea66-4e2f-ba2b-d40fb217b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bin(args):\n",
    "\n",
    "    bin_idx, bin_id, pivot_df, metadata_df, factor1_name, factor2_name = args\n",
    "    \n",
    "    # Extract measurements for this bin\n",
    "    bin_values = pivot_df.iloc[:, bin_idx].values\n",
    "    \n",
    "    # Create DataFrame with measurements and factors\n",
    "    df = pd.DataFrame({\n",
    "        'measurement': bin_values,\n",
    "        'Sample': pivot_df.index.tolist()\n",
    "    })\n",
    "\n",
    "    \n",
    "    # Merge with metadata to get the factors\n",
    "    df = pd.merge(df, metadata_df, on='Sample')\n",
    "    \n",
    "    # Perform ANOVA\n",
    "    result = perform_two_way_anova(df, factor1_name, factor2_name)\n",
    "    result['bin_id'] = bin_id\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6a6a26-23f9-4abd-9ef2-7671247dc700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(items, batch_size):\n",
    "    \"\"\"Split a list of items into batches of specified size\"\"\"\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19a5950b-b8f0-49b2-8589-b1755f452953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_args):\n",
    "    \"\"\"Process a batch of bins\"\"\"\n",
    "    batch_bins_with_idx, pivot_data, meta_data, f1_name, f2_name = batch_args\n",
    "    batch_results = []\n",
    "    \n",
    "    for bin_idx, bin_id in batch_bins_with_idx:\n",
    "        # Call the original process_bin function with the correct bin_idx\n",
    "        result = process_bin((bin_idx, bin_id, pivot_data, meta_data, f1_name, f2_name))\n",
    "        batch_results.append(result)\n",
    "        \n",
    "    return batch_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0e01962-bd37-4113-86ac-7d237ad4cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_genomic_bins(pivot_df, bin_ids, metadata_df, factor1_name='treatment', factor2_name='timepoint', \n",
    "                       figures_dir=None, data_dir=None,\n",
    "                       n_cores=64, batch_size=1000):\n",
    "\n",
    "    bin_size = re.search(r'Normalized_(\\d+)', file_path).group(1)\n",
    "    output_prefix = \"anova_\" + bin_size \n",
    "\n",
    "    # Create output directories if specified\n",
    "    if figures_dir is not None:\n",
    "        os.makedirs(figures_dir, exist_ok=True)\n",
    "    \n",
    "    if data_dir is not None:\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Determine number of cores for parallel processing\n",
    "    if n_cores is None:\n",
    "        n_cores = max(1, mp.cpu_count() - 1)  # Leave one core free\n",
    "    \n",
    "    total_bins = len(bin_ids)\n",
    "    print(f\"Processing {total_bins} bins using {n_cores} cores with batch size {batch_size}...\")\n",
    "    \n",
    "    # Create a list of (bin_idx, bin_id) pairs\n",
    "    bin_idx_id_pairs = list(enumerate(bin_ids))\n",
    "    \n",
    "    # Group these pairs into batches\n",
    "    bin_batches_with_idx = list(create_batches(bin_idx_id_pairs, batch_size))\n",
    "    \n",
    "    # Prepare batch arguments\n",
    "    batch_args_list = [(batch, pivot_df, metadata_df, factor1_name, factor2_name) \n",
    "                      for batch in bin_batches_with_idx]\n",
    "    \n",
    "    # Results container\n",
    "    all_results = []\n",
    "    \n",
    "    # Set up parallel processing with chunked work\n",
    "    with mp.Pool(n_cores) as pool:\n",
    "        # Use imap to maintain order of batches\n",
    "        batch_iterator = pool.imap(process_batch, batch_args_list)\n",
    "        \n",
    "        # Process batches with progress bar\n",
    "        with tqdm(total=total_bins) as pbar:\n",
    "            for batch_results in batch_iterator:\n",
    "                all_results.extend(batch_results)\n",
    "                pbar.update(len(batch_results))\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "\n",
    "    \n",
    "    # No need to sort since we maintained the original order with imap\n",
    "    \n",
    "    # Apply multiple testing correction\n",
    "    for col in ['factor1_pvalue', 'factor2_pvalue', 'interaction_pvalue']:\n",
    "        # Benjamini-Hochberg FDR correction\n",
    "        mask = ~np.isnan(results_df[col])\n",
    "        corrected = np.full(len(results_df), np.nan)\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            #corrected[mask] = multipletests(results_df.loc[mask, col], method='fdr_bh')[1]\n",
    "            corrected[mask] = multipletests(results_df.loc[mask, col], method='bonferroni')[1]\n",
    "            \n",
    "        results_df[f'{col}_adj'] = corrected\n",
    "        \n",
    "        # Add significance flag (True/False)\n",
    "        results_df[f'{col.replace(\"pvalue\", \"significant\")}'] = results_df[f'{col}_adj'] < 0.05\n",
    "    \n",
    "    # Prepare output path\n",
    "    if data_dir is not None:\n",
    "        results_path = os.path.join(data_dir, f\"{output_prefix}_results.csv\")\n",
    "    else:\n",
    "        results_path = f\"{output_prefix}_results.csv\"\n",
    "\n",
    "    print(\"results_df\")\n",
    "    print (results_df.head(5))\n",
    "    \n",
    "    # Write results to file\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    \n",
    "    print(f\"Analysis complete. Found:\")\n",
    "    print(f\"  - {results_df['factor1_significant'].sum()} bins significant for {factor1_name}\")\n",
    "    print(f\"  - {results_df['factor2_significant'].sum()} bins significant for {factor2_name}\")\n",
    "    print(f\"  - {results_df['interaction_significant'].sum()} bins significant for interaction\")\n",
    "    print(f\"  - Results CSV saved to: {results_path}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "909c4fef-2fe6-4d8e-8b2e-21b1e98776e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_significant_bins_data(results_df, factor1_name, factor2_name, max_bins=10, file_path=None, table_path=None, data_dir=None):\n",
    "    \n",
    "    if file_path:\n",
    "        bin_size = re.search(r'Normalized_(\\d+)', file_path).group(1)\n",
    "        output_prefix = \"anova_\" + bin_size\n",
    "    else:\n",
    "        output_prefix = \"anova\"\n",
    "    \n",
    "    print(f\"Preparing top significant bins data...\")\n",
    "    \n",
    "    # Load the chromosome naming table\n",
    "    chromosomes_name_table = pd.read_csv(table_path, sep='\\t')\n",
    "\n",
    "    # Filter chromosomes_name_table\n",
    "    chromosomes = ['chr' + str(i) for i in np.arange(1, 20, 1)] + [\"chrX\", \"chrY\"]\n",
    "    chromosomes_name_table = chromosomes_name_table[chromosomes_name_table[\"UCSC style name\"].isin(chromosomes)]\n",
    "    chromosomes_name_table = chromosomes_name_table.loc[:, [\"RefSeq seq accession\", \"UCSC style name\"]]\n",
    "\n",
    "    # Create a dictionary for faster lookups\n",
    "    chrom_dict = dict(zip(chromosomes_name_table[\"RefSeq seq accession\"], chromosomes_name_table[\"UCSC style name\"]))\n",
    "\n",
    "    # Function to parse bin_id and create new UCSC style ID\n",
    "    def create_ucsc_style_id(bin_id):\n",
    "        match = re.match(r'^(\\d+\\.\\d+)_([+-])(.+)$', bin_id)\n",
    "        if match:\n",
    "            index = float(match.group(1))\n",
    "            strand = match.group(2)\n",
    "            ref_seq = match.group(3)\n",
    "            \n",
    "            if ref_seq in chrom_dict:\n",
    "                # Convert index to integer\n",
    "                return f\"{chrom_dict[ref_seq]}{strand}{int(index)}\"\n",
    "        return None\n",
    "    \n",
    "    # For each factor, get the top significant bins\n",
    "    factor_cols = [\n",
    "        (factor1_name, 'factor1_pvalue', 'factor1_significant'),\n",
    "        (factor2_name, 'factor2_pvalue', 'factor2_significant'),\n",
    "        ('Interaction', 'interaction_pvalue', 'interaction_significant')\n",
    "    ]\n",
    "    \n",
    "    # Dictionary to store DataFrames with top bins\n",
    "    top_bins_dict = {}\n",
    "    for name, pval_col, sig_col in factor_cols:\n",
    "        # Get significant bins\n",
    "        sig_bins = results_df[results_df[sig_col] == True].copy()\n",
    "        \n",
    "        if len(sig_bins) == 0:\n",
    "            print(f\"No significant bins found for {name}\")\n",
    "            continue\n",
    "            \n",
    "        # Sort by p-value\n",
    "        sig_bins = sig_bins.sort_values(pval_col)\n",
    "        \n",
    "        # Take top bins\n",
    "        top_bins = sig_bins.head(min(max_bins, len(sig_bins))).copy()\n",
    "        \n",
    "        # Add UCSC style ID column\n",
    "        top_bins['ucsc_style_id'] = top_bins['bin_id'].apply(create_ucsc_style_id)\n",
    "        \n",
    "        top_bins_dict[f\"{name.lower()}_top_bins\"] = top_bins\n",
    "        \n",
    "        # Save the CSV with the new column if data_dir is provided\n",
    "        if data_dir:\n",
    "            top_bins.to_csv(f\"{data_dir}/{output_prefix}_{name.lower()}_top_bins.csv\", index=False)\n",
    "    \n",
    "    # Create a fourth file with the most significant factor for all significant bins\n",
    "    print(\"Creating combined significant bins file with most significant factor...\")\n",
    "    \n",
    "    # Step 1: Identify all bins that are significant for any factor\n",
    "    all_sig_bins = results_df[\n",
    "        (results_df['factor1_significant'] == True) | \n",
    "        (results_df['factor2_significant'] == True) | \n",
    "        (results_df['interaction_significant'] == True)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(all_sig_bins) == 0:\n",
    "        print(\"No significant bins found for any factor\")\n",
    "        return top_bins_dict\n",
    "    \n",
    "    # Step 2: Add a column to identify the most significant factor for each bin\n",
    "    def get_most_significant_factor(row):\n",
    "        # Create a dictionary of F-values (higher is more significant)\n",
    "        # Only consider factors that are significant\n",
    "        f_values = {\n",
    "            factor1_name: row['factor1_Fvalue'] if row['factor1_significant'] else float('-inf'),\n",
    "            factor2_name: row['factor2_Fvalue'] if row['factor2_significant'] else float('-inf'),\n",
    "            'Interaction': row['interaction_Fvalue'] if row['interaction_significant'] else float('-inf')\n",
    "        }\n",
    "        \n",
    "        # Get the factor with the highest F-value\n",
    "        most_sig_factor = max(f_values, key=f_values.get)\n",
    "        \n",
    "        # If no factor is significant, return None\n",
    "        if f_values[most_sig_factor] == float('-inf'):\n",
    "            return None\n",
    "        \n",
    "        return most_sig_factor\n",
    "    \n",
    "    # Add columns for most significant factor and its p-value\n",
    "    all_sig_bins['most_significant_factor'] = all_sig_bins.apply(get_most_significant_factor, axis=1)\n",
    "    \n",
    "    # Add F-values for each factor and the maximum F-value\n",
    "    all_sig_bins['max_f_value'] = all_sig_bins[['factor1_Fvalue', 'factor2_Fvalue', 'interaction_Fvalue']].max(axis=1)\n",
    "    \n",
    "    # Add a column indicating which factors are significant for each bin\n",
    "    def get_significant_factors(row):\n",
    "        sig_factors = []\n",
    "        if row['factor1_significant']:\n",
    "            sig_factors.append(factor1_name)\n",
    "        if row['factor2_significant']:\n",
    "            sig_factors.append(factor2_name)\n",
    "        if row['interaction_significant']:\n",
    "            sig_factors.append('Interaction')\n",
    "        return ';'.join(sig_factors)\n",
    "    \n",
    "    all_sig_bins['significant_factors'] = all_sig_bins.apply(get_significant_factors, axis=1)\n",
    "    \n",
    "    # Add UCSC style ID column\n",
    "    all_sig_bins['ucsc_style_id'] = all_sig_bins['bin_id'].apply(create_ucsc_style_id)\n",
    "    \n",
    "    # Sort by the maximum F-value (descending)\n",
    "    all_sig_bins = all_sig_bins.sort_values('max_f_value', ascending=False)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    top_bins_dict['all_significant_bins'] = all_sig_bins\n",
    "    \n",
    "    # Save the CSV if data_dir is provided\n",
    "    if data_dir:\n",
    "        all_sig_bins.to_csv(f\"{data_dir}/{output_prefix}_all_significant_bins.csv\", index=False)\n",
    "    \n",
    "    return top_bins_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34f34155-d26b-4477-8dcb-1ad714644b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap_unified_zscore(pivot_df, bin_ids, metadata_df, results_df, factor1_name, factor2_name, top_bins_dict=None, max_bins=50, heatmap_dir=None):\n",
    "    \"\"\"\n",
    "    Creates a combined heatmap of significant bins with z-score normalization.\n",
    "    Z-scores standardize the data to show how many standard deviations each value \n",
    "    is from the mean for each bin, making patterns more visible across bins with different value ranges.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "    from scipy import stats\n",
    "    \n",
    "    print(f\"Creating a combined heatmap of significant bins with z-score normalization...\")\n",
    "    \n",
    "    # Get all significant bins from the dictionary\n",
    "    if 'all_significant_bins' not in top_bins_dict:\n",
    "        print(\"No significant bins found, skipping heatmap.\")\n",
    "        return\n",
    "        \n",
    "    all_sig_bins_df = top_bins_dict['all_significant_bins']\n",
    "    \n",
    "    # Create groups based on significance pattern\n",
    "    # Group 1: Only timepoint (factor2) is significant\n",
    "    # Group 2: Only treatment (factor1) is significant\n",
    "    # Group 3: All other bins (interaction or multiple factors)\n",
    "    \n",
    "    all_sig_bins_df['group'] = 'Other'\n",
    "    \n",
    "    # Find bins where ONLY factor2 (timepoint) is significant\n",
    "    only_factor2_mask = (all_sig_bins_df['factor2_significant'] == True) & \\\n",
    "                        (all_sig_bins_df['factor1_significant'] == False) & \\\n",
    "                        (all_sig_bins_df['interaction_significant'] == False)\n",
    "    all_sig_bins_df.loc[only_factor2_mask, 'group'] = f'Only {factor2_name}'\n",
    "    \n",
    "    # Find bins where ONLY factor1 (treatment) is significant\n",
    "    only_factor1_mask = (all_sig_bins_df['factor1_significant'] == True) & \\\n",
    "                        (all_sig_bins_df['factor2_significant'] == False) & \\\n",
    "                        (all_sig_bins_df['interaction_significant'] == False)\n",
    "    all_sig_bins_df.loc[only_factor1_mask, 'group'] = f'Only {factor1_name}'\n",
    "    \n",
    "    # Get the bin IDs\n",
    "    bin_ids = all_sig_bins_df['bin_id'].tolist()\n",
    "    \n",
    "    # Get the UCSC style IDs for labeling\n",
    "    if 'ucsc_style_id' in all_sig_bins_df.columns:\n",
    "        # Create a mapping from bin_id to ucsc_style_id\n",
    "        id_mapping = dict(zip(all_sig_bins_df['bin_id'], all_sig_bins_df['ucsc_style_id']))\n",
    "    else:\n",
    "        id_mapping = {bin_id: bin_id for bin_id in bin_ids}\n",
    "    \n",
    "    # Create group mapping for coloring\n",
    "    group_mapping = dict(zip(all_sig_bins_df['bin_id'], all_sig_bins_df['group']))\n",
    "    \n",
    "    # Filter pivot table to only include these bins\n",
    "    # Check if all bin IDs exist in the pivot table\n",
    "    existing_bins = [b for b in bin_ids if b in pivot_df.columns]\n",
    "    \n",
    "    if len(existing_bins) < 2:\n",
    "        print(f\"Not enough valid bins to create a heatmap, need at least 2.\")\n",
    "        return\n",
    "        \n",
    "    bins_pivot = pivot_df[existing_bins]\n",
    "    \n",
    "    # Add the factors to the index\n",
    "    heatmap_df = bins_pivot.copy()\n",
    "    heatmap_df[factor1_name] = metadata_df.set_index('Sample')[factor1_name]\n",
    "    heatmap_df[factor2_name] = metadata_df.set_index('Sample')[factor2_name]\n",
    "    \n",
    "    # Sort by factors\n",
    "    heatmap_df = heatmap_df.sort_values([factor1_name, factor2_name])\n",
    "    \n",
    "    # Extract the factors for the row colors\n",
    "    row_colors = pd.DataFrame({\n",
    "        factor1_name: heatmap_df[factor1_name],\n",
    "        factor2_name: heatmap_df[factor2_name]\n",
    "    })\n",
    "    \n",
    "    # Remove the factors from the dataframe before plotting\n",
    "    heatmap_data = heatmap_df.drop([factor1_name, factor2_name], axis=1)\n",
    "    \n",
    "    # Reorder columns by group to ensure bins from the same group are together\n",
    "    # First, create a DataFrame with the group information for sorting\n",
    "    column_group_df = pd.DataFrame({\n",
    "        'bin_id': existing_bins,\n",
    "        'group': [group_mapping.get(bin_id, 'Other') for bin_id in existing_bins]\n",
    "    })\n",
    "    \n",
    "    # Define a custom group order for sorting\n",
    "    group_order = [f'Only {factor1_name}', f'Only {factor2_name}', 'Other']\n",
    "    group_order_dict = {group: i for i, group in enumerate(group_order)}\n",
    "    \n",
    "    # Sort by group first, then by bin_id\n",
    "    column_group_df['group_order'] = column_group_df['group'].map(group_order_dict)\n",
    "    column_group_df = column_group_df.sort_values(['group_order', 'bin_id'])\n",
    "    \n",
    "    # Reorder the columns in the heatmap_data\n",
    "    ordered_bins = column_group_df['bin_id'].tolist()\n",
    "    heatmap_data = heatmap_data[ordered_bins]\n",
    "    \n",
    "    # ===== Z-SCORE CALCULATION (NEW) =====\n",
    "    # Calculate z-scores for each bin across all samples\n",
    "    zscore_df = pd.DataFrame(index=heatmap_data.index, columns=heatmap_data.columns)\n",
    "    \n",
    "    for bin_id in heatmap_data.columns:\n",
    "        bin_values = heatmap_data[bin_id].values\n",
    "        # Check if there's any variation in this bin's values\n",
    "        if np.std(bin_values) > 0:\n",
    "            # Calculate z-score for each value\n",
    "            zscore_df[bin_id] = stats.zscore(bin_values)\n",
    "        else:\n",
    "            # If all values are identical, set z-score to 0\n",
    "            zscore_df[bin_id] = 0\n",
    "    \n",
    "    # Log the z-score range\n",
    "    zmin = zscore_df.min().min()\n",
    "    zmax = zscore_df.max().max()\n",
    "    print(f\"Z-score range: {zmin:.2f} to {zmax:.2f}\")\n",
    "    \n",
    "    # Check for any extreme z-scores that might need capping\n",
    "    extreme_threshold = 5.0  # Consider z-scores beyond +/- 5 as extreme\n",
    "    extreme_count = ((zscore_df.abs() > extreme_threshold).sum().sum())\n",
    "    if extreme_count > 0:\n",
    "        print(f\"Note: {extreme_count} extreme z-scores (> |{extreme_threshold}|) detected.\")\n",
    "        # Optionally cap extreme values to make visualization more balanced\n",
    "        # zscore_df = zscore_df.clip(-extreme_threshold, extreme_threshold)\n",
    "    # ===== END Z-SCORE CALCULATION =====\n",
    "    \n",
    "    # Define color map for the groups - use distinct colors\n",
    "    group_colors = {\n",
    "        f'Only {factor1_name}': \"#1f77b4\",  # Blue\n",
    "        f'Only {factor2_name}': \"#ff7f0e\",  # Orange\n",
    "        'Other': \"#7f7f7f\"                  # Gray\n",
    "    }\n",
    "    \n",
    "    # Create column colors based on groups\n",
    "    col_colors = pd.Series({\n",
    "        id_mapping.get(col, col): group_colors.get(group_mapping.get(col, 'Other'), \"#7f7f7f\") \n",
    "        for col in ordered_bins\n",
    "    })\n",
    "    \n",
    "    # Rename columns from bin_id to ucsc_style_id for better readability\n",
    "    heatmap_data.columns = [id_mapping.get(col, col) for col in heatmap_data.columns]\n",
    "    zscore_df.columns = [id_mapping.get(col, col) for col in zscore_df.columns]\n",
    "    \n",
    "    # Check if we have enough data to create a heatmap\n",
    "    if heatmap_data.shape[1] < 2 or heatmap_data.shape[0] < 2:\n",
    "        print(\"Not enough data to create a heatmap.\")\n",
    "        return\n",
    "        \n",
    "    # Create color map for factors - ENSURE CONSISTENT COLORS\n",
    "    # Define fixed colors for treatments and timepoints - these will be consistent across plots\n",
    "    treatment_palette = dict(zip(sorted(row_colors[factor1_name].unique()), \n",
    "                                sns.color_palette(\"Set1\", len(row_colors[factor1_name].unique()))))\n",
    "    timepoint_palette = dict(zip(sorted(row_colors[factor2_name].unique()), \n",
    "                                sns.color_palette(\"Set2\", len(row_colors[factor2_name].unique()))))\n",
    "    \n",
    "    # Apply color maps using the consistent palettes\n",
    "    row_colors_mapped = pd.DataFrame({\n",
    "        factor1_name: row_colors[factor1_name].map(treatment_palette),\n",
    "        factor2_name: row_colors[factor2_name].map(timepoint_palette)\n",
    "    })\n",
    "    \n",
    "    # Calculate optimal figure size\n",
    "    # Add extra space for legends\n",
    "    fig_width = max(12, len(existing_bins) * 0.4) + 2  # Add 2 inches for legend space\n",
    "    fig_height = max(8, len(heatmap_data) * 0.4)\n",
    "    \n",
    "    # Create a new figure for the combined plot\n",
    "    combined_fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "    \n",
    "    # Calculate the width proportions\n",
    "    row_colors_width = 0.05  # 5% for each row color band\n",
    "    heatmap_width = 1 - (row_colors_width * 2) - 0.15  # 15% for legend\n",
    "    \n",
    "    # Create GridSpec\n",
    "    gs = plt.GridSpec(1, 4, width_ratios=[row_colors_width, row_colors_width, heatmap_width, 0.15])\n",
    "    \n",
    "    # Create axes for each component\n",
    "    factor1_ax = combined_fig.add_subplot(gs[0, 0])\n",
    "    factor2_ax = combined_fig.add_subplot(gs[0, 1])\n",
    "    heatmap_ax = combined_fig.add_subplot(gs[0, 2])\n",
    "    legend_ax = combined_fig.add_subplot(gs[0, 3])\n",
    "    \n",
    "    # Draw factor1 colors\n",
    "    for i, (idx, row) in enumerate(row_colors_mapped.iterrows()):\n",
    "        factor1_ax.add_patch(plt.Rectangle((0, i), 1, 1, color=row[factor1_name]))\n",
    "    \n",
    "    # Draw factor2 colors\n",
    "    for i, (idx, row) in enumerate(row_colors_mapped.iterrows()):\n",
    "        factor2_ax.add_patch(plt.Rectangle((0, i), 1, 1, color=row[factor2_name]))\n",
    "    \n",
    "    # Set axes properties for color bands\n",
    "    for ax, title in [(factor1_ax, factor1_name), (factor2_ax, factor2_name)]:\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, len(heatmap_data))\n",
    "        ax.set_xticks([0.5])\n",
    "        ax.set_xticklabels([title])\n",
    "        ax.set_yticks([])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    # Draw the heatmap in the main section using z-scores\n",
    "    sns.heatmap(\n",
    "        zscore_df,  # Using z-scores instead of raw values\n",
    "        cmap=\"viridis\",  # Use viridis colormap as requested\n",
    "        vmin=-3,    # Limit color scale to +/- 3 standard deviations\n",
    "        vmax=3,\n",
    "        ax=heatmap_ax,\n",
    "        cbar_ax=legend_ax,\n",
    "        cbar_kws={\"label\": \"Z-Score (Standard deviations from mean)\"}\n",
    "    )\n",
    "    \n",
    "    # Add column colors at the top\n",
    "    col_colors_ax = combined_fig.add_axes([\n",
    "        heatmap_ax.get_position().x0, \n",
    "        heatmap_ax.get_position().y1, \n",
    "        heatmap_ax.get_position().width, \n",
    "        0.02\n",
    "    ])\n",
    "    \n",
    "    # Draw the column color patches\n",
    "    for i, col in enumerate(zscore_df.columns):\n",
    "        col_colors_ax.add_patch(plt.Rectangle(\n",
    "            (i, 0), \n",
    "            1.0, \n",
    "            1.0, \n",
    "            color=col_colors.get(col, \"#7f7f7f\")\n",
    "        ))\n",
    "    \n",
    "    # Set column color axes properties\n",
    "    col_colors_ax.set_xlim(0, len(zscore_df.columns))\n",
    "    col_colors_ax.set_ylim(0, 1)\n",
    "    col_colors_ax.set_xticks([])\n",
    "    col_colors_ax.set_yticks([])\n",
    "    col_colors_ax.spines['top'].set_visible(False)\n",
    "    col_colors_ax.spines['right'].set_visible(False)\n",
    "    col_colors_ax.spines['bottom'].set_visible(False)\n",
    "    col_colors_ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    # Add title to the combined figure\n",
    "    combined_fig.suptitle(f\"Combined Heatmap of {len(existing_bins)} Significant Bins (Z-Score Normalized)\", \n",
    "                          fontsize=16, y=0.98)\n",
    "    \n",
    "    # Create legend for all color elements\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "    \n",
    "    # Add factor1 (treatment) items\n",
    "    for label in sorted(row_colors[factor1_name].unique()):\n",
    "        legend_handles.append(plt.Rectangle((0, 0), 1, 1, color=treatment_palette[label]))\n",
    "        legend_labels.append(f\"{factor1_name}: {label}\")\n",
    "    \n",
    "    # Add factor2 (timepoint) items\n",
    "    for label in sorted(row_colors[factor2_name].unique()):\n",
    "        legend_handles.append(plt.Rectangle((0, 0), 1, 1, color=timepoint_palette[label]))\n",
    "        legend_labels.append(f\"{factor2_name}: {label}\")\n",
    "    \n",
    "    # Add bin group items\n",
    "    for group, color in group_colors.items():\n",
    "        if group in group_mapping.values():\n",
    "            legend_handles.append(plt.Rectangle((0, 0), 1, 1, color=color))\n",
    "            legend_labels.append(group)\n",
    "    \n",
    "    # Add z-score explanation to legend\n",
    "    legend_handles.extend([\n",
    "        plt.Rectangle((0, 0), 1, 1, color='#440154'),  # Dark purple (lowest in viridis)\n",
    "        plt.Rectangle((0, 0), 1, 1, color='#21918c'),  # Teal (middle in viridis)\n",
    "        plt.Rectangle((0, 0), 1, 1, color='#fde725')   # Yellow (highest in viridis)\n",
    "    ])\n",
    "    \n",
    "    legend_labels.extend([\n",
    "        \"Z-score < 0 (Value below mean)\",\n",
    "        \"Z-score = 0 (Mean value)\",\n",
    "        \"Z-score > 0 (Value above mean)\"\n",
    "    ])\n",
    "    \n",
    "    # Create a separate figure for the legend\n",
    "    legend_fig = plt.figure(figsize=(3, 6))\n",
    "    legend_ax = legend_fig.add_subplot(111)\n",
    "    legend_ax.axis('off')\n",
    "    \n",
    "    legend = legend_ax.legend(\n",
    "        legend_handles, \n",
    "        legend_labels, \n",
    "        loc='center', \n",
    "        frameon=True\n",
    "    )\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(heatmap_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the figures\n",
    "    combined_fig.savefig(f\"{heatmap_dir}/combined_significance_heatmap_zscore.png\", \n",
    "                         dpi=300, bbox_inches='tight')\n",
    "    legend_fig.savefig(f\"{heatmap_dir}/heatmap_legend_zscore.png\", \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "  \n",
    "\n",
    "    print(f\"Z-score normalized heatmap complete. Files saved to:\")\n",
    "    print(f\"  - {heatmap_dir}/combined_significance_heatmap_zscore.png\")\n",
    "    print(f\"  - {heatmap_dir}/heatmap_legend_zscore.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efe0aa18-4ff8-497b-9486-f5d7b38b656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/taekim/stressed_mice/jupyter_notebooks\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a3a3ad0-e4f0-4187-8268-a9fe3bfd1f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../data_normalized/cleaned_Normalized_1000.csv...\n",
      "Detected bin size: 1000\n",
      "Sample of the processed dataframe:\n",
      "                       Sample        Bin Strand               FeatureID Group  \\\n",
      "0  Sample_14_CRS_evening_S14_  3049000.0      -  3049000.0_-NC_000067.7     0   \n",
      "1  Sample_14_CRS_evening_S14_  3050000.0      +  3050000.0_+NC_000067.7     0   \n",
      "2  Sample_14_CRS_evening_S14_  3050000.0      -  3050000.0_-NC_000067.7     0   \n",
      "3  Sample_14_CRS_evening_S14_  3051000.0      +  3051000.0_+NC_000067.7     0   \n",
      "4  Sample_14_CRS_evening_S14_  3051000.0      -  3051000.0_-NC_000067.7     0   \n",
      "\n",
      "   Median_Normalized_Damage  \n",
      "0                  0.000000  \n",
      "1                  1.745443  \n",
      "2                  1.944181  \n",
      "3                  3.758130  \n",
      "4                  6.334267  \n",
      "Created pivot table with 20 samples and 5300115 genomic features\n",
      "Extracted metadata with factors: treatment (['Ctrl' 'CRS']) and timepoint (['morning' 'evening'])\n",
      "                       Sample treatment timepoint\n",
      "0  Sample_01_Ctrl_morning_S1_      Ctrl   morning\n",
      "1   Sample_02_CRS_morning_S2_       CRS   morning\n",
      "2  Sample_03_Ctrl_morning_S3_      Ctrl   morning\n",
      "3   Sample_04_CRS_morning_S4_       CRS   morning\n",
      "4  Sample_05_Ctrl_morning_S5_      Ctrl   morning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>FeatureID</th>\n",
       "      <th>1000000.0_+NC_000087.8</th>\n",
       "      <th>1000000.0_-NC_000087.8</th>\n",
       "      <th>10000000.0_+NC_000067.7</th>\n",
       "      <th>10000000.0_+NC_000068.8</th>\n",
       "      <th>10000000.0_+NC_000069.7</th>\n",
       "      <th>10000000.0_+NC_000070.7</th>\n",
       "      <th>10000000.0_+NC_000071.7</th>\n",
       "      <th>10000000.0_+NC_000072.7</th>\n",
       "      <th>10000000.0_+NC_000073.7</th>\n",
       "      <th>10000000.0_+NC_000074.7</th>\n",
       "      <th>...</th>\n",
       "      <th>99999000.0_-NC_000073.7</th>\n",
       "      <th>99999000.0_-NC_000074.7</th>\n",
       "      <th>99999000.0_-NC_000075.7</th>\n",
       "      <th>99999000.0_-NC_000076.7</th>\n",
       "      <th>99999000.0_-NC_000077.7</th>\n",
       "      <th>99999000.0_-NC_000078.7</th>\n",
       "      <th>99999000.0_-NC_000079.7</th>\n",
       "      <th>99999000.0_-NC_000080.7</th>\n",
       "      <th>99999000.0_-NC_000081.7</th>\n",
       "      <th>99999000.0_-NC_000086.8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sample_01_Ctrl_morning_S1_</th>\n",
       "      <td>4.572128</td>\n",
       "      <td>4.748145</td>\n",
       "      <td>2.104067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.100413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756436</td>\n",
       "      <td>0.865145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891847</td>\n",
       "      <td>0.545205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.632534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628171</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_02_CRS_morning_S2_</th>\n",
       "      <td>6.194920</td>\n",
       "      <td>7.877645</td>\n",
       "      <td>2.715111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.400484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.582617</td>\n",
       "      <td>1.726274</td>\n",
       "      <td>0.527654</td>\n",
       "      <td>1.294705</td>\n",
       "      <td>0.789990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.607949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_03_Ctrl_morning_S3_</th>\n",
       "      <td>6.641426</td>\n",
       "      <td>3.941203</td>\n",
       "      <td>0.509391</td>\n",
       "      <td>2.088250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502079</td>\n",
       "      <td>2.702177</td>\n",
       "      <td>0.655841</td>\n",
       "      <td>0.606558</td>\n",
       "      <td>1.065325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437227</td>\n",
       "      <td>1.943232</td>\n",
       "      <td>0.395979</td>\n",
       "      <td>0.485808</td>\n",
       "      <td>3.557103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.368711</td>\n",
       "      <td>1.380718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_04_CRS_morning_S4_</th>\n",
       "      <td>6.097946</td>\n",
       "      <td>9.800611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.994508</td>\n",
       "      <td>1.638564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.003620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.075615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.956149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.907227</td>\n",
       "      <td>0.806931</td>\n",
       "      <td>0.698171</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_05_Ctrl_morning_S5_</th>\n",
       "      <td>8.187601</td>\n",
       "      <td>7.591789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.547417</td>\n",
       "      <td>4.164080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.804135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.936588</td>\n",
       "      <td>4.042628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.830624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913588</td>\n",
       "      <td>2.437766</td>\n",
       "      <td>2.109197</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5300115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "FeatureID                   1000000.0_+NC_000087.8  1000000.0_-NC_000087.8  \\\n",
       "Sample                                                                       \n",
       "Sample_01_Ctrl_morning_S1_                4.572128                4.748145   \n",
       "Sample_02_CRS_morning_S2_                 6.194920                7.877645   \n",
       "Sample_03_Ctrl_morning_S3_                6.641426                3.941203   \n",
       "Sample_04_CRS_morning_S4_                 6.097946                9.800611   \n",
       "Sample_05_Ctrl_morning_S5_                8.187601                7.591789   \n",
       "\n",
       "FeatureID                   10000000.0_+NC_000067.7  10000000.0_+NC_000068.8  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 2.104067                 0.000000   \n",
       "Sample_02_CRS_morning_S2_                  2.715111                 0.000000   \n",
       "Sample_03_Ctrl_morning_S3_                 0.509391                 2.088250   \n",
       "Sample_04_CRS_morning_S4_                  0.000000                 3.994508   \n",
       "Sample_05_Ctrl_morning_S5_                 0.000000                 0.000000   \n",
       "\n",
       "FeatureID                   10000000.0_+NC_000069.7  10000000.0_+NC_000070.7  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 0.491426                 0.000000   \n",
       "Sample_02_CRS_morning_S2_                  0.475606                 0.000000   \n",
       "Sample_03_Ctrl_morning_S3_                 0.000000                 0.502079   \n",
       "Sample_04_CRS_morning_S4_                  1.638564                 0.000000   \n",
       "Sample_05_Ctrl_morning_S5_                 0.000000                 1.547417   \n",
       "\n",
       "FeatureID                   10000000.0_+NC_000071.7  10000000.0_+NC_000072.7  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 3.100413                 0.000000   \n",
       "Sample_02_CRS_morning_S2_                  2.400484                 0.000000   \n",
       "Sample_03_Ctrl_morning_S3_                 2.702177                 0.655841   \n",
       "Sample_04_CRS_morning_S4_                  0.000000                 1.003620   \n",
       "Sample_05_Ctrl_morning_S5_                 4.164080                 0.000000   \n",
       "\n",
       "FeatureID                   10000000.0_+NC_000073.7  10000000.0_+NC_000074.7  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 0.000000                 0.000000   \n",
       "Sample_02_CRS_morning_S2_                  0.000000                 0.000000   \n",
       "Sample_03_Ctrl_morning_S3_                 0.606558                 1.065325   \n",
       "Sample_04_CRS_morning_S4_                  0.000000                 4.075615   \n",
       "Sample_05_Ctrl_morning_S5_                 2.804135                 0.000000   \n",
       "\n",
       "FeatureID                   ...  99999000.0_-NC_000073.7  \\\n",
       "Sample                      ...                            \n",
       "Sample_01_Ctrl_morning_S1_  ...                 0.756436   \n",
       "Sample_02_CRS_morning_S2_   ...                 0.000000   \n",
       "Sample_03_Ctrl_morning_S3_  ...                 0.000000   \n",
       "Sample_04_CRS_morning_S4_   ...                 0.000000   \n",
       "Sample_05_Ctrl_morning_S5_  ...                 0.000000   \n",
       "\n",
       "FeatureID                   99999000.0_-NC_000074.7  99999000.0_-NC_000075.7  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 0.865145                 0.000000   \n",
       "Sample_02_CRS_morning_S2_                  0.000000                 0.582617   \n",
       "Sample_03_Ctrl_morning_S3_                 0.000000                 0.437227   \n",
       "Sample_04_CRS_morning_S4_                  0.000000                 0.000000   \n",
       "Sample_05_Ctrl_morning_S5_                 1.936588                 4.042628   \n",
       "\n",
       "FeatureID                   99999000.0_-NC_000076.7  99999000.0_-NC_000077.7  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 0.891847                 0.545205   \n",
       "Sample_02_CRS_morning_S2_                  1.726274                 0.527654   \n",
       "Sample_03_Ctrl_morning_S3_                 1.943232                 0.395979   \n",
       "Sample_04_CRS_morning_S4_                  4.956149                 0.000000   \n",
       "Sample_05_Ctrl_morning_S5_                 0.000000                 1.830624   \n",
       "\n",
       "FeatureID                   99999000.0_-NC_000078.7  99999000.0_-NC_000079.7  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 0.000000                 1.632534   \n",
       "Sample_02_CRS_morning_S2_                  1.294705                 0.789990   \n",
       "Sample_03_Ctrl_morning_S3_                 0.485808                 3.557103   \n",
       "Sample_04_CRS_morning_S4_                  0.000000                 0.907227   \n",
       "Sample_05_Ctrl_morning_S5_                 0.000000                 0.913588   \n",
       "\n",
       "FeatureID                   99999000.0_-NC_000080.7  99999000.0_-NC_000081.7  \\\n",
       "Sample                                                                         \n",
       "Sample_01_Ctrl_morning_S1_                 0.000000                 0.628171   \n",
       "Sample_02_CRS_morning_S2_                  0.000000                 0.607949   \n",
       "Sample_03_Ctrl_morning_S3_                 0.000000                 1.368711   \n",
       "Sample_04_CRS_morning_S4_                  0.806931                 0.698171   \n",
       "Sample_05_Ctrl_morning_S5_                 2.437766                 2.109197   \n",
       "\n",
       "FeatureID                   99999000.0_-NC_000086.8  \n",
       "Sample                                               \n",
       "Sample_01_Ctrl_morning_S1_                 0.000000  \n",
       "Sample_02_CRS_morning_S2_                  0.000000  \n",
       "Sample_03_Ctrl_morning_S3_                 1.380718  \n",
       "Sample_04_CRS_morning_S4_                  0.000000  \n",
       "Sample_05_Ctrl_morning_S5_                 0.000000  \n",
       "\n",
       "[5 rows x 5300115 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>treatment</th>\n",
       "      <th>timepoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sample_01_Ctrl_morning_S1_</td>\n",
       "      <td>Ctrl</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sample_02_CRS_morning_S2_</td>\n",
       "      <td>CRS</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sample_03_Ctrl_morning_S3_</td>\n",
       "      <td>Ctrl</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sample_04_CRS_morning_S4_</td>\n",
       "      <td>CRS</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sample_05_Ctrl_morning_S5_</td>\n",
       "      <td>Ctrl</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Sample treatment timepoint\n",
       "0  Sample_01_Ctrl_morning_S1_      Ctrl   morning\n",
       "1   Sample_02_CRS_morning_S2_       CRS   morning\n",
       "2  Sample_03_Ctrl_morning_S3_      Ctrl   morning\n",
       "3   Sample_04_CRS_morning_S4_       CRS   morning\n",
       "4  Sample_05_Ctrl_morning_S5_      Ctrl   morning"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "\n",
    "file_path = '../data_normalized/cleaned_Normalized_1000.csv' #CHANGE PATH\n",
    "\n",
    "\n",
    "pivot_df, bin_ids, metadata_df = preprocess_data(file_path)\n",
    "\n",
    "# View the first few rows of processed data\n",
    "display(pivot_df.head())\n",
    "display(metadata_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76351f-aaf4-4697-b943-742831bfa2db",
   "metadata": {},
   "source": [
    "In the sorted unified z-scored, heatmap, we see the division of the groups more clearly. In the first group where the bins are only significant with treatment, we see that "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
