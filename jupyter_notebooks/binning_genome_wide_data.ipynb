{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning genome-wide data\n",
    "\n",
    "<div style=\"text-align: right\">\n",
    "    02.04.2025\n",
    "    <br>\n",
    "    Tae Kim, MSc\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.6 (main, Jun  7 2024, 07:09:59) [GCC 13.2.0]\n"
     ]
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import os\n",
    "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
    "os.environ['LANG'] = 'en_US.UTF-8'\n",
    "os.environ[\"MPLBACKEND\"] = \"TkAgg\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "matplotlib.rcParams['font.sans-serif'] = \"Arial\"\n",
    "matplotlib.rcParams['font.family'] = \"sans-serif\"\n",
    "matplotlib.rcParams['mathtext.default'] = \"regular\"\n",
    "\n",
    "matplotlib.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.26.4\n",
      "pandas 2.2.2\n",
      "matplotlib 3.9.0\n",
      "seaborn 0.13.2\n",
      "scipy 1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(\"numpy\", np.__version__)\n",
    "print(\"pandas\", pd.__version__)\n",
    "print(\"matplotlib\", matplotlib.__version__)\n",
    "print(\"seaborn\", sns.__version__)\n",
    "print(\"scipy\", scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of chromosome names in standard format\n",
    "# Generate chromosomes 1-19 plus sex chromosomes X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chr1',\n",
       " 'chr2',\n",
       " 'chr3',\n",
       " 'chr4',\n",
       " 'chr5',\n",
       " 'chr6',\n",
       " 'chr7',\n",
       " 'chr8',\n",
       " 'chr9',\n",
       " 'chr10',\n",
       " 'chr11',\n",
       " 'chr12',\n",
       " 'chr13',\n",
       " 'chr14',\n",
       " 'chr15',\n",
       " 'chr16',\n",
       " 'chr17',\n",
       " 'chr18',\n",
       " 'chr19',\n",
       " 'chrX',\n",
       " 'chrY']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chromosomes = ['chr' + str(i) for i in np.arange(1, 20, 1)] + [\"chrX\", \"chrY\"]\n",
    "chromosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RefSeq seq accession</th>\n",
       "      <th>UCSC style name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_000067.7</td>\n",
       "      <td>chr1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC_000068.8</td>\n",
       "      <td>chr2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC_000069.7</td>\n",
       "      <td>chr3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC_000070.7</td>\n",
       "      <td>chr4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC_000071.7</td>\n",
       "      <td>chr5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NC_000072.7</td>\n",
       "      <td>chr6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NC_000073.7</td>\n",
       "      <td>chr7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NC_000074.7</td>\n",
       "      <td>chr8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NC_000075.7</td>\n",
       "      <td>chr9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NC_000076.7</td>\n",
       "      <td>chr10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NC_000077.7</td>\n",
       "      <td>chr11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NC_000078.7</td>\n",
       "      <td>chr12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NC_000079.7</td>\n",
       "      <td>chr13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NC_000080.7</td>\n",
       "      <td>chr14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NC_000081.7</td>\n",
       "      <td>chr15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NC_000082.7</td>\n",
       "      <td>chr16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NC_000083.7</td>\n",
       "      <td>chr17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NC_000084.7</td>\n",
       "      <td>chr18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NC_000085.7</td>\n",
       "      <td>chr19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NC_000086.8</td>\n",
       "      <td>chrX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NC_000087.8</td>\n",
       "      <td>chrY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RefSeq seq accession UCSC style name\n",
       "0           NC_000067.7            chr1\n",
       "1           NC_000068.8            chr2\n",
       "2           NC_000069.7            chr3\n",
       "3           NC_000070.7            chr4\n",
       "4           NC_000071.7            chr5\n",
       "5           NC_000072.7            chr6\n",
       "6           NC_000073.7            chr7\n",
       "7           NC_000074.7            chr8\n",
       "8           NC_000075.7            chr9\n",
       "9           NC_000076.7           chr10\n",
       "10          NC_000077.7           chr11\n",
       "11          NC_000078.7           chr12\n",
       "12          NC_000079.7           chr13\n",
       "13          NC_000080.7           chr14\n",
       "14          NC_000081.7           chr15\n",
       "15          NC_000082.7           chr16\n",
       "16          NC_000083.7           chr17\n",
       "17          NC_000084.7           chr18\n",
       "18          NC_000085.7           chr19\n",
       "19          NC_000086.8            chrX\n",
       "20          NC_000087.8            chrY"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chromosomes_name_table = \"/nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Navnit/genomes/mouse/Chromosome_naming_table_17.09.2024_Vakil.tsv\"\n",
    "chromosomes_name_table = pd.read_csv(chromosomes_name_table, sep = \"\\t\", header = 0)\n",
    "chromosomes_name_table = chromosomes_name_table[chromosomes_name_table[\"UCSC style name\"].isin(chromosomes)]\n",
    "chromosomes_name_table = chromosomes_name_table.loc[:, [\"RefSeq seq accession\", \"UCSC style name\"]]\n",
    "chromosomes_name_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chromsomes/contigs in the genome assembly: 61\n",
      "Number of chromsomes/contigs in the genome assembly: 21\n"
     ]
    }
   ],
   "source": [
    "chr_sizesGRCh38 = \"/nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Navnit/genomes/mouse/GRCm39_NCBI_Bowtie2.fasta.fai\"\n",
    "DF_chrsizes = pd.read_csv(chr_sizesGRCh38, sep = \"\\t\", header = None)\n",
    "print(\"Number of chromsomes/contigs in the genome assembly:\", DF_chrsizes[0].nunique())\n",
    "DF_chrsizes = DF_chrsizes[DF_chrsizes[0].isin(chromosomes_name_table[\"RefSeq seq accession\"].values)]\n",
    "print(\"Number of chromsomes/contigs in the genome assembly:\", DF_chrsizes[0].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_000067.7</td>\n",
       "      <td>195154279</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NC_000068.8</td>\n",
       "      <td>181755017</td>\n",
       "      <td>199241831</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NC_000069.7</td>\n",
       "      <td>159745316</td>\n",
       "      <td>384026162</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NC_000070.7</td>\n",
       "      <td>156860686</td>\n",
       "      <td>546433963</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NC_000071.7</td>\n",
       "      <td>151758149</td>\n",
       "      <td>705911172</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NC_000072.7</td>\n",
       "      <td>149588044</td>\n",
       "      <td>861966554</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NC_000073.7</td>\n",
       "      <td>144995196</td>\n",
       "      <td>1014047796</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NC_000074.7</td>\n",
       "      <td>130127694</td>\n",
       "      <td>1161638654</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NC_000075.7</td>\n",
       "      <td>124359700</td>\n",
       "      <td>1293935206</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NC_000076.7</td>\n",
       "      <td>130530862</td>\n",
       "      <td>1420367632</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NC_000077.7</td>\n",
       "      <td>121973369</td>\n",
       "      <td>1553074073</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NC_000078.7</td>\n",
       "      <td>120092757</td>\n",
       "      <td>1677080396</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NC_000079.7</td>\n",
       "      <td>120883175</td>\n",
       "      <td>1799174763</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NC_000080.7</td>\n",
       "      <td>125139656</td>\n",
       "      <td>1922072722</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NC_000081.7</td>\n",
       "      <td>104073951</td>\n",
       "      <td>2049298103</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NC_000082.7</td>\n",
       "      <td>98008968</td>\n",
       "      <td>2155106684</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NC_000083.7</td>\n",
       "      <td>95294699</td>\n",
       "      <td>2254749199</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NC_000084.7</td>\n",
       "      <td>90720763</td>\n",
       "      <td>2351632207</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NC_000085.7</td>\n",
       "      <td>61420004</td>\n",
       "      <td>2443865047</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NC_000086.8</td>\n",
       "      <td>169476592</td>\n",
       "      <td>2506308781</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NC_000087.8</td>\n",
       "      <td>91455967</td>\n",
       "      <td>2679178579</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1           2   3   4\n",
       "0   NC_000067.7  195154279          63  60  61\n",
       "7   NC_000068.8  181755017   199241831  60  61\n",
       "8   NC_000069.7  159745316   384026162  60  61\n",
       "9   NC_000070.7  156860686   546433963  60  61\n",
       "11  NC_000071.7  151758149   705911172  60  61\n",
       "17  NC_000072.7  149588044   861966554  60  61\n",
       "18  NC_000073.7  144995196  1014047796  60  61\n",
       "20  NC_000074.7  130127694  1161638654  60  61\n",
       "21  NC_000075.7  124359700  1293935206  60  61\n",
       "22  NC_000076.7  130530862  1420367632  60  61\n",
       "23  NC_000077.7  121973369  1553074073  60  61\n",
       "24  NC_000078.7  120092757  1677080396  60  61\n",
       "25  NC_000079.7  120883175  1799174763  60  61\n",
       "26  NC_000080.7  125139656  1922072722  60  61\n",
       "27  NC_000081.7  104073951  2049298103  60  61\n",
       "28  NC_000082.7   98008968  2155106684  60  61\n",
       "29  NC_000083.7   95294699  2254749199  60  61\n",
       "30  NC_000084.7   90720763  2351632207  60  61\n",
       "31  NC_000085.7   61420004  2443865047  60  61\n",
       "32  NC_000086.8  169476592  2506308781  60  61\n",
       "34  NC_000087.8   91455967  2679178579  60  61"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_chrsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_chrsizes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 379M\n",
      "-rwxrwx--- 1 vtakhaveev vtakhaveev-group 198M Sep 16  2024 Sample_01_Ctrl_morning_S1_.GRCh38.p13_G_minus_strand.bedgraph\n",
      "-rwxrwx--- 1 vtakhaveev vtakhaveev-group 179M Sep 16  2024 Sample_01_Ctrl_morning_S1_.GRCh38.p13_G_plus_strand.bedgraph\n",
      "-rwxrwx--- 1 vtakhaveev vtakhaveev-group 2.0M Sep 16  2024 Sample_01_Ctrl_morning_S1_.GRCh38.p13.dedupl.filtered.forw.bam.bai\n",
      "-rwxrwx--- 1 vtakhaveev vtakhaveev-group  72K Sep 16  2024 Sample_01_Ctrl_morning_S1_.GRCh38.p13.dedupl.filtered.forw.STATS.txt\n",
      "-rwxrwx--- 1 vtakhaveev vtakhaveev-group  95K Sep 16  2024 Sample_01_Ctrl_morning_S1_.GRCh38.p13.dedupl.filtered.STATS.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lht /nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Vakil/Mouse_brain_Sept2024/mouse_brain_Sep2024_processed/Sample_01_Ctrl_morning_S1_/bed_and_bedgraph/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Sample_14_CRS_evening_S14_\n",
      "Sample_15_Ctrl_evening_S15_\n",
      "Sample_05_Ctrl_morning_S5_\n",
      "Sample_01_Ctrl_morning_S1_\n",
      "Sample_16_CRS_evening_S16_\n",
      "Sample_11_Ctrl_evening_S11_\n",
      "Sample_13_Ctrl_evening_S13_\n",
      "Sample_08_CRS_morning_S8_\n",
      "Sample_20_CRS_evening_S20_\n",
      "Sample_18_CRS_evening_S18_\n",
      "Sample_17_Ctrl_evening_S17_\n",
      "Sample_04_CRS_morning_S4_\n",
      "Sample_19_Ctrl_evening_S19_\n",
      "Sample_03_Ctrl_morning_S3_\n",
      "Sample_02_CRS_morning_S2_\n",
      "Sample_09_Ctrl_morning_S9_\n",
      "Sample_10_CRS_morning_S10_\n",
      "Sample_06_CRS_morning_S6_\n",
      "Sample_12_CRS_evening_S12_\n",
      "Sample_07_Ctrl_morning_S7_\n",
      "Finished\n",
      "Sample_14_CRS_evening_S14_\n",
      "Sample_15_Ctrl_evening_S15_\n",
      "Sample_05_Ctrl_morning_S5_\n",
      "Sample_01_Ctrl_morning_S1_\n",
      "Sample_16_CRS_evening_S16_\n",
      "Sample_11_Ctrl_evening_S11_\n",
      "Sample_13_Ctrl_evening_S13_\n",
      "Sample_08_CRS_morning_S8_\n",
      "Sample_20_CRS_evening_S20_\n",
      "Sample_18_CRS_evening_S18_\n",
      "Sample_17_Ctrl_evening_S17_\n",
      "Sample_04_CRS_morning_S4_\n",
      "Sample_19_Ctrl_evening_S19_\n",
      "Sample_03_Ctrl_morning_S3_\n",
      "Sample_02_CRS_morning_S2_\n",
      "Sample_09_Ctrl_morning_S9_\n",
      "Sample_10_CRS_morning_S10_\n",
      "Sample_06_CRS_morning_S6_\n",
      "Sample_12_CRS_evening_S12_\n",
      "Sample_07_Ctrl_morning_S7_\n",
      "Finished\n",
      "Sample_14_CRS_evening_S14_\n",
      "Sample_15_Ctrl_evening_S15_\n",
      "Sample_05_Ctrl_morning_S5_\n",
      "Sample_01_Ctrl_morning_S1_\n",
      "Sample_16_CRS_evening_S16_\n",
      "Sample_11_Ctrl_evening_S11_\n",
      "Sample_13_Ctrl_evening_S13_\n",
      "Sample_08_CRS_morning_S8_\n",
      "Sample_20_CRS_evening_S20_\n",
      "Sample_18_CRS_evening_S18_\n",
      "Sample_17_Ctrl_evening_S17_\n",
      "Sample_04_CRS_morning_S4_\n",
      "Sample_19_Ctrl_evening_S19_\n",
      "Sample_03_Ctrl_morning_S3_\n",
      "Sample_02_CRS_morning_S2_\n",
      "Sample_09_Ctrl_morning_S9_\n",
      "Sample_10_CRS_morning_S10_\n",
      "Sample_06_CRS_morning_S6_\n",
      "Sample_12_CRS_evening_S12_\n",
      "Sample_07_Ctrl_morning_S7_\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###\n",
    "###Tae, please change the paths.\n",
    "###Keep strands.\n",
    "###\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "#'''\n",
    "#Enough to run once\n",
    "\n",
    "PATH = \"/nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Vakil/Mouse_brain_Sept2024/mouse_brain_Sep2024_processed/\"\n",
    "prefix = \"\"\n",
    "suffix = \".GRCh38.p13_G_\"\n",
    "\n",
    "OUTPATH = \"/cluster/home/taekim/stressed_mice/data_binning_normalization/\"\n",
    "\n",
    "BINSIZEs = [float(10**3), float(10**4), float(10**5)]#run on 17.09.2024\n",
    "print(\"test\")\n",
    "\n",
    "for BINSIZE in BINSIZEs:\n",
    "    DF_damage_binned = pd.DataFrame({})\n",
    "\n",
    "    for sample in os.listdir(PATH):\n",
    "                #S = sample.split(\"_\")[1].replace(\"-Sample\", \"\")\n",
    "            \n",
    "                df1 = os.path.join(PATH, sample, \"bed_and_bedgraph\", prefix + sample + suffix + \"plus\" + \"_strand.bedgraph\")\n",
    "                df1 = pd.read_csv(df1, sep = \"\\t\", header = None, names = [\"Chr\", \"Start\", \"End\", \"Value\", \"MAPQ\"])\n",
    "                df2 = os.path.join(PATH, sample, \"bed_and_bedgraph\", prefix + sample + suffix + \"minus\" + \"_strand.bedgraph\")\n",
    "                df2 = pd.read_csv(df2, sep = \"\\t\", header = None, names = [\"Chr\", \"Start\", \"End\", \"Value\", \"MAPQ\"])\n",
    "\n",
    "                for chromosome in DF_chrsizes[0].values:#chromosomes:                    \n",
    "                    chr_length = float(DF_chrsizes[DF_chrsizes[0] == chromosome][1].iloc[0])\n",
    "                    bin_borders = list(np.arange(0, chr_length, BINSIZE))\n",
    "                    if bin_borders[-1] != chr_length - 1:\n",
    "                        bin_borders += [chr_length - 1]\n",
    "                    bin_borders = np.array(bin_borders)\n",
    "                    sizes_array = np.diff(bin_borders)\n",
    "                    sizes_array = [sizes_array[0] + 1] + list(sizes_array[1:])#we made the left boundary of the first bin inclusive, therefore, the length is 1 bp bigger\n",
    "                    template = pd.DataFrame({\"Bin\" : bin_borders[:-1], \"Bin_size\" : sizes_array})\n",
    "                    \n",
    "\n",
    "                    df1_ch = df1[df1[\"Chr\"] == chromosome].copy().reset_index(drop = True)\n",
    "                    df2_ch = df2[df2[\"Chr\"] == chromosome].copy().reset_index(drop = True)\n",
    "\n",
    "                    df1_ch.loc[:, 'Bin'] = pd.cut(df1_ch[\"Start\"], bins = bin_borders, labels = bin_borders[:-1], \n",
    "                                                  include_lowest = True, right = True)\n",
    "                    if df1_ch.shape[0] - df1_ch.dropna().shape[0] > 0:\n",
    "                        print(\"Warning\")\n",
    "                    df2_ch.loc[:, 'Bin'] = pd.cut(df2_ch[\"Start\"], bins = bin_borders, labels = bin_borders[:-1], \n",
    "                                                  include_lowest = True, right = True)\n",
    "                    if df2_ch.shape[0] - df2_ch.dropna().shape[0] > 0:\n",
    "                        print(\"Warning\")\n",
    "                    df1_ch.loc[:, 'Bin'] = df1_ch['Bin'].astype(float)\n",
    "                    df2_ch.loc[:, 'Bin'] = df2_ch['Bin'].astype(float)\n",
    "                    df1_ch = pd.merge(template, df1_ch, on = \"Bin\", how = \"left\").loc[:, [\"Value\", \"Bin\", \"Bin_size\"]].astype(float).fillna(0)\n",
    "                    df2_ch = pd.merge(template, df2_ch, on = \"Bin\", how = \"left\").loc[:, [\"Value\", \"Bin\", \"Bin_size\"]].astype(float).fillna(0)\n",
    "                    df1_ch = df1_ch.groupby(by = [\"Bin\", \"Bin_size\"]).sum().reset_index()\n",
    "                    df2_ch = df2_ch.groupby(by = [\"Bin\", \"Bin_size\"]).sum().reset_index()\n",
    "\n",
    "                    df1_ch.loc[:, \"Strand\"] = \"+\" \n",
    "                    df2_ch.loc[:, \"Strand\"] = \"-\" \n",
    "                    df1_ch.loc[:, \"Damage\"] = df1_ch[\"Value\"]\n",
    "                    df2_ch.loc[:, \"Damage\"] = df2_ch[\"Value\"]\n",
    "                    df1_ch.loc[:, \"Chromosome\"] = chromosome\n",
    "                    df2_ch.loc[:, \"Chromosome\"] = chromosome\n",
    "                    df1_ch.loc[:, \"Sample\"] = sample\n",
    "                    df2_ch.loc[:, \"Sample\"] = sample\n",
    "\n",
    "                    df1_ch = df1_ch.loc[:, [\"Bin\", \"Bin_size\", \"Damage\", \"Chromosome\", \"Sample\", \"Strand\"]]\n",
    "                    df2_ch = df2_ch.loc[:, [\"Bin\", \"Bin_size\", \"Damage\", \"Chromosome\", \"Sample\", \"Strand\"]]\n",
    "            \n",
    "                    df_ch = pd.concat([df1_ch, df2_ch])\n",
    "                    df_ch = df_ch.sort_values(by = [\"Bin\", \"Strand\"], ascending = [True, True])\n",
    "                    \n",
    "                    DF_damage_binned = pd.concat([DF_damage_binned, df_ch])\n",
    "\n",
    "                print(sample)\n",
    "\n",
    "    DF_damage_binned = DF_damage_binned.reset_index(drop = True)\n",
    "\n",
    "    DF_damage_binned.to_csv(OUTPATH + \"Binned_damage_GENOMEWIDE_strand\" + str(int(BINSIZE)) + \"_CCS.20_mice_Sept2024.csv\")\n",
    "    print (\"Finished\")\n",
    "\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some work in the terminal:\n",
    "\n",
    "<code>cd /nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Vakil/Mouse_brain_Sept2024/Binning_normalization/\n",
    "module load stack/2024-06 gcc/12.2.0 bedtools2/2.31.0\n",
    "bedtools getfasta -fi /nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Navnit/genomes/mouse/GRCm39_NCBI_Bowtie2.fasta -bed Genome_bins_100000.mouse.bed -bedOut > Genome_bins_100000.mouse.SEQ.bed\n",
    "bedtools getfasta -fi /nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Navnit/genomes/mouse/GRCm39_NCBI_Bowtie2.fasta -bed Genome_bins_10000.mouse.bed -bedOut > Genome_bins_10000.mouse.SEQ.bed\n",
    "bedtools getfasta -fi /nfs/nas12.ethz.ch/fs1201/green_groups_let_public/Euler/Navnit/genomes/mouse/GRCm39_NCBI_Bowtie2.fasta -bed Genome_bins_1000.mouse.bed -bedOut > Genome_bins_1000.mouse.SEQ.bed\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2723425\n",
      "272352\n",
      "27243\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###\n",
    "###Tae, please change the paths.\n",
    "###In my code, I count G and C together. (C is G on the opposite strand - complementarity principle). Since your analysis is strand specific, \n",
    "###you should count G and C separately. Number of G is the number of G on the + strand. Nubmer of C is the number C on the - strand.\n",
    "###\n",
    "\n",
    "\n",
    "#'''\n",
    "BINSIZEs = [float(10**3), float(10**4), float(10**5)]#run on 17.09.2024\n",
    "\n",
    "for BINSIZE in BINSIZEs:\n",
    "    PATH = \"/cluster/home/taekim/stressed_mice/data_binning_normalization/\"\n",
    "    df = pd.read_csv(PATH + \"Genome_bins_\" + str(int(BINSIZE)) + \".mouse.SEQ.bed\", header = None, index_col = None, sep = \"\\t\")\n",
    "    print(df.shape[0])\n",
    "\n",
    "    df.loc[:, \"Seq\"] = df[4].str.upper()\n",
    "    df.loc[:, \"G_count\"] = df[\"Seq\"].str.count(\"G\")  # For plus strand\n",
    "    df.loc[:, \"C_count\"] = df[\"Seq\"].str.count(\"C\")  # For minus strand\n",
    "    df = df.loc[:, [0, 3, \"G_count\", \"C_count\"]]\n",
    "    df.to_csv(PATH + \"Genome_bins_\" + str(int(BINSIZE)) + \".G_counts.csv\")\n",
    "    df\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2723425\n",
      "272352\n",
      "27243\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###\n",
    "###Tae, please change the paths.\n",
    "###In my code, I count G and C together. (C is G on the opposite strand - complementarity principle). Since your analysis is strand specific, \n",
    "###you should count G and C separately. Number of G is the number of G on the + strand. Nubmer of C is the number C on the - strand.\n",
    "###\n",
    "\n",
    "\n",
    "#'''\n",
    "BINSIZEs = [float(10**3), float(10**4), float(10**5)]#run on 17.09.2024\n",
    "\n",
    "for BINSIZE in BINSIZEs:\n",
    "    PATH = \"/cluster/home/taekim/stressed_mice/data_binning_normalization/\"\n",
    "    df = pd.read_csv(PATH + \"Genome_bins_\" + str(int(BINSIZE)) + \".mouse.SEQ.bed\", header = None, index_col = None, sep = \"\\t\")\n",
    "    print(df.shape[0])\n",
    "\n",
    "    df.loc[:, \"Seq\"] = df[4].str.upper()\n",
    "    df.loc[:, \"G_count\"] = df[\"Seq\"].str.count(\"G\")  # For plus strand\n",
    "    df.loc[:, \"C_count\"] = df[\"Seq\"].str.count(\"C\")  # For minus strand\n",
    "    df = df.loc[:, [0, 3, \"G_count\", \"C_count\"]]\n",
    "    df.to_csv(PATH + \"Genome_bins_\" + str(int(BINSIZE)) + \".G_counts.csv\")\n",
    "    df\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n",
      "Finished\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###\n",
    "###Tae, please change the paths.\n",
    "###The idea of this code is to create coordinates of bins so that the code below can fetch reference sequences of the bins and then we could count G \n",
    "###\n",
    "###\n",
    "\n",
    "#'''\n",
    "OUTPATH = \"/cluster/home/taekim/stressed_mice/data_binning_normalization/\"\n",
    "\n",
    "BINSIZEs = [float(10**3), float(10**4), float(10**5)]#run on 17.09.2024\n",
    "\n",
    "for BINSIZE in BINSIZEs:\n",
    "    DF_bins = pd.DataFrame({})\n",
    "\n",
    "    for chromosome in DF_chrsizes[0].values:#chromosomes:                    \n",
    "        chr_length = float(DF_chrsizes[DF_chrsizes[0] == chromosome][1].iloc[0])\n",
    "        bin_borders = list(np.arange(0, chr_length, BINSIZE))\n",
    "\n",
    "        if bin_borders[-1] != chr_length - 1:\n",
    "            bin_borders += [chr_length - 1]\n",
    "        bin_borders = np.array(bin_borders, dtype = int)\n",
    "\n",
    "        #aligning bed's [start, end) with pd.cut's (start, end] + include_lowest = Ture\n",
    "        starts = [0] + list(bin_borders[1:-1] + 1)\n",
    "        ends = list(bin_borders[1:] + 1)\n",
    "\n",
    "        template = pd.DataFrame({\"Chromosome\" : [chromosome]*len(starts), \"Bin_start\" : starts, \"Bin_end\" : ends, \"Bin\" : bin_borders[:-1]})\n",
    "\n",
    "        DF_bins = pd.concat([DF_bins, template])\n",
    "\n",
    "    DF_bins = DF_bins.reset_index(drop = True)\n",
    "    DF_bins.to_csv(OUTPATH + \"Genome_bins_\" + str(int(BINSIZE)) + \".mouse.bed\", index = False, header = False, sep = \"\\t\")\n",
    "    print(\"Finished\")\n",
    "    \n",
    "    DF_bins \n",
    "    \n",
    "#'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NC_000067.7     0  1001   0.1  \\\n",
      "0  NC_000067.7  1001  2001  1000   \n",
      "1  NC_000067.7  2001  3001  2000   \n",
      "2  NC_000067.7  3001  4001  3000   \n",
      "3  NC_000067.7  4001  5001  4000   \n",
      "4  NC_000067.7  5001  6001  5000   \n",
      "\n",
      "  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN  \n",
      "0  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "1  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "2  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "3  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "4  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "['NC_000067.7', '0', '1001', '0.1', 'NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN']\n",
      "   NC_000067.7     0  1001   0.1\n",
      "0  NC_000067.7  1001  2001  1000\n",
      "1  NC_000067.7  2001  3001  2000\n",
      "2  NC_000067.7  3001  4001  3000\n",
      "3  NC_000067.7  4001  5001  4000\n",
      "4  NC_000067.7  5001  6001  5000\n",
      "['NC_000067.7', '0', '1001', '0.1']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000.0\n",
      "27243\n",
      "1089720 54486.0\n",
      "531700 26585.0\n",
      "531660 26583.0\n",
      "                        Sample     Median\n",
      "0   Sample_14_CRS_evening_S14_   2.546314\n",
      "0  Sample_15_Ctrl_evening_S15_   3.570246\n",
      "0   Sample_05_Ctrl_morning_S5_   6.184096\n",
      "0   Sample_01_Ctrl_morning_S1_   6.921409\n",
      "0   Sample_16_CRS_evening_S16_   2.879618\n",
      "0  Sample_11_Ctrl_evening_S11_   2.485161\n",
      "0  Sample_13_Ctrl_evening_S13_   2.403964\n",
      "0    Sample_08_CRS_morning_S8_   5.246539\n",
      "0   Sample_20_CRS_evening_S20_  10.633666\n",
      "0   Sample_18_CRS_evening_S18_   4.602043\n",
      "0  Sample_17_Ctrl_evening_S17_   2.929046\n",
      "0    Sample_04_CRS_morning_S4_   6.227455\n",
      "0  Sample_19_Ctrl_evening_S19_   2.545900\n",
      "0   Sample_03_Ctrl_morning_S3_   9.529751\n",
      "0    Sample_02_CRS_morning_S2_   7.151634\n",
      "0   Sample_09_Ctrl_morning_S9_   7.923174\n",
      "0   Sample_10_CRS_morning_S10_   8.815860\n",
      "0    Sample_06_CRS_morning_S6_   7.930738\n",
      "0   Sample_12_CRS_evening_S12_   2.849694\n",
      "0   Sample_07_Ctrl_morning_S7_   4.924262\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###\n",
    "###Tae, please change the paths.\n",
    "###This code is to calculate normalization factors. \n",
    "###You need to incorporate strands.\n",
    "###\n",
    "###\n",
    "\n",
    "PATH = \"/cluster/home/taekim/stressed_mice/data_binning_normalization/\"\n",
    "BINSIZEs = [float(10**5)]\n",
    "for BINSIZE in BINSIZEs:\n",
    "    print(BINSIZE)\n",
    "    ###G counts per bin according to reference genome\n",
    "    DF_Gs = pd.read_csv(PATH + \"Genome_bins_\" + str(int(BINSIZE)) + \".G_counts.csv\", index_col = 0)\n",
    "    DF_Gs = DF_Gs.rename(columns={\"0\" : \"Chromosome\", \"3\" : \"Bin\"})\n",
    "    print(DF_Gs.shape[0])\n",
    "    \n",
    "    ###Binned damage data\n",
    "    tmp = pd.read_csv(PATH + \"Binned_damage_GENOMEWIDE_strand\" + str(int(BINSIZE)) + \"_CCS.20_mice_Sept2024.csv\")\n",
    "    print(tmp.shape[0], tmp.shape[0]/len(tmp[\"Sample\"].unique()))\n",
    "\n",
    "    # Create separate dataframes for plus and minus strands\n",
    "    tmp_plus = tmp[tmp[\"Strand\"] == \"+\"].copy()\n",
    "    tmp_minus = tmp[tmp[\"Strand\"] == \"-\"].copy()\n",
    "\n",
    "    ###Normalizing the binned damage data for G count\n",
    "    tmp_plus = pd.merge(tmp_plus, DF_Gs[DF_Gs[\"G_count\"] > 0], on = (\"Chromosome\", \"Bin\"), how = \"inner\")\n",
    "    print(tmp_plus.shape[0], tmp_plus.shape[0]/len(tmp_plus[\"Sample\"].unique()))\n",
    "    \n",
    "    tmp_plus.loc[:, \"Damage\"] = (10**3)*tmp_plus[\"Damage\"]/tmp_plus[\"G_count\"]\n",
    "\n",
    "    ###Normalizing the binned damage data for C count\n",
    "    tmp_minus = pd.merge(tmp_minus, DF_Gs[DF_Gs[\"C_count\"] > 0], on = (\"Chromosome\", \"Bin\"), how = \"inner\")\n",
    "    print(tmp_minus.shape[0], tmp_minus.shape[0]/len(tmp_minus[\"Sample\"].unique()))\n",
    "    \n",
    "    tmp_minus.loc[:, \"Damage\"] = (10**3)*tmp_minus[\"Damage\"]/tmp_minus[\"C_count\"]\n",
    "\n",
    "    tmp_normalized = pd.concat([tmp_plus, tmp_minus])\n",
    "\n",
    "    NF_df = pd.DataFrame({})\n",
    "    for sample in list(tmp_normalized[\"Sample\"].unique()): #one median per sample\n",
    "        tmp_sample = tmp_normalized[tmp_normalized[\"Sample\"] == sample]\n",
    "        M = np.median(tmp_sample[\"Damage\"].values)\n",
    "        NF_df = pd.concat([NF_df, pd.DataFrame({\"Sample\": [sample], \"Median\": [M]})])\n",
    "\n",
    "    NF_df.to_csv(PATH + \"NFstrand\" + str(int(BINSIZE)) + \".csv\")\n",
    "    print(NF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bin size: 1000\n",
      "Normalization completed for bin size 1000\n",
      "Processing bin size: 10000\n",
      "Normalization completed for bin size 10000\n",
      "Processing bin size: 100000\n",
      "Normalization completed for bin size 100000\n"
     ]
    }
   ],
   "source": [
    "# Define paths and bin sizes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BINSIZEs = [float(10**3), float(10**4), float(10**5)]\n",
    "PATH = \"/cluster/home/taekim/stressed_mice/data_binning_normalization/\"\n",
    "\n",
    "for BINSIZE in BINSIZEs:\n",
    "    binsize_str = str(int(BINSIZE))\n",
    "    print(f\"Processing bin size: {binsize_str}\")\n",
    "    \n",
    "    # Load the damage data with bins\n",
    "    damage_file = PATH + \"Binned_damage_GENOMEWIDE_strand\" + binsize_str + \"_CCS.20_mice_Sept2024.csv\"\n",
    "    damage_df = pd.read_csv(damage_file, index_col=0)\n",
    "    \n",
    "    # Load the nucleotide count reference\n",
    "    nuc_file = PATH + \"Genome_bins_\" + binsize_str + \".G_counts.csv\"\n",
    "    nuc_df = pd.read_csv(nuc_file, index_col=0)\n",
    "    \n",
    "    # Load the pre-calculated median normalization factors\n",
    "    median_factors_file = PATH + \"NFstrand100000\" + \".csv\" #only use median with this bin size\n",
    "    median_factors = pd.read_csv(median_factors_file)\n",
    "    \n",
    "    # Rename and prepare the reference dataframe columns\n",
    "    nuc_df.rename(columns={\"0\": \"Chromosome\", \"3\": \"Bin\"}, inplace=True)\n",
    "    \n",
    "    # Ensure Bin column is in the same data type\n",
    "    damage_df[\"Bin\"] = damage_df[\"Bin\"].astype(float)\n",
    "    nuc_df[\"Bin\"] = nuc_df[\"Bin\"].astype(float)\n",
    "    \n",
    "    # Merge the datasets on Chromosome and Bin\n",
    "    merged_df = pd.merge(\n",
    "        damage_df, \n",
    "        nuc_df, \n",
    "        on=[\"Chromosome\", \"Bin\"], \n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Create a new column for normalized damage\n",
    "    # For '+' strand, normalize by G_count; for '-' strand, normalize by C_count\n",
    "    merged_df[\"GC_count\"] = np.where(\n",
    "        merged_df[\"Strand\"] == \"+\",\n",
    "        merged_df[\"G_count\"],\n",
    "        merged_df[\"C_count\"]\n",
    "    )\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    merged_df[\"GC_count\"] = merged_df[\"GC_count\"].replace(0, np.nan)\n",
    "    \n",
    "    # Calculate normalized damage by G or C count\n",
    "    # scale like the code above. Calculated for each bin size \n",
    "    merged_df[\"GC_Normalized_Damage\"] = (10**3) * merged_df[\"Damage\"] / merged_df[\"GC_count\"]\n",
    "    \n",
    "    # Merge with median normalization factors\n",
    "    merged_df = pd.merge(merged_df, median_factors, on=\"Sample\", how=\"left\")\n",
    "    \n",
    "    # Apply median normalization\n",
    "    merged_df[\"Median_Normalized_Damage\"] = merged_df[\"GC_Normalized_Damage\"] / merged_df[\"Median\"]\n",
    "    \n",
    "    # Save the normalized data\n",
    "    output_file = PATH + \"Normalized_\" + binsize_str + \".csv\"\n",
    "    merged_df.to_csv(output_file)\n",
    "    \n",
    "    print(f\"Normalization completed for bin size {binsize_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
