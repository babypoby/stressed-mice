{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea859c3-150c-4e59-8614-678e962b571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_xgboost_feature_selection_by_factor(input_file_path, output_data_dir=None, output_plots_dir=None, top_n=50):\n",
    "    \"\"\"\n",
    "    Perform XGBoost feature selection analysis on genomic data, \n",
    "    with separate analyses for each factor of interest.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file_path : str\n",
    "        Path to the input CSV file with genomic data\n",
    "    output_data_dir : str, optional\n",
    "        Directory where output data files (CSVs) will be saved. If None, uses the current directory.\n",
    "    output_plots_dir : str, optional\n",
    "        Directory where output plot files (PNGs) will be saved. If None, uses the same as output_data_dir.\n",
    "    top_n : int, optional\n",
    "        Number of top features to highlight in detailed analysis, default is 50\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results_dict : dict\n",
    "        Dictionary containing results for each factor analysis\n",
    "    importance_dict : dict\n",
    "        Dictionary containing feature importance DataFrames for each factor\n",
    "    performance_dict : dict\n",
    "        Dictionary containing performance metrics for each factor model\n",
    "    \"\"\"\n",
    "    # Set default output directories if none provided\n",
    "    if output_data_dir is None:\n",
    "        output_data_dir = ''\n",
    "    \n",
    "    if output_plots_dir is None:\n",
    "        output_plots_dir = output_data_dir\n",
    "    \n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(output_data_dir, exist_ok=True)\n",
    "    os.makedirs(output_plots_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Output data files will be saved to: {output_data_dir}\")\n",
    "    print(f\"Output plot files will be saved to: {output_plots_dir}\")\n",
    "    \n",
    "    # Define a mapping from NCBI RefSeq accessions to UCSC chromosome names\n",
    "    # Based on standard mouse genome mapping (assuming this is mouse data from the RefSeq IDs)\n",
    "    chrom_dict = {\n",
    "        'NC_000067.7': 'chr1',\n",
    "        'NC_000067.8': 'chr1',\n",
    "        'NC_000068.7': 'chr2',\n",
    "        'NC_000068.8': 'chr2',\n",
    "        'NC_000069.7': 'chr3',\n",
    "        'NC_000069.8': 'chr3',\n",
    "        'NC_000070.7': 'chr4',\n",
    "        'NC_000070.8': 'chr4',\n",
    "        'NC_000071.7': 'chr5',\n",
    "        'NC_000071.8': 'chr5',\n",
    "        'NC_000072.7': 'chr6',\n",
    "        'NC_000072.8': 'chr6',\n",
    "        'NC_000073.7': 'chr7',\n",
    "        'NC_000073.8': 'chr7',\n",
    "        'NC_000074.7': 'chr8',\n",
    "        'NC_000074.8': 'chr8',\n",
    "        'NC_000075.7': 'chr9',\n",
    "        'NC_000075.8': 'chr9',\n",
    "        'NC_000076.7': 'chr10',\n",
    "        'NC_000076.8': 'chr10',\n",
    "        'NC_000077.7': 'chr11',\n",
    "        'NC_000077.8': 'chr11',\n",
    "        'NC_000078.7': 'chr12',\n",
    "        'NC_000078.8': 'chr12',\n",
    "        'NC_000079.7': 'chr13',\n",
    "        'NC_000079.8': 'chr13',\n",
    "        'NC_000080.7': 'chr14',\n",
    "        'NC_000080.8': 'chr14',\n",
    "        'NC_000081.7': 'chr15',\n",
    "        'NC_000081.8': 'chr15',\n",
    "        'NC_000082.7': 'chr16',\n",
    "        'NC_000082.8': 'chr16',\n",
    "        'NC_000083.7': 'chr17',\n",
    "        'NC_000083.8': 'chr17',\n",
    "        'NC_000084.7': 'chr18',\n",
    "        'NC_000084.8': 'chr18',\n",
    "        'NC_000085.7': 'chr19',\n",
    "        'NC_000085.8': 'chr19',\n",
    "        'NC_000086.8': 'chrX',\n",
    "        'NC_000086.9': 'chrX',\n",
    "        'NC_000087.8': 'chrY',\n",
    "        'NC_000087.9': 'chrY',\n",
    "        'NC_005089.1': 'chrM'\n",
    "    }\n",
    "    \n",
    "    # Function to parse bin_id and create new UCSC style ID\n",
    "    def create_ucsc_style_id(bin_id):\n",
    "        match = re.match(r'^(\\d+\\.\\d+)_([+-])(.+)$', bin_id)\n",
    "        if match:\n",
    "            index = float(match.group(1))\n",
    "            strand = match.group(2)\n",
    "            ref_seq = match.group(3)\n",
    "            \n",
    "            if ref_seq in chrom_dict:\n",
    "                # Convert index to integer\n",
    "                return f\"{chrom_dict[ref_seq]}{strand}{int(index)}\"\n",
    "        return bin_id  # Return original if conversion fails\n",
    "        \n",
    "    # Read the data\n",
    "    print(f\"Reading data from {input_file_path}...\")\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    # Create feature IDs first\n",
    "    df['FeatureID'] = df['Bin'].astype(str) + '_' + df['Strand'] + df['Chromosome'].astype(str)\n",
    "    \n",
    "    # Now convert these IDs to UCSC style\n",
    "    df['UCSCFeatureID'] = df['FeatureID'].apply(create_ucsc_style_id)\n",
    "    \n",
    "    # Create pivot table with UCSC-style IDs\n",
    "    pivot_df = df.pivot_table(index='Sample', \n",
    "                             columns='UCSCFeatureID', \n",
    "                             values='Median_Normalized_Damage',\n",
    "                             aggfunc='mean')\n",
    "    \n",
    "    print(f\"Data shape: {pivot_df.shape} (samples × features)\")\n",
    "    \n",
    "    # Extract sample groups and components\n",
    "    def extract_group_components(sample_name):\n",
    "        match = re.search(r'(CRS|Ctrl)_(evening|morning)', sample_name)\n",
    "        if match:\n",
    "            treatment = match.group(1)  # CRS or Ctrl\n",
    "            time = match.group(2)       # evening or morning\n",
    "            full_group = match.group(0)  # e.g., \"CRS_evening\"\n",
    "            return full_group, treatment, time\n",
    "        else:\n",
    "            return \"Unknown\", \"Unknown\", \"Unknown\"\n",
    "    \n",
    "    # Create target variables\n",
    "    samples = pivot_df.index\n",
    "    sample_info = [extract_group_components(sample) for sample in samples]\n",
    "    full_groups = [info[0] for info in sample_info]\n",
    "    treatments = [info[1] for info in sample_info]\n",
    "    times = [info[2] for info in sample_info]\n",
    "    \n",
    "    # Print group distribution\n",
    "    print(\"\\nSample group distribution:\")\n",
    "    for group, count in zip(*np.unique(full_groups, return_counts=True)):\n",
    "        print(f\"  {group}: {count} samples\")\n",
    "    \n",
    "    # Prepare feature data\n",
    "    X = pivot_df.values\n",
    "    X = np.nan_to_num(X, nan=0.0)  # Replace NaN with zero\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Define the factors to analyze\n",
    "    factor_analyses = {\n",
    "        'full_groups': full_groups,               # All groups (CRS_evening, CRS_morning, Ctrl_evening, Ctrl_morning)\n",
    "        'treatment': treatments,                  # CRS vs Ctrl (regardless of time)\n",
    "        'time': times,                            # evening vs morning (regardless of treatment)\n",
    "        'CRS_time': [t for i, t in enumerate(times) if treatments[i] == 'CRS'],  # Time effect within CRS\n",
    "        'Ctrl_time': [t for i, t in enumerate(times) if treatments[i] == 'Ctrl'],  # Time effect within Ctrl\n",
    "        'evening_treatment': [tr for i, tr in enumerate(treatments) if times[i] == 'evening'],  # Treatment effect in evening\n",
    "        'morning_treatment': [tr for i, tr in enumerate(treatments) if times[i] == 'morning']   # Treatment effect in morning\n",
    "    }\n",
    "    \n",
    "    # Filter indices for subset analyses\n",
    "    CRS_indices = [i for i, tr in enumerate(treatments) if tr == 'CRS']\n",
    "    Ctrl_indices = [i for i, tr in enumerate(treatments) if tr == 'Ctrl']\n",
    "    evening_indices = [i for i, t in enumerate(times) if t == 'evening']\n",
    "    morning_indices = [i for i, t in enumerate(times) if t == 'morning']\n",
    "    \n",
    "    factor_indices = {\n",
    "        'full_groups': list(range(len(full_groups))),\n",
    "        'treatment': list(range(len(treatments))),\n",
    "        'time': list(range(len(times))),\n",
    "        'CRS_time': CRS_indices,\n",
    "        'Ctrl_time': Ctrl_indices,\n",
    "        'evening_treatment': evening_indices,\n",
    "        'morning_treatment': morning_indices\n",
    "    }\n",
    "    \n",
    "    # Store results\n",
    "    results_dict = {}\n",
    "    importance_dict = {}\n",
    "    performance_dict = {}  # Dictionary for performance metrics\n",
    "    \n",
    "    # Prepare DataFrame to store top features from all factors\n",
    "    all_top_features_df = pd.DataFrame()\n",
    "    \n",
    "    # Run analysis for each factor\n",
    "    for factor_name, factor_values in factor_analyses.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Analyzing factor: {factor_name}\")\n",
    "        \n",
    "        # Get relevant indices for this analysis\n",
    "        indices = factor_indices[factor_name]\n",
    "        \n",
    "        # Skip if not enough samples\n",
    "        if len(indices) < 4:  # Need at least a few samples for meaningful analysis\n",
    "            print(f\"Skipping {factor_name} due to insufficient samples.\")\n",
    "            continue\n",
    "            \n",
    "        # Filter data\n",
    "        X_factor = X_scaled[indices]\n",
    "        y_factor = np.array(factor_values)\n",
    "        \n",
    "        # Skip if only one class\n",
    "        unique_classes = np.unique(y_factor)\n",
    "        if len(unique_classes) < 2:\n",
    "            print(f\"Skipping {factor_name} due to only one class: {unique_classes[0]}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Classes for {factor_name}: {unique_classes}\")\n",
    "        print(f\"Class distribution: {[np.sum(y_factor == c) for c in unique_classes]}\")\n",
    "        \n",
    "        # Encode labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y_factor)\n",
    "        \n",
    "        # Create XGBoost classifier\n",
    "        print(f\"Training XGBoost for {factor_name}...\")\n",
    "        \n",
    "        # Configure XGBoost parameters\n",
    "        xgb_params = {\n",
    "            'n_estimators': 500,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': 20,\n",
    "            'eval_metric': 'logloss',\n",
    "            'verbosity': 0  # Suppress XGBoost output\n",
    "        }\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        class_counts = np.bincount(y_encoded)\n",
    "        n_classes = len(class_counts)\n",
    "        if n_classes == 2:\n",
    "            # For binary classification, use scale_pos_weight\n",
    "            scale_pos_weight = class_counts[0] / class_counts[1]\n",
    "            xgb_params['scale_pos_weight'] = scale_pos_weight\n",
    "            xgb_classifier = xgb.XGBClassifier(**xgb_params)\n",
    "        else:\n",
    "            # For multiclass, XGBoost doesn't have direct class_weight, but we can use sample_weight\n",
    "            xgb_classifier = xgb.XGBClassifier(**xgb_params)\n",
    "        \n",
    "        # Performance evaluation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Calculate cross-validation scores for multiple metrics\n",
    "        accuracy_scores = cross_val_score(xgb_classifier, X_factor, y_encoded, cv=cv, scoring='accuracy')\n",
    "        f1_scores = cross_val_score(xgb_classifier, X_factor, y_encoded, cv=cv, scoring='f1_weighted')\n",
    "        precision_scores = cross_val_score(xgb_classifier, X_factor, y_encoded, cv=cv, scoring='precision_weighted')\n",
    "        recall_scores = cross_val_score(xgb_classifier, X_factor, y_encoded, cv=cv, scoring='recall_weighted')\n",
    "        \n",
    "        # Store performance metrics\n",
    "        performance_dict[factor_name] = {\n",
    "            'accuracy': {\n",
    "                'mean': np.mean(accuracy_scores),\n",
    "                'std': np.std(accuracy_scores),\n",
    "                'values': accuracy_scores\n",
    "            },\n",
    "            'f1': {\n",
    "                'mean': np.mean(f1_scores),\n",
    "                'std': np.std(f1_scores),\n",
    "                'values': f1_scores\n",
    "            },\n",
    "            'precision': {\n",
    "                'mean': np.mean(precision_scores),\n",
    "                'std': np.std(precision_scores),\n",
    "                'values': precision_scores\n",
    "            },\n",
    "            'recall': {\n",
    "                'mean': np.mean(recall_scores),\n",
    "                'std': np.std(recall_scores),\n",
    "                'values': recall_scores\n",
    "            },\n",
    "            'n_samples': len(y_factor),\n",
    "            'class_distribution': {str(c): int(np.sum(y_factor == c)) for c in unique_classes}\n",
    "        }\n",
    "        \n",
    "        # Print performance metrics\n",
    "        print(f\"\\nPerformance metrics for {factor_name}:\")\n",
    "        print(f\"  Accuracy: {performance_dict[factor_name]['accuracy']['mean']:.4f} ± {performance_dict[factor_name]['accuracy']['std']:.4f}\")\n",
    "        print(f\"  F1 Score: {performance_dict[factor_name]['f1']['mean']:.4f} ± {performance_dict[factor_name]['f1']['std']:.4f}\")\n",
    "        print(f\"  Precision: {performance_dict[factor_name]['precision']['mean']:.4f} ± {performance_dict[factor_name]['precision']['std']:.4f}\")\n",
    "        print(f\"  Recall: {performance_dict[factor_name]['recall']['mean']:.4f} ± {performance_dict[factor_name]['recall']['std']:.4f}\")\n",
    "        \n",
    "        # Now train on full dataset for feature importance\n",
    "        xgb_classifier.fit(X_factor, y_encoded)\n",
    "        \n",
    "        # Get feature importance from XGBoost\n",
    "        # XGBoost provides multiple importance types: 'weight', 'gain', 'cover'\n",
    "        feature_importance = xgb_classifier.feature_importances_  # This is 'gain' by default\n",
    "        \n",
    "        # Create importance dataframe\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': pivot_df.columns,\n",
    "            'Importance': feature_importance\n",
    "        })\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Extract genomic information from UCSC-style IDs\n",
    "        def extract_ucsc_info(ucsc_id):\n",
    "            # Pattern to match UCSC style IDs like chr1+12345\n",
    "            match = re.match(r'^(chr[^+-]+)([+-])(\\d+)$', ucsc_id)\n",
    "            if match:\n",
    "                chrom = match.group(1)\n",
    "                strand = match.group(2)\n",
    "                position = int(match.group(3))\n",
    "                return chrom, strand, position\n",
    "            else:\n",
    "                # For non-matching IDs, try to extract using original pattern\n",
    "                match = re.match(r'^(\\d+\\.\\d+)_([+-])(.+)$', ucsc_id)\n",
    "                if match:\n",
    "                    bin_val = match.group(1)\n",
    "                    strand = match.group(2)\n",
    "                    chrom = match.group(3)\n",
    "                    return chrom, strand, bin_val\n",
    "                return \"Unknown\", \"?\", \"Unknown\"\n",
    "        \n",
    "        # Add genomic information to importance dataframe\n",
    "        importance_df['Chromosome'] = importance_df['Feature'].apply(lambda x: extract_ucsc_info(x)[0])\n",
    "        importance_df['Strand'] = importance_df['Feature'].apply(lambda x: extract_ucsc_info(x)[1])\n",
    "        importance_df['Position'] = importance_df['Feature'].apply(lambda x: extract_ucsc_info(x)[2])\n",
    "        \n",
    "        # Add percentile and cumulative importance to ALL features\n",
    "        importance_df['Percentile'] = importance_df['Importance'].rank(pct=True) * 100\n",
    "        importance_df['Cumulative_Importance'] = importance_df['Importance'].cumsum() / importance_df['Importance'].sum() * 100\n",
    "        \n",
    "        # Save all features with enhanced information in one file (DATA)\n",
    "        all_features_file = os.path.join(output_data_dir, f'{factor_name}_all_features_importance.csv')\n",
    "        importance_df.to_csv(all_features_file, index=False)\n",
    "        print(f\"Saved all features with importance metrics for {factor_name} to {all_features_file}\")\n",
    "        \n",
    "        top_n_features = importance_df.head(top_n)\n",
    "        \n",
    "        # Add top features to combined DataFrame\n",
    "        top_features_for_combined = top_n_features.head(top_n).copy()\n",
    "        top_features_for_combined['Factor'] = factor_name\n",
    "        top_features_for_combined['Rank'] = range(1, len(top_features_for_combined) + 1)\n",
    "        all_top_features_df = pd.concat([all_top_features_df, top_features_for_combined], ignore_index=True)\n",
    "        \n",
    "        # Store results\n",
    "        results_dict[factor_name] = {\n",
    "            'classes': label_encoder.classes_,\n",
    "            'top_features': top_n_features.head(10)['Feature'].tolist(),\n",
    "            'n_samples': len(y_factor)\n",
    "        }\n",
    "        importance_dict[factor_name] = importance_df\n",
    "    \n",
    "    # SAVE A CSV DOCUMENT WHERE TOP 50 FOR EACH FACTOR IS SAVED\n",
    "    # Sort the combined DataFrame by Factor and Rank\n",
    "    all_top_features_df = all_top_features_df.sort_values(['Factor', 'Rank'])\n",
    "    \n",
    "    # Save to CSV (DATA)\n",
    "    combined_top_features_file = os.path.join(output_data_dir, 'all_factors_top_features.csv')\n",
    "    all_top_features_df.to_csv(combined_top_features_file, index=False)\n",
    "    print(f\"\\nSaved top {top_n} features for all factors to {combined_top_features_file}\")\n",
    "    \n",
    "    # CREATE PERFORMANCE COMPARISON VISUALIZATION\n",
    "    if len(performance_dict) > 1:\n",
    "        # Create a performance comparison chart\n",
    "        print(\"\\nCreating performance comparison visualization...\")\n",
    "        \n",
    "        # Extract metrics for comparison\n",
    "        factors = list(performance_dict.keys())\n",
    "        accuracy_means = [performance_dict[f]['accuracy']['mean'] for f in factors]\n",
    "        f1_means = [performance_dict[f]['f1']['mean'] for f in factors]\n",
    "        precision_means = [performance_dict[f]['precision']['mean'] for f in factors]\n",
    "        recall_means = [performance_dict[f]['recall']['mean'] for f in factors]\n",
    "        \n",
    "        # Create a DataFrame for easy plotting\n",
    "        performance_df = pd.DataFrame({\n",
    "            'Factor': factors,\n",
    "            'Accuracy': accuracy_means,\n",
    "            'F1 Score': f1_means,\n",
    "            'Precision': precision_means,\n",
    "            'Recall': recall_means\n",
    "        })\n",
    "        \n",
    "        # Save performance comparison to CSV (DATA)\n",
    "        perf_file = os.path.join(output_data_dir, 'model_performance_comparison.csv')\n",
    "        performance_df.to_csv(perf_file, index=False)\n",
    "        print(f\"Saved performance comparison to {perf_file}\")\n",
    "        \n",
    "        # Create a bar chart visualization\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Set width of bars\n",
    "        bar_width = 0.2\n",
    "        index = np.arange(len(factors))\n",
    "        \n",
    "        plt.bar(index - 1.5*bar_width, accuracy_means, bar_width, label='Accuracy')\n",
    "        plt.bar(index - 0.5*bar_width, f1_means, bar_width, label='F1 Score')\n",
    "        plt.bar(index + 0.5*bar_width, precision_means, bar_width, label='Precision')\n",
    "        plt.bar(index + 1.5*bar_width, recall_means, bar_width, label='Recall')\n",
    "        \n",
    "        plt.xlabel('Factor')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('XGBoost Model Performance Comparison Across Factors')\n",
    "        plt.xticks(index, factors, rotation=45, ha='right')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(accuracy_means):\n",
    "            plt.text(i - 1.5*bar_width, v + 0.01, f'{v:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "        for i, v in enumerate(f1_means):\n",
    "            plt.text(i - 0.5*bar_width, v + 0.01, f'{v:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "        for i, v in enumerate(precision_means):\n",
    "            plt.text(i + 0.5*bar_width, v + 0.01, f'{v:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "        for i, v in enumerate(recall_means):\n",
    "            plt.text(i + 1.5*bar_width, v + 0.01, f'{v:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # Save performance comparison plot (PLOT)\n",
    "        perf_plot_file = os.path.join(output_plots_dir, 'xgboost_performance_comparison_plot.png')\n",
    "        plt.savefig(perf_plot_file, dpi=300)\n",
    "        print(f\"Saved performance comparison plot to {perf_plot_file}\")\n",
    "        plt.show()  # Show the figure (maintained from original)\n",
    "    \n",
    "    # Save feature comparison CSV but don't create heatmap\n",
    "    if len(importance_dict) > 1:\n",
    "        print(\"\\nCreating comparison of top features across factors...\")\n",
    "        \n",
    "        # Get top 5 features from each factor\n",
    "        top_features_by_factor = {}\n",
    "        all_top_features = set()\n",
    "        \n",
    "        for factor, imp_df in importance_dict.items():\n",
    "            top_features = imp_df.head(5)['Feature'].tolist()\n",
    "            top_features_by_factor[factor] = top_features\n",
    "            all_top_features.update(top_features)\n",
    "        \n",
    "        # Create comparison dataframe\n",
    "        comparison_data = []\n",
    "        for feature in all_top_features:\n",
    "            row = {'Feature': feature}\n",
    "            for factor, imp_df in importance_dict.items():\n",
    "                feature_imp = imp_df.loc[imp_df['Feature'] == feature, 'Importance'].values\n",
    "                row[factor] = feature_imp[0] if len(feature_imp) > 0 else 0\n",
    "            comparison_data.append(row)\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_df = comparison_df.sort_values('Feature')\n",
    "        \n",
    "        # Save comparison (DATA)\n",
    "        comparison_file = os.path.join(output_data_dir, 'top_features_comparison.csv')\n",
    "        comparison_df.to_csv(comparison_file, index=False)\n",
    "        print(f\"Saved feature comparison to {comparison_file}\")\n",
    "    \n",
    "    return results_dict, importance_dict, performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c668e-ed06-4155-b100-7bb6cb4ebf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict, importance_dict, performance_dict = run_random_forest_feature_selection_by_factor(\n",
    "   '../data_normalized/cleaned_Normalized_1000.csv', \n",
    "    '../data_xg/bin1000', '../images/xg_results/bin1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b1f5a-ea62-46f2-918f-fee340720c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict, importance_dict, performance_dict = run_random_forest_feature_selection_by_factor(\n",
    "   '../data_normalized/cleaned_Normalized_10000.csv', \n",
    "    '../data_xg/bin10000', '../images/xg_results/bin10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33305162-dd36-4b58-b42f-b67c38e0691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict, importance_dict, performance_dict = run_random_forest_feature_selection_by_factor(\n",
    "   '../data_normalized/cleaned_Normalized_100000.csv', \n",
    "    '../data_xg/bin100000', '../images/xg_results/bin100000')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
